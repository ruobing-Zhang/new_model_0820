/root/miniconda3/lib/python3.10/site-packages/lightning_fabric/__init__.py:36: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  __import__("pkg_resources").declare_namespace(__name__)
Global seed set to 1234
[NeMo I 2025-08-20 09:47:01 features:306] PADDING: 0
[NeMo I 2025-08-20 09:47:02 rnnt_models:226] Using RNNT Loss : warprnnt_numba
    Loss warprnnt_numba_kwargs: {'fastemit_lambda': 0, 'clamp': -1.0}
[NeMo I 2025-08-20 09:47:02 rnnt_models:226] Using RNNT Loss : warprnnt_numba
    Loss warprnnt_numba_kwargs: {'fastemit_lambda': 0, 'clamp': -1.0}
[NeMo W 2025-08-20 09:47:02 label_looping_base:109] No conditional node support for Cuda.
    Cuda graphs with while loops are disabled, decoding speed will be slower
    Reason: No `cuda-python` module. Please do `pip install cuda-python>=12.3`
[NeMo I 2025-08-20 09:47:02 save_restore_connector:282] Model EncDecRNNTModel was successfully restored from /root/autodl-tmp/joint_sortformer_and_asr_0815/pretrained_models/zh_conformer_transducer_large_bpe_init.nemo.
[DEBUG] base_model没有tokenizer属性，类型: <class 'nemo.collections.asr.models.rnnt_models.EncDecRNNTModel'>
[DEBUG] base_model所有属性: ['CHECKPOINT_HYPER_PARAMS_KEY', 'CHECKPOINT_HYPER_PARAMS_NAME', 'CHECKPOINT_HYPER_PARAMS_TYPE', 'T_destination', 'adapter_global_cfg_key', 'adapter_metadata_cfg_key', 'adapter_module_names', 'add_adapter', 'add_auxiliary_losses', 'add_module', 'all_gather', 'allow_zero_length_dataloader_with_multiple_devices', 'apply', 'automatic_optimization', 'backward', 'bfloat16', 'buffers', 'call_super_init', 'cfg', 'change_attention_model']
[DEBUG] 找到vocabulary对象，类型: <class 'omegaconf.listconfig.ListConfig'>
[DEBUG] 直接使用joint.vocabulary作为tokenizer
[NeMo I 2025-08-20 09:47:03 train_multispeaker_asr_adapter:52] [load_state_dict] missing keys: ['alpha', 'Gamma']
[NeMo I 2025-08-20 09:47:03 train_multispeaker_asr_adapter:59] Encoder adapter 支持状态: False
[NeMo W 2025-08-20 09:47:03 train_multispeaker_asr_adapter:62] 当前 NeMo 版本 encoder 不支持 add_adapter(); 使用 fallback 注入方法。
[NeMo I 2025-08-20 09:47:03 train_multispeaker_asr_adapter:67] 解冻参数: alpha
[NeMo I 2025-08-20 09:47:03 train_multispeaker_asr_adapter:81] 已冻结所有模型参数
[NeMo I 2025-08-20 09:47:03 train_multispeaker_asr_adapter:98] 解冻说话人注入参数: alpha
[NeMo I 2025-08-20 09:47:03 train_multispeaker_asr_adapter:103] 解冻解码器参数: prediction.embed.weight
[NeMo I 2025-08-20 09:47:03 train_multispeaker_asr_adapter:103] 解冻解码器参数: prediction.dec_rnn.lstm.weight_ih_l0
[NeMo I 2025-08-20 09:47:03 train_multispeaker_asr_adapter:103] 解冻解码器参数: prediction.dec_rnn.lstm.weight_hh_l0
[NeMo I 2025-08-20 09:47:03 train_multispeaker_asr_adapter:103] 解冻解码器参数: prediction.dec_rnn.lstm.bias_ih_l0
[NeMo I 2025-08-20 09:47:03 train_multispeaker_asr_adapter:103] 解冻解码器参数: prediction.dec_rnn.lstm.bias_hh_l0
[NeMo I 2025-08-20 09:47:03 train_multispeaker_asr_adapter:103] 解冻解码器参数: prediction.dec_rnn.lstm.weight_ih_l1
[NeMo I 2025-08-20 09:47:03 train_multispeaker_asr_adapter:103] 解冻解码器参数: prediction.dec_rnn.lstm.weight_hh_l1
[NeMo I 2025-08-20 09:47:03 train_multispeaker_asr_adapter:103] 解冻解码器参数: prediction.dec_rnn.lstm.bias_ih_l1
[NeMo I 2025-08-20 09:47:03 train_multispeaker_asr_adapter:103] 解冻解码器参数: prediction.dec_rnn.lstm.bias_hh_l1
[NeMo I 2025-08-20 09:47:03 train_multispeaker_asr_adapter:108] 解冻joint网络参数: pred.weight
[NeMo I 2025-08-20 09:47:03 train_multispeaker_asr_adapter:108] 解冻joint网络参数: pred.bias
[NeMo I 2025-08-20 09:47:03 train_multispeaker_asr_adapter:108] 解冻joint网络参数: enc.weight
[NeMo I 2025-08-20 09:47:03 train_multispeaker_asr_adapter:108] 解冻joint网络参数: enc.bias
[NeMo I 2025-08-20 09:47:03 train_multispeaker_asr_adapter:108] 解冻joint网络参数: joint_net.2.weight
[NeMo I 2025-08-20 09:47:03 train_multispeaker_asr_adapter:108] 解冻joint网络参数: joint_net.2.bias
[NeMo I 2025-08-20 09:47:03 train_multispeaker_asr_adapter:118] === 参数统计 ===
[NeMo I 2025-08-20 09:47:03 train_multispeaker_asr_adapter:119] 总参数数量: 123,696,106
[NeMo I 2025-08-20 09:47:03 train_multispeaker_asr_adapter:120] 可训练参数数量: 8,584,682
[NeMo I 2025-08-20 09:47:03 train_multispeaker_asr_adapter:121] 可训练参数比例: 6.94%
[NeMo I 2025-08-20 09:47:03 train_multispeaker_asr_adapter:122] ===============
[NeMo W 2025-08-20 09:47:03 nemo_logging:361] /root/miniconda3/lib/python3.10/site-packages/lightning_fabric/connector.py:554: 16 is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!
    
Using 16bit Automatic Mixed Precision (AMP)
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
You are using a CUDA device ('NVIDIA GeForce RTX 4090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[NeMo I 2025-08-20 09:47:03 train_multispeaker_asr_adapter:167] === 优化器参数详情 ===
[NeMo I 2025-08-20 09:47:03 train_multispeaker_asr_adapter:168] Encoder 可训练参数: 0 个
[NeMo I 2025-08-20 09:47:03 train_multispeaker_asr_adapter:172] Decoder 可训练参数: 9 个
[NeMo I 2025-08-20 09:47:03 train_multispeaker_asr_adapter:174]   - decoder.prediction.embed.weight: torch.Size([1001, 640]) (640,640 参数)
[NeMo I 2025-08-20 09:47:03 train_multispeaker_asr_adapter:174]   - decoder.prediction.dec_rnn.lstm.weight_ih_l0: torch.Size([2560, 640]) (1,638,400 参数)
[NeMo I 2025-08-20 09:47:03 train_multispeaker_asr_adapter:174]   - decoder.prediction.dec_rnn.lstm.weight_hh_l0: torch.Size([2560, 640]) (1,638,400 参数)
[NeMo I 2025-08-20 09:47:03 train_multispeaker_asr_adapter:174]   - decoder.prediction.dec_rnn.lstm.bias_ih_l0: torch.Size([2560]) (2,560 参数)
[NeMo I 2025-08-20 09:47:03 train_multispeaker_asr_adapter:174]   - decoder.prediction.dec_rnn.lstm.bias_hh_l0: torch.Size([2560]) (2,560 参数)
[NeMo I 2025-08-20 09:47:03 train_multispeaker_asr_adapter:174]   - decoder.prediction.dec_rnn.lstm.weight_ih_l1: torch.Size([2560, 640]) (1,638,400 参数)
[NeMo I 2025-08-20 09:47:03 train_multispeaker_asr_adapter:174]   - decoder.prediction.dec_rnn.lstm.weight_hh_l1: torch.Size([2560, 640]) (1,638,400 参数)
[NeMo I 2025-08-20 09:47:03 train_multispeaker_asr_adapter:174]   - decoder.prediction.dec_rnn.lstm.bias_ih_l1: torch.Size([2560]) (2,560 参数)
[NeMo I 2025-08-20 09:47:03 train_multispeaker_asr_adapter:174]   - decoder.prediction.dec_rnn.lstm.bias_hh_l1: torch.Size([2560]) (2,560 参数)
[NeMo I 2025-08-20 09:47:03 train_multispeaker_asr_adapter:176] Joint 可训练参数: 6 个
[NeMo I 2025-08-20 09:47:03 train_multispeaker_asr_adapter:178]   - joint.pred.weight: torch.Size([640, 640]) (409,600 参数)
[NeMo I 2025-08-20 09:47:03 train_multispeaker_asr_adapter:178]   - joint.pred.bias: torch.Size([640]) (640 参数)
[NeMo I 2025-08-20 09:47:03 train_multispeaker_asr_adapter:178]   - joint.enc.weight: torch.Size([640, 512]) (327,680 参数)
[NeMo I 2025-08-20 09:47:03 train_multispeaker_asr_adapter:178]   - joint.enc.bias: torch.Size([640]) (640 参数)
[NeMo I 2025-08-20 09:47:03 train_multispeaker_asr_adapter:178]   - joint.joint_net.2.weight: torch.Size([1001, 640]) (640,640 参数)
[NeMo I 2025-08-20 09:47:03 train_multispeaker_asr_adapter:178]   - joint.joint_net.2.bias: torch.Size([1001]) (1,001 参数)
[NeMo I 2025-08-20 09:47:03 train_multispeaker_asr_adapter:180] 其他可训练参数: 1 个
[NeMo I 2025-08-20 09:47:03 train_multispeaker_asr_adapter:182]   - alpha: torch.Size([]) (1 参数)
[NeMo I 2025-08-20 09:47:03 train_multispeaker_asr_adapter:198] 优化器总计可训练参数: 8,584,682
[NeMo I 2025-08-20 09:47:03 train_multispeaker_asr_adapter:199] =====================

  | Name  | Type              | Params
--------------------------------------------
0 | model | RNNTWithSpkInject | 123 M 
--------------------------------------------
8.6 M     Trainable params
115 M     Non-trainable params
123 M     Total params
494.784   Total estimated model params size (MB)
Sanity Checking: 0it [00:00, ?it/s][NeMo W 2025-08-20 09:47:03 nemo_logging:361] /root/miniconda3/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:438: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 128 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
    
Sanity Checking:   0%|          | 0/2 [00:00<?, ?it/s]Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]DEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 684]), enc_len = tensor([684], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 684])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=684
DEBUG: 转换后 enc_out.shape = torch.Size([1, 684, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 684, 512])
DEBUG: actual_T: 684, enc_len调整后: tensor([684], device='cuda:0')
[NeMo I 2025-08-20 09:47:04 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:04 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[NeMo W 2025-08-20 09:47:04 nemo_logging:361] /root/miniconda3/lib/python3.10/site-packages/numba/np/ufunc/parallel.py:371: NumbaWarning: [1mThe TBB threading layer requires TBB version 2021 update 6 or later i.e., TBB_INTERFACE_VERSION >= 12060. Found TBB_INTERFACE_VERSION = 12050. The TBB threading layer is disabled.[0m
      warnings.warn(problem)
    
[NeMo W 2025-08-20 09:47:05 nemo_logging:361] /root/miniconda3/lib/python3.10/site-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: [1mGrid size 1 will likely result in GPU under-utilization due to low occupancy.[0m
      warn(NumbaPerformanceWarning(msg))
    
[NeMo W 2025-08-20 09:47:05 nemo_logging:361] /root/miniconda3/lib/python3.10/site-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: [1mGrid size 1 will likely result in GPU under-utilization due to low occupancy.[0m
      warn(NumbaPerformanceWarning(msg))
    
[DEBUG] targets.shape: torch.Size([1, 169]), target_length: tensor([169], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 169])
[DEBUG] target_length = tensor([169], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 169
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 170]), dec_len=tensor([169], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 170]) -> torch.Size([1, 170, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 170, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 684, 512]), dec_out.shape=torch.Size([1, 170, 640])
[DEBUG] enc_len=tensor([684], device='cuda:0'), target_length=tensor([169], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([684], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 684, 512]), dec_out.shape: torch.Size([1, 170, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 684, 170, 1001])
Sanity Checking DataLoader 0:  50%|█████     | 1/2 [00:02<00:02,  2.05s/it]DEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 699]), enc_len = tensor([699], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 699])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=699
DEBUG: 转换后 enc_out.shape = torch.Size([1, 699, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 699, 512])
DEBUG: actual_T: 699, enc_len调整后: tensor([699], device='cuda:0')
[NeMo I 2025-08-20 09:47:05 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:05 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[NeMo W 2025-08-20 09:47:06 nemo_logging:361] /root/miniconda3/lib/python3.10/site-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: [1mGrid size 1 will likely result in GPU under-utilization due to low occupancy.[0m
      warn(NumbaPerformanceWarning(msg))
    
[DEBUG] targets.shape: torch.Size([1, 175]), target_length: tensor([175], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 175])
[DEBUG] target_length = tensor([175], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 175
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 176]), dec_len=tensor([175], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 176]) -> torch.Size([1, 176, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 176, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 699, 512]), dec_out.shape=torch.Size([1, 176, 640])
[DEBUG] enc_len=tensor([699], device='cuda:0'), target_length=tensor([175], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([699], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 699, 512]), dec_out.shape: torch.Size([1, 176, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 699, 176, 1001])
Sanity Checking DataLoader 0: 100%|██████████| 2/2 [00:02<00:00,  1.06s/it]                                                                           [NeMo W 2025-08-20 09:47:06 nemo_logging:361] /root/miniconda3/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:438: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 128 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
    
Training: 0it [00:00, ?it/s]Training:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/63 [00:00<?, ?it/s] DEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 912]), enc_len = tensor([912], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 912])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=912
DEBUG: 转换后 enc_out.shape = torch.Size([1, 912, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 912, 512])
DEBUG: actual_T: 912, enc_len调整后: tensor([912], device='cuda:0')
[NeMo I 2025-08-20 09:47:06 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:06 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[NeMo W 2025-08-20 09:47:06 nemo_logging:361] /root/miniconda3/lib/python3.10/site-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: [1mGrid size 1 will likely result in GPU under-utilization due to low occupancy.[0m
      warn(NumbaPerformanceWarning(msg))
    
[DEBUG] targets.shape: torch.Size([1, 225]), target_length: tensor([225], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 225])
[DEBUG] target_length = tensor([225], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 225
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 226]), dec_len=tensor([225], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 226]) -> torch.Size([1, 226, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 226, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 912, 512]), dec_out.shape=torch.Size([1, 226, 640])
[DEBUG] enc_len=tensor([912], device='cuda:0'), target_length=tensor([225], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([912], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 912, 512]), dec_out.shape: torch.Size([1, 226, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 912, 226, 1001])
Epoch 0:   2%|▏         | 1/63 [00:00<00:51,  1.22it/s]Epoch 0:   2%|▏         | 1/63 [00:00<00:51,  1.21it/s, v_num=31, train_loss_step=7.3e+3]DEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 745]), enc_len = tensor([745], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 745])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=745
DEBUG: 转换后 enc_out.shape = torch.Size([1, 745, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 745, 512])
DEBUG: actual_T: 745, enc_len调整后: tensor([745], device='cuda:0')
[NeMo I 2025-08-20 09:47:07 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:07 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 185]), target_length: tensor([185], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 185])
[DEBUG] target_length = tensor([185], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 185
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 186]), dec_len=tensor([185], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 186]) -> torch.Size([1, 186, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 186, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 745, 512]), dec_out.shape=torch.Size([1, 186, 640])
[DEBUG] enc_len=tensor([745], device='cuda:0'), target_length=tensor([185], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([745], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 745, 512]), dec_out.shape: torch.Size([1, 186, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 745, 186, 1001])
Epoch 0:   3%|▎         | 2/63 [00:00<00:27,  2.20it/s, v_num=31, train_loss_step=7.3e+3]Epoch 0:   3%|▎         | 2/63 [00:00<00:27,  2.20it/s, v_num=31, train_loss_step=5.97e+3]DEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 887]), enc_len = tensor([887], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 887])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=887
DEBUG: 转换后 enc_out.shape = torch.Size([1, 887, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 887, 512])
DEBUG: actual_T: 887, enc_len调整后: tensor([887], device='cuda:0')
[NeMo I 2025-08-20 09:47:07 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:07 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 231]), target_length: tensor([231], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 231])
[DEBUG] target_length = tensor([231], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 231
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 232]), dec_len=tensor([231], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 232]) -> torch.Size([1, 232, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 232, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 887, 512]), dec_out.shape=torch.Size([1, 232, 640])
[DEBUG] enc_len=tensor([887], device='cuda:0'), target_length=tensor([231], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([887], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 887, 512]), dec_out.shape: torch.Size([1, 232, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 887, 232, 1001])
Epoch 0:   5%|▍         | 3/63 [00:00<00:19,  3.02it/s, v_num=31, train_loss_step=5.97e+3]Epoch 0:   5%|▍         | 3/63 [00:00<00:19,  3.02it/s, v_num=31, train_loss_step=7.16e+3]DEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 549]), enc_len = tensor([549], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 549])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=549
DEBUG: 转换后 enc_out.shape = torch.Size([1, 549, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 549, 512])
DEBUG: actual_T: 549, enc_len调整后: tensor([549], device='cuda:0')
[NeMo I 2025-08-20 09:47:07 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:07 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 114]), target_length: tensor([114], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 114])
[DEBUG] target_length = tensor([114], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 114
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 115]), dec_len=tensor([114], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 115]) -> torch.Size([1, 115, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 115, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 549, 512]), dec_out.shape=torch.Size([1, 115, 640])
[DEBUG] enc_len=tensor([549], device='cuda:0'), target_length=tensor([114], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([549], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 549, 512]), dec_out.shape: torch.Size([1, 115, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 549, 115, 1001])
Epoch 0:   6%|▋         | 4/63 [00:01<00:15,  3.75it/s, v_num=31, train_loss_step=7.16e+3]Epoch 0:   6%|▋         | 4/63 [00:01<00:15,  3.75it/s, v_num=31, train_loss_step=4.28e+3]DEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 562]), enc_len = tensor([562], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 562])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=562
DEBUG: 转换后 enc_out.shape = torch.Size([1, 562, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 562, 512])
DEBUG: actual_T: 562, enc_len调整后: tensor([562], device='cuda:0')
[NeMo I 2025-08-20 09:47:07 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:07 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 139]), target_length: tensor([139], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 139])
[DEBUG] target_length = tensor([139], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 139
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 140]), dec_len=tensor([139], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 140]) -> torch.Size([1, 140, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 140, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 562, 512]), dec_out.shape=torch.Size([1, 140, 640])
[DEBUG] enc_len=tensor([562], device='cuda:0'), target_length=tensor([139], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([562], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 562, 512]), dec_out.shape: torch.Size([1, 140, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 562, 140, 1001])
Epoch 0:   8%|▊         | 5/63 [00:01<00:13,  4.39it/s, v_num=31, train_loss_step=4.28e+3]Epoch 0:   8%|▊         | 5/63 [00:01<00:13,  4.39it/s, v_num=31, train_loss_step=4.49e+3]DEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 833]), enc_len = tensor([833], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 833])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=833
DEBUG: 转换后 enc_out.shape = torch.Size([1, 833, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 833, 512])
DEBUG: actual_T: 833, enc_len调整后: tensor([833], device='cuda:0')
[NeMo I 2025-08-20 09:47:07 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:07 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 188]), target_length: tensor([188], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 188])
[DEBUG] target_length = tensor([188], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 188
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 189]), dec_len=tensor([188], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 189]) -> torch.Size([1, 189, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 189, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 833, 512]), dec_out.shape=torch.Size([1, 189, 640])
[DEBUG] enc_len=tensor([833], device='cuda:0'), target_length=tensor([188], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([833], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 833, 512]), dec_out.shape: torch.Size([1, 189, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 833, 189, 1001])
Epoch 0:  10%|▉         | 6/63 [00:01<00:11,  4.93it/s, v_num=31, train_loss_step=4.49e+3]Epoch 0:  10%|▉         | 6/63 [00:01<00:11,  4.92it/s, v_num=31, train_loss_step=6.58e+3]DEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 575]), enc_len = tensor([575], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 575])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=575
DEBUG: 转换后 enc_out.shape = torch.Size([1, 575, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 575, 512])
DEBUG: actual_T: 575, enc_len调整后: tensor([575], device='cuda:0')
[NeMo I 2025-08-20 09:47:07 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:07 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 114]), target_length: tensor([114], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 114])
[DEBUG] target_length = tensor([114], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 114
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 115]), dec_len=tensor([114], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 115]) -> torch.Size([1, 115, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 115, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 575, 512]), dec_out.shape=torch.Size([1, 115, 640])
[DEBUG] enc_len=tensor([575], device='cuda:0'), target_length=tensor([114], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([575], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 575, 512]), dec_out.shape: torch.Size([1, 115, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 575, 115, 1001])
Epoch 0:  11%|█         | 7/63 [00:01<00:10,  5.43it/s, v_num=31, train_loss_step=6.58e+3]Epoch 0:  11%|█         | 7/63 [00:01<00:10,  5.43it/s, v_num=31, train_loss_step=4.46e+3]DEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 559]), enc_len = tensor([559], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 559])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=559
DEBUG: 转换后 enc_out.shape = torch.Size([1, 559, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 559, 512])
DEBUG: actual_T: 559, enc_len调整后: tensor([559], device='cuda:0')
[NeMo I 2025-08-20 09:47:07 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:07 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 128]), target_length: tensor([128], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 128])
[DEBUG] target_length = tensor([128], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 128
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 129]), dec_len=tensor([128], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 129]) -> torch.Size([1, 129, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 129, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 559, 512]), dec_out.shape=torch.Size([1, 129, 640])
[DEBUG] enc_len=tensor([559], device='cuda:0'), target_length=tensor([128], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([559], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 559, 512]), dec_out.shape: torch.Size([1, 129, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 559, 129, 1001])
Epoch 0:  13%|█▎        | 8/63 [00:01<00:09,  5.90it/s, v_num=31, train_loss_step=4.46e+3]Epoch 0:  13%|█▎        | 8/63 [00:01<00:09,  5.90it/s, v_num=31, train_loss_step=4.42e+3]DEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 869]), enc_len = tensor([869], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 869])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=869
DEBUG: 转换后 enc_out.shape = torch.Size([1, 869, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 869, 512])
DEBUG: actual_T: 869, enc_len调整后: tensor([869], device='cuda:0')
[NeMo I 2025-08-20 09:47:07 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:07 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 210]), target_length: tensor([210], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 210])
[DEBUG] target_length = tensor([210], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 210
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 211]), dec_len=tensor([210], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 211]) -> torch.Size([1, 211, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 211, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 869, 512]), dec_out.shape=torch.Size([1, 211, 640])
[DEBUG] enc_len=tensor([869], device='cuda:0'), target_length=tensor([210], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([869], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 869, 512]), dec_out.shape: torch.Size([1, 211, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 869, 211, 1001])
Epoch 0:  14%|█▍        | 9/63 [00:01<00:08,  6.23it/s, v_num=31, train_loss_step=4.42e+3]Epoch 0:  14%|█▍        | 9/63 [00:01<00:08,  6.23it/s, v_num=31, train_loss_step=6.93e+3]DEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 758]), enc_len = tensor([758], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 758])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=758
DEBUG: 转换后 enc_out.shape = torch.Size([1, 758, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 758, 512])
DEBUG: actual_T: 758, enc_len调整后: tensor([758], device='cuda:0')
[NeMo I 2025-08-20 09:47:07 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:07 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 193]), target_length: tensor([193], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 193])
[DEBUG] target_length = tensor([193], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 193
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 194]), dec_len=tensor([193], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 194]) -> torch.Size([1, 194, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 194, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 758, 512]), dec_out.shape=torch.Size([1, 194, 640])
[DEBUG] enc_len=tensor([758], device='cuda:0'), target_length=tensor([193], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([758], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 758, 512]), dec_out.shape: torch.Size([1, 194, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 758, 194, 1001])
Epoch 0:  16%|█▌        | 10/63 [00:01<00:08,  6.56it/s, v_num=31, train_loss_step=6.93e+3]Epoch 0:  16%|█▌        | 10/63 [00:01<00:08,  6.56it/s, v_num=31, train_loss_step=6.09e+3]DEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 699]), enc_len = tensor([699], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 699])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=699
DEBUG: 转换后 enc_out.shape = torch.Size([1, 699, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 699, 512])
DEBUG: actual_T: 699, enc_len调整后: tensor([699], device='cuda:0')
[NeMo I 2025-08-20 09:47:07 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:07 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 175]), target_length: tensor([175], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 175])
[DEBUG] target_length = tensor([175], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 175
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 176]), dec_len=tensor([175], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 176]) -> torch.Size([1, 176, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 176, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 699, 512]), dec_out.shape=torch.Size([1, 176, 640])
[DEBUG] enc_len=tensor([699], device='cuda:0'), target_length=tensor([175], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([699], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 699, 512]), dec_out.shape: torch.Size([1, 176, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 699, 176, 1001])
Epoch 0:  17%|█▋        | 11/63 [00:01<00:07,  6.74it/s, v_num=31, train_loss_step=6.09e+3]Epoch 0:  17%|█▋        | 11/63 [00:01<00:07,  6.74it/s, v_num=31, train_loss_step=5.61e+3]DEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 787]), enc_len = tensor([787], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 787])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=787
DEBUG: 转换后 enc_out.shape = torch.Size([1, 787, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 787, 512])
DEBUG: actual_T: 787, enc_len调整后: tensor([787], device='cuda:0')
[NeMo I 2025-08-20 09:47:07 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:07 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 189]), target_length: tensor([189], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 189])
[DEBUG] target_length = tensor([189], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 189
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 190]), dec_len=tensor([189], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 190]) -> torch.Size([1, 190, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 190, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 787, 512]), dec_out.shape=torch.Size([1, 190, 640])
[DEBUG] enc_len=tensor([787], device='cuda:0'), target_length=tensor([189], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([787], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 787, 512]), dec_out.shape: torch.Size([1, 190, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 787, 190, 1001])
Epoch 0:  19%|█▉        | 12/63 [00:01<00:07,  7.02it/s, v_num=31, train_loss_step=5.61e+3]Epoch 0:  19%|█▉        | 12/63 [00:01<00:07,  7.02it/s, v_num=31, train_loss_step=6.21e+3]DEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 773]), enc_len = tensor([773], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 773])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=773
DEBUG: 转换后 enc_out.shape = torch.Size([1, 773, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 773, 512])
DEBUG: actual_T: 773, enc_len调整后: tensor([773], device='cuda:0')
[NeMo I 2025-08-20 09:47:07 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:07 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 184]), target_length: tensor([184], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 184])
[DEBUG] target_length = tensor([184], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 184
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 185]), dec_len=tensor([184], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 185]) -> torch.Size([1, 185, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 185, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 773, 512]), dec_out.shape=torch.Size([1, 185, 640])
[DEBUG] enc_len=tensor([773], device='cuda:0'), target_length=tensor([184], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([773], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 773, 512]), dec_out.shape: torch.Size([1, 185, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 773, 185, 1001])
Epoch 0:  21%|██        | 13/63 [00:01<00:06,  7.27it/s, v_num=31, train_loss_step=6.21e+3]Epoch 0:  21%|██        | 13/63 [00:01<00:06,  7.27it/s, v_num=31, train_loss_step=6.05e+3]DEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 585]), enc_len = tensor([584], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 585])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=585
DEBUG: 转换后 enc_out.shape = torch.Size([1, 585, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 585, 512])
DEBUG: actual_T: 585, enc_len调整后: tensor([584], device='cuda:0')
[NeMo I 2025-08-20 09:47:08 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:08 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 112]), target_length: tensor([112], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 112])
[DEBUG] target_length = tensor([112], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 112
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 113]), dec_len=tensor([112], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 113]) -> torch.Size([1, 113, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 113, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 585, 512]), dec_out.shape=torch.Size([1, 113, 640])
[DEBUG] enc_len=tensor([584], device='cuda:0'), target_length=tensor([112], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([584], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 585, 512]), dec_out.shape: torch.Size([1, 113, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 585, 113, 1001])
Epoch 0:  22%|██▏       | 14/63 [00:01<00:06,  7.53it/s, v_num=31, train_loss_step=6.05e+3]Epoch 0:  22%|██▏       | 14/63 [00:01<00:06,  7.52it/s, v_num=31, train_loss_step=4.38e+3]DEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 577]), enc_len = tensor([577], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 577])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=577
DEBUG: 转换后 enc_out.shape = torch.Size([1, 577, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 577, 512])
DEBUG: actual_T: 577, enc_len调整后: tensor([577], device='cuda:0')
[NeMo I 2025-08-20 09:47:08 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:08 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 153]), target_length: tensor([153], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 153])
[DEBUG] target_length = tensor([153], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 153
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 154]), dec_len=tensor([153], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 154]) -> torch.Size([1, 154, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 154, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 577, 512]), dec_out.shape=torch.Size([1, 154, 640])
[DEBUG] enc_len=tensor([577], device='cuda:0'), target_length=tensor([153], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([577], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 577, 512]), dec_out.shape: torch.Size([1, 154, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 577, 154, 1001])
Epoch 0:  24%|██▍       | 15/63 [00:01<00:06,  7.77it/s, v_num=31, train_loss_step=4.38e+3]Epoch 0:  24%|██▍       | 15/63 [00:01<00:06,  7.77it/s, v_num=31, train_loss_step=4.48e+3]
Validation: 0it [00:00, ?it/s][A
Validation:   0%|          | 0/63 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/63 [00:00<?, ?it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 684]), enc_len = tensor([684], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 684])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=684
DEBUG: 转换后 enc_out.shape = torch.Size([1, 684, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 684, 512])
DEBUG: actual_T: 684, enc_len调整后: tensor([684], device='cuda:0')
[NeMo I 2025-08-20 09:47:08 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:08 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 169]), target_length: tensor([169], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 169])
[DEBUG] target_length = tensor([169], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 169
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 170]), dec_len=tensor([169], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 170]) -> torch.Size([1, 170, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 170, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 684, 512]), dec_out.shape=torch.Size([1, 170, 640])
[DEBUG] enc_len=tensor([684], device='cuda:0'), target_length=tensor([169], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([684], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 684, 512]), dec_out.shape: torch.Size([1, 170, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 684, 170, 1001])

Validation DataLoader 0:   2%|▏         | 1/63 [00:00<00:05, 12.00it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 699]), enc_len = tensor([699], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 699])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=699
DEBUG: 转换后 enc_out.shape = torch.Size([1, 699, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 699, 512])
DEBUG: actual_T: 699, enc_len调整后: tensor([699], device='cuda:0')
[NeMo I 2025-08-20 09:47:08 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:08 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 175]), target_length: tensor([175], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 175])
[DEBUG] target_length = tensor([175], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 175
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 176]), dec_len=tensor([175], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 176]) -> torch.Size([1, 176, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 176, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 699, 512]), dec_out.shape=torch.Size([1, 176, 640])
[DEBUG] enc_len=tensor([699], device='cuda:0'), target_length=tensor([175], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([699], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 699, 512]), dec_out.shape: torch.Size([1, 176, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 699, 176, 1001])

Validation DataLoader 0:   3%|▎         | 2/63 [00:00<00:03, 15.69it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 887]), enc_len = tensor([887], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 887])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=887
DEBUG: 转换后 enc_out.shape = torch.Size([1, 887, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 887, 512])
DEBUG: actual_T: 887, enc_len调整后: tensor([887], device='cuda:0')
[NeMo I 2025-08-20 09:47:08 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:08 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 231]), target_length: tensor([231], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 231])
[DEBUG] target_length = tensor([231], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 231
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 232]), dec_len=tensor([231], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 232]) -> torch.Size([1, 232, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 232, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 887, 512]), dec_out.shape=torch.Size([1, 232, 640])
[DEBUG] enc_len=tensor([887], device='cuda:0'), target_length=tensor([231], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([887], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 887, 512]), dec_out.shape: torch.Size([1, 232, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 887, 232, 1001])

Validation DataLoader 0:   5%|▍         | 3/63 [00:00<00:03, 17.26it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 678]), enc_len = tensor([678], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 678])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=678
DEBUG: 转换后 enc_out.shape = torch.Size([1, 678, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 678, 512])
DEBUG: actual_T: 678, enc_len调整后: tensor([678], device='cuda:0')
[NeMo I 2025-08-20 09:47:08 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:08 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 145]), target_length: tensor([145], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 145])
[DEBUG] target_length = tensor([145], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 145
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 146]), dec_len=tensor([145], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 146]) -> torch.Size([1, 146, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 146, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 678, 512]), dec_out.shape=torch.Size([1, 146, 640])
[DEBUG] enc_len=tensor([678], device='cuda:0'), target_length=tensor([145], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([678], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 678, 512]), dec_out.shape: torch.Size([1, 146, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 678, 146, 1001])

Validation DataLoader 0:   6%|▋         | 4/63 [00:00<00:03, 17.57it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 677]), enc_len = tensor([676], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 677])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=677
DEBUG: 转换后 enc_out.shape = torch.Size([1, 677, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 677, 512])
DEBUG: actual_T: 677, enc_len调整后: tensor([676], device='cuda:0')
[NeMo I 2025-08-20 09:47:08 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:08 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 155]), target_length: tensor([155], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 155])
[DEBUG] target_length = tensor([155], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 155
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 156]), dec_len=tensor([155], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 156]) -> torch.Size([1, 156, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 156, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 677, 512]), dec_out.shape=torch.Size([1, 156, 640])
[DEBUG] enc_len=tensor([676], device='cuda:0'), target_length=tensor([155], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([676], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 677, 512]), dec_out.shape: torch.Size([1, 156, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 677, 156, 1001])

Validation DataLoader 0:   8%|▊         | 5/63 [00:00<00:03, 18.16it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 694]), enc_len = tensor([694], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 694])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=694
DEBUG: 转换后 enc_out.shape = torch.Size([1, 694, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 694, 512])
DEBUG: actual_T: 694, enc_len调整后: tensor([694], device='cuda:0')
[NeMo I 2025-08-20 09:47:08 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:08 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 165]), target_length: tensor([165], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 165])
[DEBUG] target_length = tensor([165], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 165
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 166]), dec_len=tensor([165], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 166]) -> torch.Size([1, 166, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 166, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 694, 512]), dec_out.shape=torch.Size([1, 166, 640])
[DEBUG] enc_len=tensor([694], device='cuda:0'), target_length=tensor([165], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([694], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 694, 512]), dec_out.shape: torch.Size([1, 166, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 694, 166, 1001])

Validation DataLoader 0:  10%|▉         | 6/63 [00:00<00:03, 18.57it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 615]), enc_len = tensor([615], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 615])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=615
DEBUG: 转换后 enc_out.shape = torch.Size([1, 615, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 615, 512])
DEBUG: actual_T: 615, enc_len调整后: tensor([615], device='cuda:0')
[NeMo I 2025-08-20 09:47:08 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:08 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 112]), target_length: tensor([112], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 112])
[DEBUG] target_length = tensor([112], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 112
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 113]), dec_len=tensor([112], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 113]) -> torch.Size([1, 113, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 113, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 615, 512]), dec_out.shape=torch.Size([1, 113, 640])
[DEBUG] enc_len=tensor([615], device='cuda:0'), target_length=tensor([112], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([615], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 615, 512]), dec_out.shape: torch.Size([1, 113, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 615, 113, 1001])

Validation DataLoader 0:  11%|█         | 7/63 [00:00<00:02, 18.76it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 966]), enc_len = tensor([966], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 966])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=966
DEBUG: 转换后 enc_out.shape = torch.Size([1, 966, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 966, 512])
DEBUG: actual_T: 966, enc_len调整后: tensor([966], device='cuda:0')
[NeMo I 2025-08-20 09:47:08 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:08 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 237]), target_length: tensor([237], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 237])
[DEBUG] target_length = tensor([237], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 237
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 238]), dec_len=tensor([237], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 238]) -> torch.Size([1, 238, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 238, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 966, 512]), dec_out.shape=torch.Size([1, 238, 640])
[DEBUG] enc_len=tensor([966], device='cuda:0'), target_length=tensor([237], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([966], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 966, 512]), dec_out.shape: torch.Size([1, 238, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 966, 238, 1001])

Validation DataLoader 0:  13%|█▎        | 8/63 [00:00<00:02, 18.54it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 809]), enc_len = tensor([809], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 809])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=809
DEBUG: 转换后 enc_out.shape = torch.Size([1, 809, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 809, 512])
DEBUG: actual_T: 809, enc_len调整后: tensor([809], device='cuda:0')
[NeMo I 2025-08-20 09:47:08 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:08 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 154]), target_length: tensor([154], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 154])
[DEBUG] target_length = tensor([154], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 154
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 155]), dec_len=tensor([154], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 155]) -> torch.Size([1, 155, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 155, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 809, 512]), dec_out.shape=torch.Size([1, 155, 640])
[DEBUG] enc_len=tensor([809], device='cuda:0'), target_length=tensor([154], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([809], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 809, 512]), dec_out.shape: torch.Size([1, 155, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 809, 155, 1001])

Validation DataLoader 0:  14%|█▍        | 9/63 [00:00<00:02, 18.73it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 820]), enc_len = tensor([820], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 820])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=820
DEBUG: 转换后 enc_out.shape = torch.Size([1, 820, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 820, 512])
DEBUG: actual_T: 820, enc_len调整后: tensor([820], device='cuda:0')
[NeMo I 2025-08-20 09:47:08 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:08 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 236]), target_length: tensor([236], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 236])
[DEBUG] target_length = tensor([236], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 236
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 237]), dec_len=tensor([236], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 237]) -> torch.Size([1, 237, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 237, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 820, 512]), dec_out.shape=torch.Size([1, 237, 640])
[DEBUG] enc_len=tensor([820], device='cuda:0'), target_length=tensor([236], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([820], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 820, 512]), dec_out.shape: torch.Size([1, 237, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 820, 237, 1001])

Validation DataLoader 0:  16%|█▌        | 10/63 [00:00<00:02, 18.79it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 537]), enc_len = tensor([537], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 537])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=537
DEBUG: 转换后 enc_out.shape = torch.Size([1, 537, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 537, 512])
DEBUG: actual_T: 537, enc_len调整后: tensor([537], device='cuda:0')
[NeMo I 2025-08-20 09:47:08 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:08 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 99]), target_length: tensor([99], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 99])
[DEBUG] target_length = tensor([99], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 99
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 100]), dec_len=tensor([99], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 100]) -> torch.Size([1, 100, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 100, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 537, 512]), dec_out.shape=torch.Size([1, 100, 640])
[DEBUG] enc_len=tensor([537], device='cuda:0'), target_length=tensor([99], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([537], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 537, 512]), dec_out.shape: torch.Size([1, 100, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 537, 100, 1001])

Validation DataLoader 0:  17%|█▋        | 11/63 [00:00<00:02, 19.04it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 848]), enc_len = tensor([848], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 848])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=848
DEBUG: 转换后 enc_out.shape = torch.Size([1, 848, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 848, 512])
DEBUG: actual_T: 848, enc_len调整后: tensor([848], device='cuda:0')
[NeMo I 2025-08-20 09:47:08 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:08 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 179]), target_length: tensor([179], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 179])
[DEBUG] target_length = tensor([179], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 179
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 180]), dec_len=tensor([179], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 180]) -> torch.Size([1, 180, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 180, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 848, 512]), dec_out.shape=torch.Size([1, 180, 640])
[DEBUG] enc_len=tensor([848], device='cuda:0'), target_length=tensor([179], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([848], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 848, 512]), dec_out.shape: torch.Size([1, 180, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 848, 180, 1001])

Validation DataLoader 0:  19%|█▉        | 12/63 [00:00<00:02, 19.12it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 693]), enc_len = tensor([693], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 693])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=693
DEBUG: 转换后 enc_out.shape = torch.Size([1, 693, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 693, 512])
DEBUG: actual_T: 693, enc_len调整后: tensor([693], device='cuda:0')
[NeMo I 2025-08-20 09:47:08 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:08 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 175]), target_length: tensor([175], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 175])
[DEBUG] target_length = tensor([175], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 175
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 176]), dec_len=tensor([175], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 176]) -> torch.Size([1, 176, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 176, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 693, 512]), dec_out.shape=torch.Size([1, 176, 640])
[DEBUG] enc_len=tensor([693], device='cuda:0'), target_length=tensor([175], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([693], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 693, 512]), dec_out.shape: torch.Size([1, 176, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 693, 176, 1001])

Validation DataLoader 0:  21%|██        | 13/63 [00:00<00:02, 19.25it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 690]), enc_len = tensor([690], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 690])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=690
DEBUG: 转换后 enc_out.shape = torch.Size([1, 690, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 690, 512])
DEBUG: actual_T: 690, enc_len调整后: tensor([690], device='cuda:0')
[NeMo I 2025-08-20 09:47:08 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:08 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 176]), target_length: tensor([176], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 176])
[DEBUG] target_length = tensor([176], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 176
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 177]), dec_len=tensor([176], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 177]) -> torch.Size([1, 177, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 177, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 690, 512]), dec_out.shape=torch.Size([1, 177, 640])
[DEBUG] enc_len=tensor([690], device='cuda:0'), target_length=tensor([176], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([690], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 690, 512]), dec_out.shape: torch.Size([1, 177, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 690, 177, 1001])

Validation DataLoader 0:  22%|██▏       | 14/63 [00:00<00:02, 19.36it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 629]), enc_len = tensor([629], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 629])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=629
DEBUG: 转换后 enc_out.shape = torch.Size([1, 629, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 629, 512])
DEBUG: actual_T: 629, enc_len调整后: tensor([629], device='cuda:0')
[NeMo I 2025-08-20 09:47:09 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:09 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 178]), target_length: tensor([178], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 178])
[DEBUG] target_length = tensor([178], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 178
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 179]), dec_len=tensor([178], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 179]) -> torch.Size([1, 179, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 179, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 629, 512]), dec_out.shape=torch.Size([1, 179, 640])
[DEBUG] enc_len=tensor([629], device='cuda:0'), target_length=tensor([178], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([629], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 629, 512]), dec_out.shape: torch.Size([1, 179, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 629, 179, 1001])

Validation DataLoader 0:  24%|██▍       | 15/63 [00:00<00:02, 19.47it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 669]), enc_len = tensor([669], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 669])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=669
DEBUG: 转换后 enc_out.shape = torch.Size([1, 669, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 669, 512])
DEBUG: actual_T: 669, enc_len调整后: tensor([669], device='cuda:0')
[NeMo I 2025-08-20 09:47:09 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:09 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 156]), target_length: tensor([156], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 156])
[DEBUG] target_length = tensor([156], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 156
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 157]), dec_len=tensor([156], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 157]) -> torch.Size([1, 157, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 157, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 669, 512]), dec_out.shape=torch.Size([1, 157, 640])
[DEBUG] enc_len=tensor([669], device='cuda:0'), target_length=tensor([156], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([669], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 669, 512]), dec_out.shape: torch.Size([1, 157, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 669, 157, 1001])

Validation DataLoader 0:  25%|██▌       | 16/63 [00:00<00:02, 19.57it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 607]), enc_len = tensor([607], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 607])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=607
DEBUG: 转换后 enc_out.shape = torch.Size([1, 607, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 607, 512])
DEBUG: actual_T: 607, enc_len调整后: tensor([607], device='cuda:0')
[NeMo I 2025-08-20 09:47:09 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:09 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 166]), target_length: tensor([166], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 166])
[DEBUG] target_length = tensor([166], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 166
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 167]), dec_len=tensor([166], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 167]) -> torch.Size([1, 167, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 167, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 607, 512]), dec_out.shape=torch.Size([1, 167, 640])
[DEBUG] enc_len=tensor([607], device='cuda:0'), target_length=tensor([166], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([607], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 607, 512]), dec_out.shape: torch.Size([1, 167, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 607, 167, 1001])

Validation DataLoader 0:  27%|██▋       | 17/63 [00:00<00:02, 19.67it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 633]), enc_len = tensor([632], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 633])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=633
DEBUG: 转换后 enc_out.shape = torch.Size([1, 633, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 633, 512])
DEBUG: actual_T: 633, enc_len调整后: tensor([632], device='cuda:0')
[NeMo I 2025-08-20 09:47:09 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:09 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 162]), target_length: tensor([162], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 162])
[DEBUG] target_length = tensor([162], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 162
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 163]), dec_len=tensor([162], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 163]) -> torch.Size([1, 163, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 163, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 633, 512]), dec_out.shape=torch.Size([1, 163, 640])
[DEBUG] enc_len=tensor([632], device='cuda:0'), target_length=tensor([162], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([632], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 633, 512]), dec_out.shape: torch.Size([1, 163, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 633, 163, 1001])

Validation DataLoader 0:  29%|██▊       | 18/63 [00:00<00:02, 19.76it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 745]), enc_len = tensor([744], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 745])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=745
DEBUG: 转换后 enc_out.shape = torch.Size([1, 745, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 745, 512])
DEBUG: actual_T: 745, enc_len调整后: tensor([744], device='cuda:0')
[NeMo I 2025-08-20 09:47:09 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:09 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 171]), target_length: tensor([171], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 171])
[DEBUG] target_length = tensor([171], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 171
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 172]), dec_len=tensor([171], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 172]) -> torch.Size([1, 172, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 172, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 745, 512]), dec_out.shape=torch.Size([1, 172, 640])
[DEBUG] enc_len=tensor([744], device='cuda:0'), target_length=tensor([171], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([744], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 745, 512]), dec_out.shape: torch.Size([1, 172, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 745, 172, 1001])

Validation DataLoader 0:  30%|███       | 19/63 [00:00<00:02, 19.86it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 825]), enc_len = tensor([825], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 825])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=825
DEBUG: 转换后 enc_out.shape = torch.Size([1, 825, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 825, 512])
DEBUG: actual_T: 825, enc_len调整后: tensor([825], device='cuda:0')
[NeMo I 2025-08-20 09:47:09 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:09 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 168]), target_length: tensor([168], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 168])
[DEBUG] target_length = tensor([168], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 168
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 169]), dec_len=tensor([168], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 169]) -> torch.Size([1, 169, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 169, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 825, 512]), dec_out.shape=torch.Size([1, 169, 640])
[DEBUG] enc_len=tensor([825], device='cuda:0'), target_length=tensor([168], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([825], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 825, 512]), dec_out.shape: torch.Size([1, 169, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 825, 169, 1001])

Validation DataLoader 0:  32%|███▏      | 20/63 [00:01<00:02, 19.89it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 758]), enc_len = tensor([758], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 758])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=758
DEBUG: 转换后 enc_out.shape = torch.Size([1, 758, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 758, 512])
DEBUG: actual_T: 758, enc_len调整后: tensor([758], device='cuda:0')
[NeMo I 2025-08-20 09:47:09 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:09 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 193]), target_length: tensor([193], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 193])
[DEBUG] target_length = tensor([193], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 193
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 194]), dec_len=tensor([193], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 194]) -> torch.Size([1, 194, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 194, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 758, 512]), dec_out.shape=torch.Size([1, 194, 640])
[DEBUG] enc_len=tensor([758], device='cuda:0'), target_length=tensor([193], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([758], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 758, 512]), dec_out.shape: torch.Size([1, 194, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 758, 194, 1001])

Validation DataLoader 0:  33%|███▎      | 21/63 [00:01<00:02, 20.02it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 803]), enc_len = tensor([802], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 803])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=803
DEBUG: 转换后 enc_out.shape = torch.Size([1, 803, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 803, 512])
DEBUG: actual_T: 803, enc_len调整后: tensor([802], device='cuda:0')
[NeMo I 2025-08-20 09:47:09 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:09 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 177]), target_length: tensor([177], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 177])
[DEBUG] target_length = tensor([177], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 177
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 178]), dec_len=tensor([177], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 178]) -> torch.Size([1, 178, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 178, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 803, 512]), dec_out.shape=torch.Size([1, 178, 640])
[DEBUG] enc_len=tensor([802], device='cuda:0'), target_length=tensor([177], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([802], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 803, 512]), dec_out.shape: torch.Size([1, 178, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 803, 178, 1001])

Validation DataLoader 0:  35%|███▍      | 22/63 [00:01<00:02, 20.04it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 731]), enc_len = tensor([731], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 731])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=731
DEBUG: 转换后 enc_out.shape = torch.Size([1, 731, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 731, 512])
DEBUG: actual_T: 731, enc_len调整后: tensor([731], device='cuda:0')
[NeMo I 2025-08-20 09:47:09 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:09 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 176]), target_length: tensor([176], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 176])
[DEBUG] target_length = tensor([176], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 176
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 177]), dec_len=tensor([176], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 177]) -> torch.Size([1, 177, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 177, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 731, 512]), dec_out.shape=torch.Size([1, 177, 640])
[DEBUG] enc_len=tensor([731], device='cuda:0'), target_length=tensor([176], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([731], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 731, 512]), dec_out.shape: torch.Size([1, 177, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 731, 177, 1001])

Validation DataLoader 0:  37%|███▋      | 23/63 [00:01<00:01, 20.07it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 824]), enc_len = tensor([824], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 824])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=824
DEBUG: 转换后 enc_out.shape = torch.Size([1, 824, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 824, 512])
DEBUG: actual_T: 824, enc_len调整后: tensor([824], device='cuda:0')
[NeMo I 2025-08-20 09:47:09 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:09 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 181]), target_length: tensor([181], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 181])
[DEBUG] target_length = tensor([181], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 181
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 182]), dec_len=tensor([181], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 182]) -> torch.Size([1, 182, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 182, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 824, 512]), dec_out.shape=torch.Size([1, 182, 640])
[DEBUG] enc_len=tensor([824], device='cuda:0'), target_length=tensor([181], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([824], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 824, 512]), dec_out.shape: torch.Size([1, 182, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 824, 182, 1001])

Validation DataLoader 0:  38%|███▊      | 24/63 [00:01<00:01, 20.10it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 559]), enc_len = tensor([559], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 559])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=559
DEBUG: 转换后 enc_out.shape = torch.Size([1, 559, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 559, 512])
DEBUG: actual_T: 559, enc_len调整后: tensor([559], device='cuda:0')
[NeMo I 2025-08-20 09:47:09 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:09 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 128]), target_length: tensor([128], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 128])
[DEBUG] target_length = tensor([128], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 128
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 129]), dec_len=tensor([128], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 129]) -> torch.Size([1, 129, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 129, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 559, 512]), dec_out.shape=torch.Size([1, 129, 640])
[DEBUG] enc_len=tensor([559], device='cuda:0'), target_length=tensor([128], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([559], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 559, 512]), dec_out.shape: torch.Size([1, 129, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 559, 129, 1001])

Validation DataLoader 0:  40%|███▉      | 25/63 [00:01<00:01, 20.24it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 440]), enc_len = tensor([439], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 440])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=440
DEBUG: 转换后 enc_out.shape = torch.Size([1, 440, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 440, 512])
DEBUG: actual_T: 440, enc_len调整后: tensor([439], device='cuda:0')
[NeMo I 2025-08-20 09:47:09 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:09 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 92]), target_length: tensor([92], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 92])
[DEBUG] target_length = tensor([92], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 92
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 93]), dec_len=tensor([92], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 93]) -> torch.Size([1, 93, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 93, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 440, 512]), dec_out.shape=torch.Size([1, 93, 640])
[DEBUG] enc_len=tensor([439], device='cuda:0'), target_length=tensor([92], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([439], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 440, 512]), dec_out.shape: torch.Size([1, 93, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 440, 93, 1001])

Validation DataLoader 0:  41%|████▏     | 26/63 [00:01<00:01, 20.16it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 642]), enc_len = tensor([642], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 642])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=642
DEBUG: 转换后 enc_out.shape = torch.Size([1, 642, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 642, 512])
DEBUG: actual_T: 642, enc_len调整后: tensor([642], device='cuda:0')
[NeMo I 2025-08-20 09:47:09 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:09 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 145]), target_length: tensor([145], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 145])
[DEBUG] target_length = tensor([145], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 145
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 146]), dec_len=tensor([145], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 146]) -> torch.Size([1, 146, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 146, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 642, 512]), dec_out.shape=torch.Size([1, 146, 640])
[DEBUG] enc_len=tensor([642], device='cuda:0'), target_length=tensor([145], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([642], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 642, 512]), dec_out.shape: torch.Size([1, 146, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 642, 146, 1001])

Validation DataLoader 0:  43%|████▎     | 27/63 [00:01<00:01, 20.19it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 925]), enc_len = tensor([925], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 925])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=925
DEBUG: 转换后 enc_out.shape = torch.Size([1, 925, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 925, 512])
DEBUG: actual_T: 925, enc_len调整后: tensor([925], device='cuda:0')
[NeMo I 2025-08-20 09:47:09 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:09 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 227]), target_length: tensor([227], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 227])
[DEBUG] target_length = tensor([227], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 227
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 228]), dec_len=tensor([227], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 228]) -> torch.Size([1, 228, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 228, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 925, 512]), dec_out.shape=torch.Size([1, 228, 640])
[DEBUG] enc_len=tensor([925], device='cuda:0'), target_length=tensor([227], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([925], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 925, 512]), dec_out.shape: torch.Size([1, 228, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 925, 228, 1001])

Validation DataLoader 0:  44%|████▍     | 28/63 [00:01<00:01, 20.14it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 952]), enc_len = tensor([952], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 952])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=952
DEBUG: 转换后 enc_out.shape = torch.Size([1, 952, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 952, 512])
DEBUG: actual_T: 952, enc_len调整后: tensor([952], device='cuda:0')
[NeMo I 2025-08-20 09:47:09 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:09 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 233]), target_length: tensor([233], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 233])
[DEBUG] target_length = tensor([233], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 233
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 234]), dec_len=tensor([233], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 234]) -> torch.Size([1, 234, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 234, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 952, 512]), dec_out.shape=torch.Size([1, 234, 640])
[DEBUG] enc_len=tensor([952], device='cuda:0'), target_length=tensor([233], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([952], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 952, 512]), dec_out.shape: torch.Size([1, 234, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 952, 234, 1001])

Validation DataLoader 0:  46%|████▌     | 29/63 [00:01<00:01, 20.06it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 562]), enc_len = tensor([562], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 562])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=562
DEBUG: 转换后 enc_out.shape = torch.Size([1, 562, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 562, 512])
DEBUG: actual_T: 562, enc_len调整后: tensor([562], device='cuda:0')
[NeMo I 2025-08-20 09:47:09 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:09 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 139]), target_length: tensor([139], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 139])
[DEBUG] target_length = tensor([139], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 139
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 140]), dec_len=tensor([139], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 140]) -> torch.Size([1, 140, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 140, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 562, 512]), dec_out.shape=torch.Size([1, 140, 640])
[DEBUG] enc_len=tensor([562], device='cuda:0'), target_length=tensor([139], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([562], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 562, 512]), dec_out.shape: torch.Size([1, 140, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 562, 140, 1001])

Validation DataLoader 0:  48%|████▊     | 30/63 [00:01<00:01, 20.18it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 726]), enc_len = tensor([725], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 726])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=726
DEBUG: 转换后 enc_out.shape = torch.Size([1, 726, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 726, 512])
DEBUG: actual_T: 726, enc_len调整后: tensor([725], device='cuda:0')
[NeMo I 2025-08-20 09:47:09 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:09 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 178]), target_length: tensor([178], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 178])
[DEBUG] target_length = tensor([178], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 178
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 179]), dec_len=tensor([178], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 179]) -> torch.Size([1, 179, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 179, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 726, 512]), dec_out.shape=torch.Size([1, 179, 640])
[DEBUG] enc_len=tensor([725], device='cuda:0'), target_length=tensor([178], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([725], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 726, 512]), dec_out.shape: torch.Size([1, 179, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 726, 179, 1001])

Validation DataLoader 0:  49%|████▉     | 31/63 [00:01<00:01, 20.17it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 606]), enc_len = tensor([605], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 606])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=606
DEBUG: 转换后 enc_out.shape = torch.Size([1, 606, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 606, 512])
DEBUG: actual_T: 606, enc_len调整后: tensor([605], device='cuda:0')
[NeMo I 2025-08-20 09:47:09 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:09 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 144]), target_length: tensor([144], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 144])
[DEBUG] target_length = tensor([144], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 144
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 145]), dec_len=tensor([144], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 145]) -> torch.Size([1, 145, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 145, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 606, 512]), dec_out.shape=torch.Size([1, 145, 640])
[DEBUG] enc_len=tensor([605], device='cuda:0'), target_length=tensor([144], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([605], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 606, 512]), dec_out.shape: torch.Size([1, 145, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 606, 145, 1001])

Validation DataLoader 0:  51%|█████     | 32/63 [00:01<00:01, 20.19it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 468]), enc_len = tensor([468], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 468])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=468
DEBUG: 转换后 enc_out.shape = torch.Size([1, 468, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 468, 512])
DEBUG: actual_T: 468, enc_len调整后: tensor([468], device='cuda:0')
[NeMo I 2025-08-20 09:47:09 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:09 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 111]), target_length: tensor([111], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 111])
[DEBUG] target_length = tensor([111], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 111
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 112]), dec_len=tensor([111], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 112]) -> torch.Size([1, 112, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 112, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 468, 512]), dec_out.shape=torch.Size([1, 112, 640])
[DEBUG] enc_len=tensor([468], device='cuda:0'), target_length=tensor([111], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([468], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 468, 512]), dec_out.shape: torch.Size([1, 112, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 468, 112, 1001])

Validation DataLoader 0:  52%|█████▏    | 33/63 [00:01<00:01, 20.21it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 575]), enc_len = tensor([575], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 575])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=575
DEBUG: 转换后 enc_out.shape = torch.Size([1, 575, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 575, 512])
DEBUG: actual_T: 575, enc_len调整后: tensor([575], device='cuda:0')
[NeMo I 2025-08-20 09:47:09 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:09 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 114]), target_length: tensor([114], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 114])
[DEBUG] target_length = tensor([114], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 114
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 115]), dec_len=tensor([114], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 115]) -> torch.Size([1, 115, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 115, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 575, 512]), dec_out.shape=torch.Size([1, 115, 640])
[DEBUG] enc_len=tensor([575], device='cuda:0'), target_length=tensor([114], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([575], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 575, 512]), dec_out.shape: torch.Size([1, 115, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 575, 115, 1001])

Validation DataLoader 0:  54%|█████▍    | 34/63 [00:01<00:01, 20.31it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 787]), enc_len = tensor([787], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 787])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=787
DEBUG: 转换后 enc_out.shape = torch.Size([1, 787, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 787, 512])
DEBUG: actual_T: 787, enc_len调整后: tensor([787], device='cuda:0')
[NeMo I 2025-08-20 09:47:09 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:09 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 189]), target_length: tensor([189], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 189])
[DEBUG] target_length = tensor([189], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 189
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 190]), dec_len=tensor([189], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 190]) -> torch.Size([1, 190, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 190, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 787, 512]), dec_out.shape=torch.Size([1, 190, 640])
[DEBUG] enc_len=tensor([787], device='cuda:0'), target_length=tensor([189], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([787], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 787, 512]), dec_out.shape: torch.Size([1, 190, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 787, 190, 1001])

Validation DataLoader 0:  56%|█████▌    | 35/63 [00:01<00:01, 20.32it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 812]), enc_len = tensor([812], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 812])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=812
DEBUG: 转换后 enc_out.shape = torch.Size([1, 812, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 812, 512])
DEBUG: actual_T: 812, enc_len调整后: tensor([812], device='cuda:0')
[NeMo I 2025-08-20 09:47:10 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:10 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 205]), target_length: tensor([205], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 205])
[DEBUG] target_length = tensor([205], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 205
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 206]), dec_len=tensor([205], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 206]) -> torch.Size([1, 206, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 206, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 812, 512]), dec_out.shape=torch.Size([1, 206, 640])
[DEBUG] enc_len=tensor([812], device='cuda:0'), target_length=tensor([205], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([812], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 812, 512]), dec_out.shape: torch.Size([1, 206, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 812, 206, 1001])

Validation DataLoader 0:  57%|█████▋    | 36/63 [00:01<00:01, 20.30it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 549]), enc_len = tensor([549], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 549])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=549
DEBUG: 转换后 enc_out.shape = torch.Size([1, 549, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 549, 512])
DEBUG: actual_T: 549, enc_len调整后: tensor([549], device='cuda:0')
[NeMo I 2025-08-20 09:47:10 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:10 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 114]), target_length: tensor([114], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 114])
[DEBUG] target_length = tensor([114], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 114
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 115]), dec_len=tensor([114], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 115]) -> torch.Size([1, 115, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 115, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 549, 512]), dec_out.shape=torch.Size([1, 115, 640])
[DEBUG] enc_len=tensor([549], device='cuda:0'), target_length=tensor([114], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([549], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 549, 512]), dec_out.shape: torch.Size([1, 115, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 549, 115, 1001])

Validation DataLoader 0:  59%|█████▊    | 37/63 [00:01<00:01, 20.39it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 828]), enc_len = tensor([828], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 828])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=828
DEBUG: 转换后 enc_out.shape = torch.Size([1, 828, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 828, 512])
DEBUG: actual_T: 828, enc_len调整后: tensor([828], device='cuda:0')
[NeMo I 2025-08-20 09:47:10 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:10 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 199]), target_length: tensor([199], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 199])
[DEBUG] target_length = tensor([199], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 199
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 200]), dec_len=tensor([199], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 200]) -> torch.Size([1, 200, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 200, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 828, 512]), dec_out.shape=torch.Size([1, 200, 640])
[DEBUG] enc_len=tensor([828], device='cuda:0'), target_length=tensor([199], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([828], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 828, 512]), dec_out.shape: torch.Size([1, 200, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 828, 200, 1001])

Validation DataLoader 0:  60%|██████    | 38/63 [00:01<00:01, 20.34it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 731]), enc_len = tensor([731], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 731])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=731
DEBUG: 转换后 enc_out.shape = torch.Size([1, 731, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 731, 512])
DEBUG: actual_T: 731, enc_len调整后: tensor([731], device='cuda:0')
[NeMo I 2025-08-20 09:47:10 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:10 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 175]), target_length: tensor([175], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 175])
[DEBUG] target_length = tensor([175], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 175
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 176]), dec_len=tensor([175], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 176]) -> torch.Size([1, 176, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 176, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 731, 512]), dec_out.shape=torch.Size([1, 176, 640])
[DEBUG] enc_len=tensor([731], device='cuda:0'), target_length=tensor([175], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([731], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 731, 512]), dec_out.shape: torch.Size([1, 176, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 731, 176, 1001])

Validation DataLoader 0:  62%|██████▏   | 39/63 [00:01<00:01, 20.33it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 803]), enc_len = tensor([803], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 803])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=803
DEBUG: 转换后 enc_out.shape = torch.Size([1, 803, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 803, 512])
DEBUG: actual_T: 803, enc_len调整后: tensor([803], device='cuda:0')
[NeMo I 2025-08-20 09:47:10 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:10 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 202]), target_length: tensor([202], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 202])
[DEBUG] target_length = tensor([202], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 202
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 203]), dec_len=tensor([202], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 203]) -> torch.Size([1, 203, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 203, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 803, 512]), dec_out.shape=torch.Size([1, 203, 640])
[DEBUG] enc_len=tensor([803], device='cuda:0'), target_length=tensor([202], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([803], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 803, 512]), dec_out.shape: torch.Size([1, 203, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 803, 203, 1001])

Validation DataLoader 0:  63%|██████▎   | 40/63 [00:01<00:01, 20.36it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 773]), enc_len = tensor([773], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 773])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=773
DEBUG: 转换后 enc_out.shape = torch.Size([1, 773, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 773, 512])
DEBUG: actual_T: 773, enc_len调整后: tensor([773], device='cuda:0')
[NeMo I 2025-08-20 09:47:10 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:10 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 184]), target_length: tensor([184], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 184])
[DEBUG] target_length = tensor([184], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 184
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 185]), dec_len=tensor([184], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 185]) -> torch.Size([1, 185, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 185, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 773, 512]), dec_out.shape=torch.Size([1, 185, 640])
[DEBUG] enc_len=tensor([773], device='cuda:0'), target_length=tensor([184], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([773], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 773, 512]), dec_out.shape: torch.Size([1, 185, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 773, 185, 1001])

Validation DataLoader 0:  65%|██████▌   | 41/63 [00:02<00:01, 20.42it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 466]), enc_len = tensor([466], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 466])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=466
DEBUG: 转换后 enc_out.shape = torch.Size([1, 466, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 466, 512])
DEBUG: actual_T: 466, enc_len调整后: tensor([466], device='cuda:0')
[NeMo I 2025-08-20 09:47:10 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:10 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 119]), target_length: tensor([119], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 119])
[DEBUG] target_length = tensor([119], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 119
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 120]), dec_len=tensor([119], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 120]) -> torch.Size([1, 120, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 120, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 466, 512]), dec_out.shape=torch.Size([1, 120, 640])
[DEBUG] enc_len=tensor([466], device='cuda:0'), target_length=tensor([119], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([466], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 466, 512]), dec_out.shape: torch.Size([1, 120, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 466, 120, 1001])

Validation DataLoader 0:  67%|██████▋   | 42/63 [00:02<00:01, 20.45it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 877]), enc_len = tensor([877], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 877])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=877
DEBUG: 转换后 enc_out.shape = torch.Size([1, 877, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 877, 512])
DEBUG: actual_T: 877, enc_len调整后: tensor([877], device='cuda:0')
[NeMo I 2025-08-20 09:47:10 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:10 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 196]), target_length: tensor([196], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 196])
[DEBUG] target_length = tensor([196], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 196
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 197]), dec_len=tensor([196], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 197]) -> torch.Size([1, 197, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 197, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 877, 512]), dec_out.shape=torch.Size([1, 197, 640])
[DEBUG] enc_len=tensor([877], device='cuda:0'), target_length=tensor([196], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([877], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 877, 512]), dec_out.shape: torch.Size([1, 197, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 877, 197, 1001])

Validation DataLoader 0:  68%|██████▊   | 43/63 [00:02<00:00, 20.42it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 613]), enc_len = tensor([613], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 613])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=613
DEBUG: 转换后 enc_out.shape = torch.Size([1, 613, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 613, 512])
DEBUG: actual_T: 613, enc_len调整后: tensor([613], device='cuda:0')
[NeMo I 2025-08-20 09:47:10 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:10 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 129]), target_length: tensor([129], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 129])
[DEBUG] target_length = tensor([129], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 129
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 130]), dec_len=tensor([129], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 130]) -> torch.Size([1, 130, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 130, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 613, 512]), dec_out.shape=torch.Size([1, 130, 640])
[DEBUG] enc_len=tensor([613], device='cuda:0'), target_length=tensor([129], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([613], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 613, 512]), dec_out.shape: torch.Size([1, 130, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 613, 130, 1001])

Validation DataLoader 0:  70%|██████▉   | 44/63 [00:02<00:00, 20.44it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 807]), enc_len = tensor([807], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 807])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=807
DEBUG: 转换后 enc_out.shape = torch.Size([1, 807, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 807, 512])
DEBUG: actual_T: 807, enc_len调整后: tensor([807], device='cuda:0')
[NeMo I 2025-08-20 09:47:10 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:10 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 192]), target_length: tensor([192], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 192])
[DEBUG] target_length = tensor([192], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 192
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 193]), dec_len=tensor([192], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 193]) -> torch.Size([1, 193, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 193, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 807, 512]), dec_out.shape=torch.Size([1, 193, 640])
[DEBUG] enc_len=tensor([807], device='cuda:0'), target_length=tensor([192], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([807], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 807, 512]), dec_out.shape: torch.Size([1, 193, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 807, 193, 1001])

Validation DataLoader 0:  71%|███████▏  | 45/63 [00:02<00:00, 20.43it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 732]), enc_len = tensor([732], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 732])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=732
DEBUG: 转换后 enc_out.shape = torch.Size([1, 732, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 732, 512])
DEBUG: actual_T: 732, enc_len调整后: tensor([732], device='cuda:0')
[NeMo I 2025-08-20 09:47:10 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:10 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 145]), target_length: tensor([145], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 145])
[DEBUG] target_length = tensor([145], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 145
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 146]), dec_len=tensor([145], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 146]) -> torch.Size([1, 146, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 146, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 732, 512]), dec_out.shape=torch.Size([1, 146, 640])
[DEBUG] enc_len=tensor([732], device='cuda:0'), target_length=tensor([145], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([732], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 732, 512]), dec_out.shape: torch.Size([1, 146, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 732, 146, 1001])

Validation DataLoader 0:  73%|███████▎  | 46/63 [00:02<00:00, 20.44it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 754]), enc_len = tensor([754], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 754])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=754
DEBUG: 转换后 enc_out.shape = torch.Size([1, 754, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 754, 512])
DEBUG: actual_T: 754, enc_len调整后: tensor([754], device='cuda:0')
[NeMo I 2025-08-20 09:47:10 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:10 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 170]), target_length: tensor([170], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 170])
[DEBUG] target_length = tensor([170], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 170
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 171]), dec_len=tensor([170], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 171]) -> torch.Size([1, 171, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 171, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 754, 512]), dec_out.shape=torch.Size([1, 171, 640])
[DEBUG] enc_len=tensor([754], device='cuda:0'), target_length=tensor([170], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([754], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 754, 512]), dec_out.shape: torch.Size([1, 171, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 754, 171, 1001])

Validation DataLoader 0:  75%|███████▍  | 47/63 [00:02<00:00, 20.45it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 772]), enc_len = tensor([772], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 772])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=772
DEBUG: 转换后 enc_out.shape = torch.Size([1, 772, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 772, 512])
DEBUG: actual_T: 772, enc_len调整后: tensor([772], device='cuda:0')
[NeMo I 2025-08-20 09:47:10 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:10 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 188]), target_length: tensor([188], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 188])
[DEBUG] target_length = tensor([188], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 188
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 189]), dec_len=tensor([188], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 189]) -> torch.Size([1, 189, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 189, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 772, 512]), dec_out.shape=torch.Size([1, 189, 640])
[DEBUG] enc_len=tensor([772], device='cuda:0'), target_length=tensor([188], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([772], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 772, 512]), dec_out.shape: torch.Size([1, 189, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 772, 189, 1001])

Validation DataLoader 0:  76%|███████▌  | 48/63 [00:02<00:00, 20.45it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 585]), enc_len = tensor([584], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 585])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=585
DEBUG: 转换后 enc_out.shape = torch.Size([1, 585, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 585, 512])
DEBUG: actual_T: 585, enc_len调整后: tensor([584], device='cuda:0')
[NeMo I 2025-08-20 09:47:10 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:10 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 112]), target_length: tensor([112], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 112])
[DEBUG] target_length = tensor([112], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 112
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 113]), dec_len=tensor([112], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 113]) -> torch.Size([1, 113, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 113, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 585, 512]), dec_out.shape=torch.Size([1, 113, 640])
[DEBUG] enc_len=tensor([584], device='cuda:0'), target_length=tensor([112], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([584], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 585, 512]), dec_out.shape: torch.Size([1, 113, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 585, 113, 1001])

Validation DataLoader 0:  78%|███████▊  | 49/63 [00:02<00:00, 20.53it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 713]), enc_len = tensor([713], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 713])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=713
DEBUG: 转换后 enc_out.shape = torch.Size([1, 713, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 713, 512])
DEBUG: actual_T: 713, enc_len调整后: tensor([713], device='cuda:0')
[NeMo I 2025-08-20 09:47:10 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:10 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 162]), target_length: tensor([162], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 162])
[DEBUG] target_length = tensor([162], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 162
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 163]), dec_len=tensor([162], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 163]) -> torch.Size([1, 163, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 163, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 713, 512]), dec_out.shape=torch.Size([1, 163, 640])
[DEBUG] enc_len=tensor([713], device='cuda:0'), target_length=tensor([162], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([713], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 713, 512]), dec_out.shape: torch.Size([1, 163, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 713, 163, 1001])

Validation DataLoader 0:  79%|███████▉  | 50/63 [00:02<00:00, 20.54it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 570]), enc_len = tensor([570], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 570])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=570
DEBUG: 转换后 enc_out.shape = torch.Size([1, 570, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 570, 512])
DEBUG: actual_T: 570, enc_len调整后: tensor([570], device='cuda:0')
[NeMo I 2025-08-20 09:47:10 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:10 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 108]), target_length: tensor([108], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 108])
[DEBUG] target_length = tensor([108], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 108
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 109]), dec_len=tensor([108], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 109]) -> torch.Size([1, 109, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 109, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 570, 512]), dec_out.shape=torch.Size([1, 109, 640])
[DEBUG] enc_len=tensor([570], device='cuda:0'), target_length=tensor([108], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([570], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 570, 512]), dec_out.shape: torch.Size([1, 109, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 570, 109, 1001])

Validation DataLoader 0:  81%|████████  | 51/63 [00:02<00:00, 20.57it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 745]), enc_len = tensor([745], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 745])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=745
DEBUG: 转换后 enc_out.shape = torch.Size([1, 745, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 745, 512])
DEBUG: actual_T: 745, enc_len调整后: tensor([745], device='cuda:0')
[NeMo I 2025-08-20 09:47:10 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:10 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 185]), target_length: tensor([185], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 185])
[DEBUG] target_length = tensor([185], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 185
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 186]), dec_len=tensor([185], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 186]) -> torch.Size([1, 186, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 186, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 745, 512]), dec_out.shape=torch.Size([1, 186, 640])
[DEBUG] enc_len=tensor([745], device='cuda:0'), target_length=tensor([185], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([745], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 745, 512]), dec_out.shape: torch.Size([1, 186, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 745, 186, 1001])

Validation DataLoader 0:  83%|████████▎ | 52/63 [00:02<00:00, 20.58it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 973]), enc_len = tensor([973], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 973])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=973
DEBUG: 转换后 enc_out.shape = torch.Size([1, 973, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 973, 512])
DEBUG: actual_T: 973, enc_len调整后: tensor([973], device='cuda:0')
[NeMo I 2025-08-20 09:47:10 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:10 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 241]), target_length: tensor([241], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 241])
[DEBUG] target_length = tensor([241], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 241
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 242]), dec_len=tensor([241], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 242]) -> torch.Size([1, 242, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 242, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 973, 512]), dec_out.shape=torch.Size([1, 242, 640])
[DEBUG] enc_len=tensor([973], device='cuda:0'), target_length=tensor([241], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([973], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 973, 512]), dec_out.shape: torch.Size([1, 242, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 973, 242, 1001])

Validation DataLoader 0:  84%|████████▍ | 53/63 [00:02<00:00, 20.51it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 912]), enc_len = tensor([912], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 912])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=912
DEBUG: 转换后 enc_out.shape = torch.Size([1, 912, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 912, 512])
DEBUG: actual_T: 912, enc_len调整后: tensor([912], device='cuda:0')
[NeMo I 2025-08-20 09:47:10 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:10 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 225]), target_length: tensor([225], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 225])
[DEBUG] target_length = tensor([225], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 225
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 226]), dec_len=tensor([225], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 226]) -> torch.Size([1, 226, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 226, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 912, 512]), dec_out.shape=torch.Size([1, 226, 640])
[DEBUG] enc_len=tensor([912], device='cuda:0'), target_length=tensor([225], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([912], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 912, 512]), dec_out.shape: torch.Size([1, 226, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 912, 226, 1001])

Validation DataLoader 0:  86%|████████▌ | 54/63 [00:02<00:00, 20.53it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 869]), enc_len = tensor([869], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 869])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=869
DEBUG: 转换后 enc_out.shape = torch.Size([1, 869, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 869, 512])
DEBUG: actual_T: 869, enc_len调整后: tensor([869], device='cuda:0')
[NeMo I 2025-08-20 09:47:10 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:10 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 210]), target_length: tensor([210], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 210])
[DEBUG] target_length = tensor([210], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 210
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 211]), dec_len=tensor([210], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 211]) -> torch.Size([1, 211, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 211, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 869, 512]), dec_out.shape=torch.Size([1, 211, 640])
[DEBUG] enc_len=tensor([869], device='cuda:0'), target_length=tensor([210], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([869], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 869, 512]), dec_out.shape: torch.Size([1, 211, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 869, 211, 1001])

Validation DataLoader 0:  87%|████████▋ | 55/63 [00:02<00:00, 20.56it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 577]), enc_len = tensor([577], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 577])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=577
DEBUG: 转换后 enc_out.shape = torch.Size([1, 577, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 577, 512])
DEBUG: actual_T: 577, enc_len调整后: tensor([577], device='cuda:0')
[NeMo I 2025-08-20 09:47:10 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:10 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 153]), target_length: tensor([153], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 153])
[DEBUG] target_length = tensor([153], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 153
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 154]), dec_len=tensor([153], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 154]) -> torch.Size([1, 154, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 154, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 577, 512]), dec_out.shape=torch.Size([1, 154, 640])
[DEBUG] enc_len=tensor([577], device='cuda:0'), target_length=tensor([153], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([577], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 577, 512]), dec_out.shape: torch.Size([1, 154, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 577, 154, 1001])

Validation DataLoader 0:  89%|████████▉ | 56/63 [00:02<00:00, 20.59it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 912]), enc_len = tensor([912], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 912])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=912
DEBUG: 转换后 enc_out.shape = torch.Size([1, 912, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 912, 512])
DEBUG: actual_T: 912, enc_len调整后: tensor([912], device='cuda:0')
[NeMo I 2025-08-20 09:47:11 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:11 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 204]), target_length: tensor([204], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 204])
[DEBUG] target_length = tensor([204], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 204
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 205]), dec_len=tensor([204], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 205]) -> torch.Size([1, 205, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 205, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 912, 512]), dec_out.shape=torch.Size([1, 205, 640])
[DEBUG] enc_len=tensor([912], device='cuda:0'), target_length=tensor([204], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([912], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 912, 512]), dec_out.shape: torch.Size([1, 205, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 912, 205, 1001])

Validation DataLoader 0:  90%|█████████ | 57/63 [00:02<00:00, 20.62it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 522]), enc_len = tensor([522], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 522])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=522
DEBUG: 转换后 enc_out.shape = torch.Size([1, 522, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 522, 512])
DEBUG: actual_T: 522, enc_len调整后: tensor([522], device='cuda:0')
[NeMo I 2025-08-20 09:47:11 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:11 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 111]), target_length: tensor([111], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 111])
[DEBUG] target_length = tensor([111], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 111
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 112]), dec_len=tensor([111], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 112]) -> torch.Size([1, 112, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 112, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 522, 512]), dec_out.shape=torch.Size([1, 112, 640])
[DEBUG] enc_len=tensor([522], device='cuda:0'), target_length=tensor([111], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([522], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 522, 512]), dec_out.shape: torch.Size([1, 112, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 522, 112, 1001])

Validation DataLoader 0:  92%|█████████▏| 58/63 [00:02<00:00, 20.64it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 790]), enc_len = tensor([790], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 790])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=790
DEBUG: 转换后 enc_out.shape = torch.Size([1, 790, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 790, 512])
DEBUG: actual_T: 790, enc_len调整后: tensor([790], device='cuda:0')
[NeMo I 2025-08-20 09:47:11 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:11 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 213]), target_length: tensor([213], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 213])
[DEBUG] target_length = tensor([213], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 213
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 214]), dec_len=tensor([213], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 214]) -> torch.Size([1, 214, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 214, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 790, 512]), dec_out.shape=torch.Size([1, 214, 640])
[DEBUG] enc_len=tensor([790], device='cuda:0'), target_length=tensor([213], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([790], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 790, 512]), dec_out.shape: torch.Size([1, 214, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 790, 214, 1001])

Validation DataLoader 0:  94%|█████████▎| 59/63 [00:02<00:00, 20.63it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 1009]), enc_len = tensor([1008], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 1009])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=1009
DEBUG: 转换后 enc_out.shape = torch.Size([1, 1009, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 1009, 512])
DEBUG: actual_T: 1009, enc_len调整后: tensor([1008], device='cuda:0')
[NeMo I 2025-08-20 09:47:11 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:11 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 262]), target_length: tensor([262], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 262])
[DEBUG] target_length = tensor([262], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 262
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 263]), dec_len=tensor([262], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 263]) -> torch.Size([1, 263, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 263, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 1009, 512]), dec_out.shape=torch.Size([1, 263, 640])
[DEBUG] enc_len=tensor([1008], device='cuda:0'), target_length=tensor([262], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([1008], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 1009, 512]), dec_out.shape: torch.Size([1, 263, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 1009, 263, 1001])

Validation DataLoader 0:  95%|█████████▌| 60/63 [00:02<00:00, 20.56it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 383]), enc_len = tensor([382], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 383])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=383
DEBUG: 转换后 enc_out.shape = torch.Size([1, 383, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 383, 512])
DEBUG: actual_T: 383, enc_len调整后: tensor([382], device='cuda:0')
[NeMo I 2025-08-20 09:47:11 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:11 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 101]), target_length: tensor([101], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 101])
[DEBUG] target_length = tensor([101], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 101
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 102]), dec_len=tensor([101], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 102]) -> torch.Size([1, 102, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 102, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 383, 512]), dec_out.shape=torch.Size([1, 102, 640])
[DEBUG] enc_len=tensor([382], device='cuda:0'), target_length=tensor([101], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([382], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 383, 512]), dec_out.shape: torch.Size([1, 102, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 383, 102, 1001])

Validation DataLoader 0:  97%|█████████▋| 61/63 [00:02<00:00, 20.57it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 721]), enc_len = tensor([721], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 721])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=721
DEBUG: 转换后 enc_out.shape = torch.Size([1, 721, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 721, 512])
DEBUG: actual_T: 721, enc_len调整后: tensor([721], device='cuda:0')
[NeMo I 2025-08-20 09:47:11 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:11 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 135]), target_length: tensor([135], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 135])
[DEBUG] target_length = tensor([135], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 135
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 136]), dec_len=tensor([135], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 136]) -> torch.Size([1, 136, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 136, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 721, 512]), dec_out.shape=torch.Size([1, 136, 640])
[DEBUG] enc_len=tensor([721], device='cuda:0'), target_length=tensor([135], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([721], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 721, 512]), dec_out.shape: torch.Size([1, 136, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 721, 136, 1001])

Validation DataLoader 0:  98%|█████████▊| 62/63 [00:03<00:00, 20.60it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 833]), enc_len = tensor([833], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 833])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=833
DEBUG: 转换后 enc_out.shape = torch.Size([1, 833, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 833, 512])
DEBUG: actual_T: 833, enc_len调整后: tensor([833], device='cuda:0')
[NeMo I 2025-08-20 09:47:11 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:11 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 188]), target_length: tensor([188], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 188])
[DEBUG] target_length = tensor([188], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 188
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 189]), dec_len=tensor([188], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 189]) -> torch.Size([1, 189, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 189, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 833, 512]), dec_out.shape=torch.Size([1, 189, 640])
[DEBUG] enc_len=tensor([833], device='cuda:0'), target_length=tensor([188], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([833], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 833, 512]), dec_out.shape: torch.Size([1, 189, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 833, 189, 1001])

Validation DataLoader 0: 100%|██████████| 63/63 [00:03<00:00, 20.62it/s][AEpoch 0:  24%|██▍       | 15/63 [00:05<00:16,  2.93it/s, v_num=31, train_loss_step=4.48e+3, val_loss=5.36e+3]
                                                                        [ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 732]), enc_len = tensor([732], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 732])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=732
DEBUG: 转换后 enc_out.shape = torch.Size([1, 732, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 732, 512])
DEBUG: actual_T: 732, enc_len调整后: tensor([732], device='cuda:0')
[NeMo I 2025-08-20 09:47:12 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:12 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 145]), target_length: tensor([145], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 145])
[DEBUG] target_length = tensor([145], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 145
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 146]), dec_len=tensor([145], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 146]) -> torch.Size([1, 146, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 146, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 732, 512]), dec_out.shape=torch.Size([1, 146, 640])
[DEBUG] enc_len=tensor([732], device='cuda:0'), target_length=tensor([145], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([732], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 732, 512]), dec_out.shape: torch.Size([1, 146, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 732, 146, 1001])
Epoch 0:  25%|██▌       | 16/63 [00:05<00:17,  2.67it/s, v_num=31, train_loss_step=4.48e+3, val_loss=5.36e+3]Epoch 0:  25%|██▌       | 16/63 [00:05<00:17,  2.67it/s, v_num=31, train_loss_step=5.31e+3, val_loss=5.36e+3]DEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 690]), enc_len = tensor([690], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 690])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=690
DEBUG: 转换后 enc_out.shape = torch.Size([1, 690, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 690, 512])
DEBUG: actual_T: 690, enc_len调整后: tensor([690], device='cuda:0')
[NeMo I 2025-08-20 09:47:12 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:12 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 176]), target_length: tensor([176], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 176])
[DEBUG] target_length = tensor([176], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 176
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 177]), dec_len=tensor([176], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 177]) -> torch.Size([1, 177, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 177, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 690, 512]), dec_out.shape=torch.Size([1, 177, 640])
[DEBUG] enc_len=tensor([690], device='cuda:0'), target_length=tensor([176], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([690], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 690, 512]), dec_out.shape: torch.Size([1, 177, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 690, 177, 1001])
Epoch 0:  27%|██▋       | 17/63 [00:06<00:16,  2.80it/s, v_num=31, train_loss_step=5.31e+3, val_loss=5.36e+3]Epoch 0:  27%|██▋       | 17/63 [00:06<00:16,  2.80it/s, v_num=31, train_loss_step=5.04e+3, val_loss=5.36e+3]DEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 642]), enc_len = tensor([642], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 642])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=642
DEBUG: 转换后 enc_out.shape = torch.Size([1, 642, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 642, 512])
DEBUG: actual_T: 642, enc_len调整后: tensor([642], device='cuda:0')
[NeMo I 2025-08-20 09:47:12 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:12 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 145]), target_length: tensor([145], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 145])
[DEBUG] target_length = tensor([145], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 145
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 146]), dec_len=tensor([145], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 146]) -> torch.Size([1, 146, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 146, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 642, 512]), dec_out.shape=torch.Size([1, 146, 640])
[DEBUG] enc_len=tensor([642], device='cuda:0'), target_length=tensor([145], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([642], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 642, 512]), dec_out.shape: torch.Size([1, 146, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 642, 146, 1001])
Epoch 0:  29%|██▊       | 18/63 [00:06<00:15,  2.94it/s, v_num=31, train_loss_step=5.04e+3, val_loss=5.36e+3]Epoch 0:  29%|██▊       | 18/63 [00:06<00:15,  2.94it/s, v_num=31, train_loss_step=4.45e+3, val_loss=5.36e+3]DEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 803]), enc_len = tensor([803], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 803])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=803
DEBUG: 转换后 enc_out.shape = torch.Size([1, 803, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 803, 512])
DEBUG: actual_T: 803, enc_len调整后: tensor([803], device='cuda:0')
[NeMo I 2025-08-20 09:47:12 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:12 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 202]), target_length: tensor([202], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 202])
[DEBUG] target_length = tensor([202], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 202
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 203]), dec_len=tensor([202], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 203]) -> torch.Size([1, 203, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 203, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 803, 512]), dec_out.shape=torch.Size([1, 203, 640])
[DEBUG] enc_len=tensor([803], device='cuda:0'), target_length=tensor([202], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([803], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 803, 512]), dec_out.shape: torch.Size([1, 203, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 803, 203, 1001])
Epoch 0:  30%|███       | 19/63 [00:06<00:14,  3.06it/s, v_num=31, train_loss_step=4.45e+3, val_loss=5.36e+3]Epoch 0:  30%|███       | 19/63 [00:06<00:14,  3.06it/s, v_num=31, train_loss_step=5.53e+3, val_loss=5.36e+3]DEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 440]), enc_len = tensor([439], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 440])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=440
DEBUG: 转换后 enc_out.shape = torch.Size([1, 440, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 440, 512])
DEBUG: actual_T: 440, enc_len调整后: tensor([439], device='cuda:0')
[NeMo I 2025-08-20 09:47:12 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:12 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 92]), target_length: tensor([92], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 92])
[DEBUG] target_length = tensor([92], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 92
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 93]), dec_len=tensor([92], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 93]) -> torch.Size([1, 93, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 93, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 440, 512]), dec_out.shape=torch.Size([1, 93, 640])
[DEBUG] enc_len=tensor([439], device='cuda:0'), target_length=tensor([92], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([439], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 440, 512]), dec_out.shape: torch.Size([1, 93, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 440, 93, 1001])
Epoch 0:  32%|███▏      | 20/63 [00:06<00:13,  3.19it/s, v_num=31, train_loss_step=5.53e+3, val_loss=5.36e+3]Epoch 0:  32%|███▏      | 20/63 [00:06<00:13,  3.19it/s, v_num=31, train_loss_step=2.77e+3, val_loss=5.36e+3]DEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 825]), enc_len = tensor([825], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 825])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=825
DEBUG: 转换后 enc_out.shape = torch.Size([1, 825, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 825, 512])
DEBUG: actual_T: 825, enc_len调整后: tensor([825], device='cuda:0')
[NeMo I 2025-08-20 09:47:12 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:12 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 168]), target_length: tensor([168], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 168])
[DEBUG] target_length = tensor([168], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 168
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 169]), dec_len=tensor([168], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 169]) -> torch.Size([1, 169, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 169, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 825, 512]), dec_out.shape=torch.Size([1, 169, 640])
[DEBUG] enc_len=tensor([825], device='cuda:0'), target_length=tensor([168], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([825], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 825, 512]), dec_out.shape: torch.Size([1, 169, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 825, 169, 1001])
Epoch 0:  33%|███▎      | 21/63 [00:06<00:12,  3.31it/s, v_num=31, train_loss_step=2.77e+3, val_loss=5.36e+3]Epoch 0:  33%|███▎      | 21/63 [00:06<00:12,  3.31it/s, v_num=31, train_loss_step=4.8e+3, val_loss=5.36e+3] DEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 570]), enc_len = tensor([570], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 570])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=570
DEBUG: 转换后 enc_out.shape = torch.Size([1, 570, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 570, 512])
DEBUG: actual_T: 570, enc_len调整后: tensor([570], device='cuda:0')
[NeMo I 2025-08-20 09:47:12 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:12 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 108]), target_length: tensor([108], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 108])
[DEBUG] target_length = tensor([108], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 108
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 109]), dec_len=tensor([108], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 109]) -> torch.Size([1, 109, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 109, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 570, 512]), dec_out.shape=torch.Size([1, 109, 640])
[DEBUG] enc_len=tensor([570], device='cuda:0'), target_length=tensor([108], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([570], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 570, 512]), dec_out.shape: torch.Size([1, 109, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 570, 109, 1001])
Epoch 0:  35%|███▍      | 22/63 [00:06<00:11,  3.43it/s, v_num=31, train_loss_step=4.8e+3, val_loss=5.36e+3]Epoch 0:  35%|███▍      | 22/63 [00:06<00:11,  3.43it/s, v_num=31, train_loss_step=3.19e+3, val_loss=5.36e+3]DEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 803]), enc_len = tensor([802], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 803])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=803
DEBUG: 转换后 enc_out.shape = torch.Size([1, 803, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 803, 512])
DEBUG: actual_T: 803, enc_len调整后: tensor([802], device='cuda:0')
[NeMo I 2025-08-20 09:47:12 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:12 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 177]), target_length: tensor([177], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 177])
[DEBUG] target_length = tensor([177], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 177
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 178]), dec_len=tensor([177], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 178]) -> torch.Size([1, 178, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 178, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 803, 512]), dec_out.shape=torch.Size([1, 178, 640])
[DEBUG] enc_len=tensor([802], device='cuda:0'), target_length=tensor([177], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([802], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 803, 512]), dec_out.shape: torch.Size([1, 178, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 803, 178, 1001])
Epoch 0:  37%|███▋      | 23/63 [00:06<00:11,  3.55it/s, v_num=31, train_loss_step=3.19e+3, val_loss=5.36e+3]Epoch 0:  37%|███▋      | 23/63 [00:06<00:11,  3.55it/s, v_num=31, train_loss_step=3.98e+3, val_loss=5.36e+3]DEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 633]), enc_len = tensor([632], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 633])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=633
DEBUG: 转换后 enc_out.shape = torch.Size([1, 633, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 633, 512])
DEBUG: actual_T: 633, enc_len调整后: tensor([632], device='cuda:0')
[NeMo I 2025-08-20 09:47:12 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:12 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 162]), target_length: tensor([162], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 162])
[DEBUG] target_length = tensor([162], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 162
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 163]), dec_len=tensor([162], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 163]) -> torch.Size([1, 163, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 163, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 633, 512]), dec_out.shape=torch.Size([1, 163, 640])
[DEBUG] enc_len=tensor([632], device='cuda:0'), target_length=tensor([162], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([632], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 633, 512]), dec_out.shape: torch.Size([1, 163, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 633, 163, 1001])
Epoch 0:  38%|███▊      | 24/63 [00:06<00:10,  3.67it/s, v_num=31, train_loss_step=3.98e+3, val_loss=5.36e+3]Epoch 0:  38%|███▊      | 24/63 [00:06<00:10,  3.67it/s, v_num=31, train_loss_step=2.96e+3, val_loss=5.36e+3]DEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 693]), enc_len = tensor([693], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 693])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=693
DEBUG: 转换后 enc_out.shape = torch.Size([1, 693, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 693, 512])
DEBUG: actual_T: 693, enc_len调整后: tensor([693], device='cuda:0')
[NeMo I 2025-08-20 09:47:12 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:12 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 175]), target_length: tensor([175], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 175])
[DEBUG] target_length = tensor([175], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 175
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 176]), dec_len=tensor([175], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 176]) -> torch.Size([1, 176, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 176, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 693, 512]), dec_out.shape=torch.Size([1, 176, 640])
[DEBUG] enc_len=tensor([693], device='cuda:0'), target_length=tensor([175], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([693], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 693, 512]), dec_out.shape: torch.Size([1, 176, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 693, 176, 1001])
Epoch 0:  40%|███▉      | 25/63 [00:06<00:10,  3.77it/s, v_num=31, train_loss_step=2.96e+3, val_loss=5.36e+3]Epoch 0:  40%|███▉      | 25/63 [00:06<00:10,  3.77it/s, v_num=31, train_loss_step=3.02e+3, val_loss=5.36e+3]DEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 607]), enc_len = tensor([607], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 607])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=607
DEBUG: 转换后 enc_out.shape = torch.Size([1, 607, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 607, 512])
DEBUG: actual_T: 607, enc_len调整后: tensor([607], device='cuda:0')
[NeMo I 2025-08-20 09:47:12 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:12 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 166]), target_length: tensor([166], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 166])
[DEBUG] target_length = tensor([166], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 166
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 167]), dec_len=tensor([166], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 167]) -> torch.Size([1, 167, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 167, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 607, 512]), dec_out.shape=torch.Size([1, 167, 640])
[DEBUG] enc_len=tensor([607], device='cuda:0'), target_length=tensor([166], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([607], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 607, 512]), dec_out.shape: torch.Size([1, 167, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 607, 167, 1001])
Epoch 0:  41%|████▏     | 26/63 [00:06<00:09,  3.88it/s, v_num=31, train_loss_step=3.02e+3, val_loss=5.36e+3]Epoch 0:  41%|████▏     | 26/63 [00:06<00:09,  3.88it/s, v_num=31, train_loss_step=2.42e+3, val_loss=5.36e+3]DEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 952]), enc_len = tensor([952], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 952])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=952
DEBUG: 转换后 enc_out.shape = torch.Size([1, 952, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 952, 512])
DEBUG: actual_T: 952, enc_len调整后: tensor([952], device='cuda:0')
[NeMo I 2025-08-20 09:47:12 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:12 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 233]), target_length: tensor([233], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 233])
[DEBUG] target_length = tensor([233], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 233
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 234]), dec_len=tensor([233], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 234]) -> torch.Size([1, 234, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 234, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 952, 512]), dec_out.shape=torch.Size([1, 234, 640])
[DEBUG] enc_len=tensor([952], device='cuda:0'), target_length=tensor([233], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([952], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 952, 512]), dec_out.shape: torch.Size([1, 234, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 952, 234, 1001])
Epoch 0:  43%|████▎     | 27/63 [00:06<00:09,  3.98it/s, v_num=31, train_loss_step=2.42e+3, val_loss=5.36e+3]Epoch 0:  43%|████▎     | 27/63 [00:06<00:09,  3.98it/s, v_num=31, train_loss_step=3.25e+3, val_loss=5.36e+3]DEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 713]), enc_len = tensor([713], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 713])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=713
DEBUG: 转换后 enc_out.shape = torch.Size([1, 713, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 713, 512])
DEBUG: actual_T: 713, enc_len调整后: tensor([713], device='cuda:0')
[NeMo I 2025-08-20 09:47:13 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:13 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 162]), target_length: tensor([162], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 162])
[DEBUG] target_length = tensor([162], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 162
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 163]), dec_len=tensor([162], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 163]) -> torch.Size([1, 163, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 163, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 713, 512]), dec_out.shape=torch.Size([1, 163, 640])
[DEBUG] enc_len=tensor([713], device='cuda:0'), target_length=tensor([162], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([713], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 713, 512]), dec_out.shape: torch.Size([1, 163, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 713, 163, 1001])
Epoch 0:  44%|████▍     | 28/63 [00:06<00:08,  4.09it/s, v_num=31, train_loss_step=3.25e+3, val_loss=5.36e+3]Epoch 0:  44%|████▍     | 28/63 [00:06<00:08,  4.09it/s, v_num=31, train_loss_step=2.42e+3, val_loss=5.36e+3]DEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 925]), enc_len = tensor([925], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 925])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=925
DEBUG: 转换后 enc_out.shape = torch.Size([1, 925, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 925, 512])
DEBUG: actual_T: 925, enc_len调整后: tensor([925], device='cuda:0')
[NeMo I 2025-08-20 09:47:13 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:13 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 227]), target_length: tensor([227], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 227])
[DEBUG] target_length = tensor([227], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 227
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 228]), dec_len=tensor([227], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 228]) -> torch.Size([1, 228, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 228, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 925, 512]), dec_out.shape=torch.Size([1, 228, 640])
[DEBUG] enc_len=tensor([925], device='cuda:0'), target_length=tensor([227], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([925], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 925, 512]), dec_out.shape: torch.Size([1, 228, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 925, 228, 1001])
Epoch 0:  46%|████▌     | 29/63 [00:06<00:08,  4.18it/s, v_num=31, train_loss_step=2.42e+3, val_loss=5.36e+3]Epoch 0:  46%|████▌     | 29/63 [00:06<00:08,  4.18it/s, v_num=31, train_loss_step=2.64e+3, val_loss=5.36e+3]DEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 812]), enc_len = tensor([812], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 812])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=812
DEBUG: 转换后 enc_out.shape = torch.Size([1, 812, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 812, 512])
DEBUG: actual_T: 812, enc_len调整后: tensor([812], device='cuda:0')
[NeMo I 2025-08-20 09:47:13 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:13 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 205]), target_length: tensor([205], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 205])
[DEBUG] target_length = tensor([205], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 205
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 206]), dec_len=tensor([205], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 206]) -> torch.Size([1, 206, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 206, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 812, 512]), dec_out.shape=torch.Size([1, 206, 640])
[DEBUG] enc_len=tensor([812], device='cuda:0'), target_length=tensor([205], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([812], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 812, 512]), dec_out.shape: torch.Size([1, 206, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 812, 206, 1001])
Epoch 0:  48%|████▊     | 30/63 [00:07<00:07,  4.28it/s, v_num=31, train_loss_step=2.64e+3, val_loss=5.36e+3]Epoch 0:  48%|████▊     | 30/63 [00:07<00:07,  4.28it/s, v_num=31, train_loss_step=2.11e+3, val_loss=5.36e+3]
Validation: 0it [00:00, ?it/s][A
Validation:   0%|          | 0/63 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/63 [00:00<?, ?it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 684]), enc_len = tensor([684], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 684])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=684
DEBUG: 转换后 enc_out.shape = torch.Size([1, 684, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 684, 512])
DEBUG: actual_T: 684, enc_len调整后: tensor([684], device='cuda:0')
[NeMo I 2025-08-20 09:47:13 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:13 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 169]), target_length: tensor([169], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 169])
[DEBUG] target_length = tensor([169], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 169
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 170]), dec_len=tensor([169], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 170]) -> torch.Size([1, 170, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 170, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 684, 512]), dec_out.shape=torch.Size([1, 170, 640])
[DEBUG] enc_len=tensor([684], device='cuda:0'), target_length=tensor([169], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([684], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 684, 512]), dec_out.shape: torch.Size([1, 170, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 684, 170, 1001])

Validation DataLoader 0:   2%|▏         | 1/63 [00:00<00:03, 17.08it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 699]), enc_len = tensor([699], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 699])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=699
DEBUG: 转换后 enc_out.shape = torch.Size([1, 699, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 699, 512])
DEBUG: actual_T: 699, enc_len调整后: tensor([699], device='cuda:0')
[NeMo I 2025-08-20 09:47:13 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:13 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 175]), target_length: tensor([175], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 175])
[DEBUG] target_length = tensor([175], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 175
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 176]), dec_len=tensor([175], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 176]) -> torch.Size([1, 176, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 176, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 699, 512]), dec_out.shape=torch.Size([1, 176, 640])
[DEBUG] enc_len=tensor([699], device='cuda:0'), target_length=tensor([175], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([699], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 699, 512]), dec_out.shape: torch.Size([1, 176, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 699, 176, 1001])

Validation DataLoader 0:   3%|▎         | 2/63 [00:00<00:03, 18.89it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 887]), enc_len = tensor([887], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 887])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=887
DEBUG: 转换后 enc_out.shape = torch.Size([1, 887, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 887, 512])
DEBUG: actual_T: 887, enc_len调整后: tensor([887], device='cuda:0')
[NeMo I 2025-08-20 09:47:13 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:13 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 231]), target_length: tensor([231], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 231])
[DEBUG] target_length = tensor([231], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 231
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 232]), dec_len=tensor([231], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 232]) -> torch.Size([1, 232, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 232, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 887, 512]), dec_out.shape=torch.Size([1, 232, 640])
[DEBUG] enc_len=tensor([887], device='cuda:0'), target_length=tensor([231], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([887], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 887, 512]), dec_out.shape: torch.Size([1, 232, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 887, 232, 1001])

Validation DataLoader 0:   5%|▍         | 3/63 [00:00<00:03, 19.64it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 678]), enc_len = tensor([678], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 678])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=678
DEBUG: 转换后 enc_out.shape = torch.Size([1, 678, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 678, 512])
DEBUG: actual_T: 678, enc_len调整后: tensor([678], device='cuda:0')
[NeMo I 2025-08-20 09:47:13 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:13 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 145]), target_length: tensor([145], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 145])
[DEBUG] target_length = tensor([145], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 145
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 146]), dec_len=tensor([145], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 146]) -> torch.Size([1, 146, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 146, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 678, 512]), dec_out.shape=torch.Size([1, 146, 640])
[DEBUG] enc_len=tensor([678], device='cuda:0'), target_length=tensor([145], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([678], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 678, 512]), dec_out.shape: torch.Size([1, 146, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 678, 146, 1001])

Validation DataLoader 0:   6%|▋         | 4/63 [00:00<00:02, 20.60it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 677]), enc_len = tensor([676], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 677])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=677
DEBUG: 转换后 enc_out.shape = torch.Size([1, 677, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 677, 512])
DEBUG: actual_T: 677, enc_len调整后: tensor([676], device='cuda:0')
[NeMo I 2025-08-20 09:47:13 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:13 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 155]), target_length: tensor([155], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 155])
[DEBUG] target_length = tensor([155], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 155
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 156]), dec_len=tensor([155], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 156]) -> torch.Size([1, 156, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 156, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 677, 512]), dec_out.shape=torch.Size([1, 156, 640])
[DEBUG] enc_len=tensor([676], device='cuda:0'), target_length=tensor([155], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([676], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 677, 512]), dec_out.shape: torch.Size([1, 156, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 677, 156, 1001])

Validation DataLoader 0:   8%|▊         | 5/63 [00:00<00:02, 21.20it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 694]), enc_len = tensor([694], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 694])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=694
DEBUG: 转换后 enc_out.shape = torch.Size([1, 694, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 694, 512])
DEBUG: actual_T: 694, enc_len调整后: tensor([694], device='cuda:0')
[NeMo I 2025-08-20 09:47:13 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:13 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 165]), target_length: tensor([165], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 165])
[DEBUG] target_length = tensor([165], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 165
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 166]), dec_len=tensor([165], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 166]) -> torch.Size([1, 166, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 166, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 694, 512]), dec_out.shape=torch.Size([1, 166, 640])
[DEBUG] enc_len=tensor([694], device='cuda:0'), target_length=tensor([165], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([694], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 694, 512]), dec_out.shape: torch.Size([1, 166, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 694, 166, 1001])

Validation DataLoader 0:  10%|▉         | 6/63 [00:00<00:02, 21.18it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 615]), enc_len = tensor([615], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 615])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=615
DEBUG: 转换后 enc_out.shape = torch.Size([1, 615, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 615, 512])
DEBUG: actual_T: 615, enc_len调整后: tensor([615], device='cuda:0')
[NeMo I 2025-08-20 09:47:13 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:13 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 112]), target_length: tensor([112], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 112])
[DEBUG] target_length = tensor([112], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 112
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 113]), dec_len=tensor([112], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 113]) -> torch.Size([1, 113, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 113, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 615, 512]), dec_out.shape=torch.Size([1, 113, 640])
[DEBUG] enc_len=tensor([615], device='cuda:0'), target_length=tensor([112], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([615], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 615, 512]), dec_out.shape: torch.Size([1, 113, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 615, 113, 1001])

Validation DataLoader 0:  11%|█         | 7/63 [00:00<00:02, 21.62it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 966]), enc_len = tensor([966], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 966])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=966
DEBUG: 转换后 enc_out.shape = torch.Size([1, 966, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 966, 512])
DEBUG: actual_T: 966, enc_len调整后: tensor([966], device='cuda:0')
[NeMo I 2025-08-20 09:47:13 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:13 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 237]), target_length: tensor([237], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 237])
[DEBUG] target_length = tensor([237], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 237
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 238]), dec_len=tensor([237], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 238]) -> torch.Size([1, 238, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 238, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 966, 512]), dec_out.shape=torch.Size([1, 238, 640])
[DEBUG] enc_len=tensor([966], device='cuda:0'), target_length=tensor([237], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([966], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 966, 512]), dec_out.shape: torch.Size([1, 238, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 966, 238, 1001])

Validation DataLoader 0:  13%|█▎        | 8/63 [00:00<00:02, 21.56it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 809]), enc_len = tensor([809], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 809])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=809
DEBUG: 转换后 enc_out.shape = torch.Size([1, 809, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 809, 512])
DEBUG: actual_T: 809, enc_len调整后: tensor([809], device='cuda:0')
[NeMo I 2025-08-20 09:47:13 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:13 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 154]), target_length: tensor([154], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 154])
[DEBUG] target_length = tensor([154], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 154
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 155]), dec_len=tensor([154], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 155]) -> torch.Size([1, 155, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 155, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 809, 512]), dec_out.shape=torch.Size([1, 155, 640])
[DEBUG] enc_len=tensor([809], device='cuda:0'), target_length=tensor([154], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([809], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 809, 512]), dec_out.shape: torch.Size([1, 155, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 809, 155, 1001])

Validation DataLoader 0:  14%|█▍        | 9/63 [00:00<00:02, 21.64it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 820]), enc_len = tensor([820], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 820])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=820
DEBUG: 转换后 enc_out.shape = torch.Size([1, 820, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 820, 512])
DEBUG: actual_T: 820, enc_len调整后: tensor([820], device='cuda:0')
[NeMo I 2025-08-20 09:47:13 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:13 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 236]), target_length: tensor([236], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 236])
[DEBUG] target_length = tensor([236], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 236
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 237]), dec_len=tensor([236], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 237]) -> torch.Size([1, 237, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 237, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 820, 512]), dec_out.shape=torch.Size([1, 237, 640])
[DEBUG] enc_len=tensor([820], device='cuda:0'), target_length=tensor([236], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([820], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 820, 512]), dec_out.shape: torch.Size([1, 237, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 820, 237, 1001])

Validation DataLoader 0:  16%|█▌        | 10/63 [00:00<00:02, 21.58it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 537]), enc_len = tensor([537], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 537])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=537
DEBUG: 转换后 enc_out.shape = torch.Size([1, 537, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 537, 512])
DEBUG: actual_T: 537, enc_len调整后: tensor([537], device='cuda:0')
[NeMo I 2025-08-20 09:47:13 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:13 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 99]), target_length: tensor([99], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 99])
[DEBUG] target_length = tensor([99], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 99
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 100]), dec_len=tensor([99], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 100]) -> torch.Size([1, 100, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 100, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 537, 512]), dec_out.shape=torch.Size([1, 100, 640])
[DEBUG] enc_len=tensor([537], device='cuda:0'), target_length=tensor([99], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([537], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 537, 512]), dec_out.shape: torch.Size([1, 100, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 537, 100, 1001])

Validation DataLoader 0:  17%|█▋        | 11/63 [00:00<00:02, 21.79it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 848]), enc_len = tensor([848], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 848])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=848
DEBUG: 转换后 enc_out.shape = torch.Size([1, 848, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 848, 512])
DEBUG: actual_T: 848, enc_len调整后: tensor([848], device='cuda:0')
[NeMo I 2025-08-20 09:47:13 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:13 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 179]), target_length: tensor([179], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 179])
[DEBUG] target_length = tensor([179], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 179
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 180]), dec_len=tensor([179], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 180]) -> torch.Size([1, 180, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 180, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 848, 512]), dec_out.shape=torch.Size([1, 180, 640])
[DEBUG] enc_len=tensor([848], device='cuda:0'), target_length=tensor([179], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([848], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 848, 512]), dec_out.shape: torch.Size([1, 180, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 848, 180, 1001])

Validation DataLoader 0:  19%|█▉        | 12/63 [00:00<00:02, 21.84it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 693]), enc_len = tensor([693], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 693])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=693
DEBUG: 转换后 enc_out.shape = torch.Size([1, 693, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 693, 512])
DEBUG: actual_T: 693, enc_len调整后: tensor([693], device='cuda:0')
[NeMo I 2025-08-20 09:47:13 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:13 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 175]), target_length: tensor([175], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 175])
[DEBUG] target_length = tensor([175], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 175
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 176]), dec_len=tensor([175], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 176]) -> torch.Size([1, 176, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 176, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 693, 512]), dec_out.shape=torch.Size([1, 176, 640])
[DEBUG] enc_len=tensor([693], device='cuda:0'), target_length=tensor([175], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([693], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 693, 512]), dec_out.shape: torch.Size([1, 176, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 693, 176, 1001])

Validation DataLoader 0:  21%|██        | 13/63 [00:00<00:02, 21.95it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 690]), enc_len = tensor([690], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 690])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=690
DEBUG: 转换后 enc_out.shape = torch.Size([1, 690, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 690, 512])
DEBUG: actual_T: 690, enc_len调整后: tensor([690], device='cuda:0')
[NeMo I 2025-08-20 09:47:13 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:13 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 176]), target_length: tensor([176], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 176])
[DEBUG] target_length = tensor([176], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 176
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 177]), dec_len=tensor([176], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 177]) -> torch.Size([1, 177, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 177, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 690, 512]), dec_out.shape=torch.Size([1, 177, 640])
[DEBUG] enc_len=tensor([690], device='cuda:0'), target_length=tensor([176], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([690], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 690, 512]), dec_out.shape: torch.Size([1, 177, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 690, 177, 1001])

Validation DataLoader 0:  22%|██▏       | 14/63 [00:00<00:02, 22.06it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 629]), enc_len = tensor([629], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 629])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=629
DEBUG: 转换后 enc_out.shape = torch.Size([1, 629, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 629, 512])
DEBUG: actual_T: 629, enc_len调整后: tensor([629], device='cuda:0')
[NeMo I 2025-08-20 09:47:13 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:13 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 178]), target_length: tensor([178], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 178])
[DEBUG] target_length = tensor([178], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 178
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 179]), dec_len=tensor([178], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 179]) -> torch.Size([1, 179, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 179, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 629, 512]), dec_out.shape=torch.Size([1, 179, 640])
[DEBUG] enc_len=tensor([629], device='cuda:0'), target_length=tensor([178], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([629], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 629, 512]), dec_out.shape: torch.Size([1, 179, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 629, 179, 1001])

Validation DataLoader 0:  24%|██▍       | 15/63 [00:00<00:02, 22.15it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 669]), enc_len = tensor([669], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 669])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=669
DEBUG: 转换后 enc_out.shape = torch.Size([1, 669, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 669, 512])
DEBUG: actual_T: 669, enc_len调整后: tensor([669], device='cuda:0')
[NeMo I 2025-08-20 09:47:14 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:14 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 156]), target_length: tensor([156], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 156])
[DEBUG] target_length = tensor([156], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 156
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 157]), dec_len=tensor([156], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 157]) -> torch.Size([1, 157, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 157, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 669, 512]), dec_out.shape=torch.Size([1, 157, 640])
[DEBUG] enc_len=tensor([669], device='cuda:0'), target_length=tensor([156], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([669], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 669, 512]), dec_out.shape: torch.Size([1, 157, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 669, 157, 1001])

Validation DataLoader 0:  25%|██▌       | 16/63 [00:00<00:02, 22.24it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 607]), enc_len = tensor([607], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 607])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=607
DEBUG: 转换后 enc_out.shape = torch.Size([1, 607, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 607, 512])
DEBUG: actual_T: 607, enc_len调整后: tensor([607], device='cuda:0')
[NeMo I 2025-08-20 09:47:14 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:14 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 166]), target_length: tensor([166], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 166])
[DEBUG] target_length = tensor([166], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 166
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 167]), dec_len=tensor([166], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 167]) -> torch.Size([1, 167, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 167, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 607, 512]), dec_out.shape=torch.Size([1, 167, 640])
[DEBUG] enc_len=tensor([607], device='cuda:0'), target_length=tensor([166], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([607], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 607, 512]), dec_out.shape: torch.Size([1, 167, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 607, 167, 1001])

Validation DataLoader 0:  27%|██▋       | 17/63 [00:00<00:02, 22.33it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 633]), enc_len = tensor([632], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 633])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=633
DEBUG: 转换后 enc_out.shape = torch.Size([1, 633, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 633, 512])
DEBUG: actual_T: 633, enc_len调整后: tensor([632], device='cuda:0')
[NeMo I 2025-08-20 09:47:14 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:14 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 162]), target_length: tensor([162], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 162])
[DEBUG] target_length = tensor([162], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 162
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 163]), dec_len=tensor([162], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 163]) -> torch.Size([1, 163, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 163, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 633, 512]), dec_out.shape=torch.Size([1, 163, 640])
[DEBUG] enc_len=tensor([632], device='cuda:0'), target_length=tensor([162], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([632], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 633, 512]), dec_out.shape: torch.Size([1, 163, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 633, 163, 1001])

Validation DataLoader 0:  29%|██▊       | 18/63 [00:00<00:02, 22.41it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 745]), enc_len = tensor([744], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 745])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=745
DEBUG: 转换后 enc_out.shape = torch.Size([1, 745, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 745, 512])
DEBUG: actual_T: 745, enc_len调整后: tensor([744], device='cuda:0')
[NeMo I 2025-08-20 09:47:14 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:14 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 171]), target_length: tensor([171], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 171])
[DEBUG] target_length = tensor([171], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 171
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 172]), dec_len=tensor([171], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 172]) -> torch.Size([1, 172, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 172, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 745, 512]), dec_out.shape=torch.Size([1, 172, 640])
[DEBUG] enc_len=tensor([744], device='cuda:0'), target_length=tensor([171], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([744], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 745, 512]), dec_out.shape: torch.Size([1, 172, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 745, 172, 1001])

Validation DataLoader 0:  30%|███       | 19/63 [00:00<00:01, 22.46it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 825]), enc_len = tensor([825], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 825])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=825
DEBUG: 转换后 enc_out.shape = torch.Size([1, 825, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 825, 512])
DEBUG: actual_T: 825, enc_len调整后: tensor([825], device='cuda:0')
[NeMo I 2025-08-20 09:47:14 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:14 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 168]), target_length: tensor([168], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 168])
[DEBUG] target_length = tensor([168], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 168
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 169]), dec_len=tensor([168], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 169]) -> torch.Size([1, 169, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 169, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 825, 512]), dec_out.shape=torch.Size([1, 169, 640])
[DEBUG] enc_len=tensor([825], device='cuda:0'), target_length=tensor([168], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([825], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 825, 512]), dec_out.shape: torch.Size([1, 169, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 825, 169, 1001])

Validation DataLoader 0:  32%|███▏      | 20/63 [00:00<00:01, 22.50it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 758]), enc_len = tensor([758], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 758])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=758
DEBUG: 转换后 enc_out.shape = torch.Size([1, 758, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 758, 512])
DEBUG: actual_T: 758, enc_len调整后: tensor([758], device='cuda:0')
[NeMo I 2025-08-20 09:47:14 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:14 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 193]), target_length: tensor([193], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 193])
[DEBUG] target_length = tensor([193], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 193
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 194]), dec_len=tensor([193], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 194]) -> torch.Size([1, 194, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 194, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 758, 512]), dec_out.shape=torch.Size([1, 194, 640])
[DEBUG] enc_len=tensor([758], device='cuda:0'), target_length=tensor([193], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([758], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 758, 512]), dec_out.shape: torch.Size([1, 194, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 758, 194, 1001])

Validation DataLoader 0:  33%|███▎      | 21/63 [00:00<00:01, 22.53it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 803]), enc_len = tensor([802], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 803])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=803
DEBUG: 转换后 enc_out.shape = torch.Size([1, 803, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 803, 512])
DEBUG: actual_T: 803, enc_len调整后: tensor([802], device='cuda:0')
[NeMo I 2025-08-20 09:47:14 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:14 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 177]), target_length: tensor([177], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 177])
[DEBUG] target_length = tensor([177], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 177
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 178]), dec_len=tensor([177], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 178]) -> torch.Size([1, 178, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 178, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 803, 512]), dec_out.shape=torch.Size([1, 178, 640])
[DEBUG] enc_len=tensor([802], device='cuda:0'), target_length=tensor([177], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([802], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 803, 512]), dec_out.shape: torch.Size([1, 178, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 803, 178, 1001])

Validation DataLoader 0:  35%|███▍      | 22/63 [00:00<00:01, 22.53it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 731]), enc_len = tensor([731], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 731])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=731
DEBUG: 转换后 enc_out.shape = torch.Size([1, 731, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 731, 512])
DEBUG: actual_T: 731, enc_len调整后: tensor([731], device='cuda:0')
[NeMo I 2025-08-20 09:47:14 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:14 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 176]), target_length: tensor([176], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 176])
[DEBUG] target_length = tensor([176], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 176
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 177]), dec_len=tensor([176], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 177]) -> torch.Size([1, 177, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 177, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 731, 512]), dec_out.shape=torch.Size([1, 177, 640])
[DEBUG] enc_len=tensor([731], device='cuda:0'), target_length=tensor([176], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([731], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 731, 512]), dec_out.shape: torch.Size([1, 177, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 731, 177, 1001])

Validation DataLoader 0:  37%|███▋      | 23/63 [00:01<00:01, 22.52it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 824]), enc_len = tensor([824], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 824])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=824
DEBUG: 转换后 enc_out.shape = torch.Size([1, 824, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 824, 512])
DEBUG: actual_T: 824, enc_len调整后: tensor([824], device='cuda:0')
[NeMo I 2025-08-20 09:47:14 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:14 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 181]), target_length: tensor([181], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 181])
[DEBUG] target_length = tensor([181], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 181
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 182]), dec_len=tensor([181], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 182]) -> torch.Size([1, 182, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 182, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 824, 512]), dec_out.shape=torch.Size([1, 182, 640])
[DEBUG] enc_len=tensor([824], device='cuda:0'), target_length=tensor([181], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([824], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 824, 512]), dec_out.shape: torch.Size([1, 182, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 824, 182, 1001])

Validation DataLoader 0:  38%|███▊      | 24/63 [00:01<00:01, 22.54it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 559]), enc_len = tensor([559], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 559])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=559
DEBUG: 转换后 enc_out.shape = torch.Size([1, 559, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 559, 512])
DEBUG: actual_T: 559, enc_len调整后: tensor([559], device='cuda:0')
[NeMo I 2025-08-20 09:47:14 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:14 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 128]), target_length: tensor([128], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 128])
[DEBUG] target_length = tensor([128], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 128
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 129]), dec_len=tensor([128], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 129]) -> torch.Size([1, 129, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 129, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 559, 512]), dec_out.shape=torch.Size([1, 129, 640])
[DEBUG] enc_len=tensor([559], device='cuda:0'), target_length=tensor([128], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([559], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 559, 512]), dec_out.shape: torch.Size([1, 129, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 559, 129, 1001])

Validation DataLoader 0:  40%|███▉      | 25/63 [00:01<00:01, 22.62it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 440]), enc_len = tensor([439], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 440])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=440
DEBUG: 转换后 enc_out.shape = torch.Size([1, 440, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 440, 512])
DEBUG: actual_T: 440, enc_len调整后: tensor([439], device='cuda:0')
[NeMo I 2025-08-20 09:47:14 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:14 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 92]), target_length: tensor([92], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 92])
[DEBUG] target_length = tensor([92], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 92
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 93]), dec_len=tensor([92], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 93]) -> torch.Size([1, 93, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 93, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 440, 512]), dec_out.shape=torch.Size([1, 93, 640])
[DEBUG] enc_len=tensor([439], device='cuda:0'), target_length=tensor([92], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([439], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 440, 512]), dec_out.shape: torch.Size([1, 93, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 440, 93, 1001])

Validation DataLoader 0:  41%|████▏     | 26/63 [00:01<00:01, 22.71it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 642]), enc_len = tensor([642], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 642])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=642
DEBUG: 转换后 enc_out.shape = torch.Size([1, 642, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 642, 512])
DEBUG: actual_T: 642, enc_len调整后: tensor([642], device='cuda:0')
[NeMo I 2025-08-20 09:47:14 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:14 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 145]), target_length: tensor([145], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 145])
[DEBUG] target_length = tensor([145], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 145
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 146]), dec_len=tensor([145], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 146]) -> torch.Size([1, 146, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 146, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 642, 512]), dec_out.shape=torch.Size([1, 146, 640])
[DEBUG] enc_len=tensor([642], device='cuda:0'), target_length=tensor([145], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([642], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 642, 512]), dec_out.shape: torch.Size([1, 146, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 642, 146, 1001])

Validation DataLoader 0:  43%|████▎     | 27/63 [00:01<00:01, 22.76it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 925]), enc_len = tensor([925], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 925])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=925
DEBUG: 转换后 enc_out.shape = torch.Size([1, 925, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 925, 512])
DEBUG: actual_T: 925, enc_len调整后: tensor([925], device='cuda:0')
[NeMo I 2025-08-20 09:47:14 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:14 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 227]), target_length: tensor([227], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 227])
[DEBUG] target_length = tensor([227], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 227
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 228]), dec_len=tensor([227], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 228]) -> torch.Size([1, 228, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 228, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 925, 512]), dec_out.shape=torch.Size([1, 228, 640])
[DEBUG] enc_len=tensor([925], device='cuda:0'), target_length=tensor([227], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([925], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 925, 512]), dec_out.shape: torch.Size([1, 228, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 925, 228, 1001])

Validation DataLoader 0:  44%|████▍     | 28/63 [00:01<00:01, 22.71it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 952]), enc_len = tensor([952], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 952])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=952
DEBUG: 转换后 enc_out.shape = torch.Size([1, 952, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 952, 512])
DEBUG: actual_T: 952, enc_len调整后: tensor([952], device='cuda:0')
[NeMo I 2025-08-20 09:47:14 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:14 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 233]), target_length: tensor([233], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 233])
[DEBUG] target_length = tensor([233], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 233
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 234]), dec_len=tensor([233], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 234]) -> torch.Size([1, 234, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 234, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 952, 512]), dec_out.shape=torch.Size([1, 234, 640])
[DEBUG] enc_len=tensor([952], device='cuda:0'), target_length=tensor([233], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([952], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 952, 512]), dec_out.shape: torch.Size([1, 234, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 952, 234, 1001])

Validation DataLoader 0:  46%|████▌     | 29/63 [00:01<00:01, 22.66it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 562]), enc_len = tensor([562], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 562])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=562
DEBUG: 转换后 enc_out.shape = torch.Size([1, 562, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 562, 512])
DEBUG: actual_T: 562, enc_len调整后: tensor([562], device='cuda:0')
[NeMo I 2025-08-20 09:47:14 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:14 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 139]), target_length: tensor([139], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 139])
[DEBUG] target_length = tensor([139], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 139
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 140]), dec_len=tensor([139], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 140]) -> torch.Size([1, 140, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 140, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 562, 512]), dec_out.shape=torch.Size([1, 140, 640])
[DEBUG] enc_len=tensor([562], device='cuda:0'), target_length=tensor([139], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([562], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 562, 512]), dec_out.shape: torch.Size([1, 140, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 562, 140, 1001])

Validation DataLoader 0:  48%|████▊     | 30/63 [00:01<00:01, 22.67it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 726]), enc_len = tensor([725], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 726])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=726
DEBUG: 转换后 enc_out.shape = torch.Size([1, 726, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 726, 512])
DEBUG: actual_T: 726, enc_len调整后: tensor([725], device='cuda:0')
[NeMo I 2025-08-20 09:47:14 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:14 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 178]), target_length: tensor([178], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 178])
[DEBUG] target_length = tensor([178], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 178
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 179]), dec_len=tensor([178], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 179]) -> torch.Size([1, 179, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 179, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 726, 512]), dec_out.shape=torch.Size([1, 179, 640])
[DEBUG] enc_len=tensor([725], device='cuda:0'), target_length=tensor([178], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([725], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 726, 512]), dec_out.shape: torch.Size([1, 179, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 726, 179, 1001])

Validation DataLoader 0:  49%|████▉     | 31/63 [00:01<00:01, 22.69it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 606]), enc_len = tensor([605], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 606])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=606
DEBUG: 转换后 enc_out.shape = torch.Size([1, 606, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 606, 512])
DEBUG: actual_T: 606, enc_len调整后: tensor([605], device='cuda:0')
[NeMo I 2025-08-20 09:47:14 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:14 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 144]), target_length: tensor([144], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 144])
[DEBUG] target_length = tensor([144], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 144
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 145]), dec_len=tensor([144], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 145]) -> torch.Size([1, 145, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 145, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 606, 512]), dec_out.shape=torch.Size([1, 145, 640])
[DEBUG] enc_len=tensor([605], device='cuda:0'), target_length=tensor([144], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([605], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 606, 512]), dec_out.shape: torch.Size([1, 145, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 606, 145, 1001])

Validation DataLoader 0:  51%|█████     | 32/63 [00:01<00:01, 22.73it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 468]), enc_len = tensor([468], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 468])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=468
DEBUG: 转换后 enc_out.shape = torch.Size([1, 468, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 468, 512])
DEBUG: actual_T: 468, enc_len调整后: tensor([468], device='cuda:0')
[NeMo I 2025-08-20 09:47:14 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:14 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 111]), target_length: tensor([111], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 111])
[DEBUG] target_length = tensor([111], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 111
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 112]), dec_len=tensor([111], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 112]) -> torch.Size([1, 112, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 112, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 468, 512]), dec_out.shape=torch.Size([1, 112, 640])
[DEBUG] enc_len=tensor([468], device='cuda:0'), target_length=tensor([111], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([468], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 468, 512]), dec_out.shape: torch.Size([1, 112, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 468, 112, 1001])

Validation DataLoader 0:  52%|█████▏    | 33/63 [00:01<00:01, 22.80it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 575]), enc_len = tensor([575], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 575])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=575
DEBUG: 转换后 enc_out.shape = torch.Size([1, 575, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 575, 512])
DEBUG: actual_T: 575, enc_len调整后: tensor([575], device='cuda:0')
[NeMo I 2025-08-20 09:47:14 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:14 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 114]), target_length: tensor([114], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 114])
[DEBUG] target_length = tensor([114], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 114
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 115]), dec_len=tensor([114], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 115]) -> torch.Size([1, 115, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 115, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 575, 512]), dec_out.shape=torch.Size([1, 115, 640])
[DEBUG] enc_len=tensor([575], device='cuda:0'), target_length=tensor([114], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([575], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 575, 512]), dec_out.shape: torch.Size([1, 115, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 575, 115, 1001])

Validation DataLoader 0:  54%|█████▍    | 34/63 [00:01<00:01, 22.85it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 787]), enc_len = tensor([787], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 787])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=787
DEBUG: 转换后 enc_out.shape = torch.Size([1, 787, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 787, 512])
DEBUG: actual_T: 787, enc_len调整后: tensor([787], device='cuda:0')
[NeMo I 2025-08-20 09:47:14 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:14 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 189]), target_length: tensor([189], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 189])
[DEBUG] target_length = tensor([189], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 189
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 190]), dec_len=tensor([189], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 190]) -> torch.Size([1, 190, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 190, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 787, 512]), dec_out.shape=torch.Size([1, 190, 640])
[DEBUG] enc_len=tensor([787], device='cuda:0'), target_length=tensor([189], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([787], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 787, 512]), dec_out.shape: torch.Size([1, 190, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 787, 190, 1001])

Validation DataLoader 0:  56%|█████▌    | 35/63 [00:01<00:01, 22.84it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 812]), enc_len = tensor([812], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 812])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=812
DEBUG: 转换后 enc_out.shape = torch.Size([1, 812, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 812, 512])
DEBUG: actual_T: 812, enc_len调整后: tensor([812], device='cuda:0')
[NeMo I 2025-08-20 09:47:14 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:14 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 205]), target_length: tensor([205], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 205])
[DEBUG] target_length = tensor([205], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 205
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 206]), dec_len=tensor([205], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 206]) -> torch.Size([1, 206, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 206, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 812, 512]), dec_out.shape=torch.Size([1, 206, 640])
[DEBUG] enc_len=tensor([812], device='cuda:0'), target_length=tensor([205], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([812], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 812, 512]), dec_out.shape: torch.Size([1, 206, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 812, 206, 1001])

Validation DataLoader 0:  57%|█████▋    | 36/63 [00:01<00:01, 22.81it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 549]), enc_len = tensor([549], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 549])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=549
DEBUG: 转换后 enc_out.shape = torch.Size([1, 549, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 549, 512])
DEBUG: actual_T: 549, enc_len调整后: tensor([549], device='cuda:0')
[NeMo I 2025-08-20 09:47:14 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:14 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 114]), target_length: tensor([114], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 114])
[DEBUG] target_length = tensor([114], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 114
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 115]), dec_len=tensor([114], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 115]) -> torch.Size([1, 115, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 115, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 549, 512]), dec_out.shape=torch.Size([1, 115, 640])
[DEBUG] enc_len=tensor([549], device='cuda:0'), target_length=tensor([114], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([549], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 549, 512]), dec_out.shape: torch.Size([1, 115, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 549, 115, 1001])

Validation DataLoader 0:  59%|█████▊    | 37/63 [00:01<00:01, 22.86it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 828]), enc_len = tensor([828], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 828])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=828
DEBUG: 转换后 enc_out.shape = torch.Size([1, 828, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 828, 512])
DEBUG: actual_T: 828, enc_len调整后: tensor([828], device='cuda:0')
[NeMo I 2025-08-20 09:47:14 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:14 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 199]), target_length: tensor([199], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 199])
[DEBUG] target_length = tensor([199], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 199
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 200]), dec_len=tensor([199], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 200]) -> torch.Size([1, 200, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 200, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 828, 512]), dec_out.shape=torch.Size([1, 200, 640])
[DEBUG] enc_len=tensor([828], device='cuda:0'), target_length=tensor([199], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([828], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 828, 512]), dec_out.shape: torch.Size([1, 200, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 828, 200, 1001])

Validation DataLoader 0:  60%|██████    | 38/63 [00:01<00:01, 22.77it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 731]), enc_len = tensor([731], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 731])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=731
DEBUG: 转换后 enc_out.shape = torch.Size([1, 731, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 731, 512])
DEBUG: actual_T: 731, enc_len调整后: tensor([731], device='cuda:0')
[NeMo I 2025-08-20 09:47:15 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:15 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 175]), target_length: tensor([175], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 175])
[DEBUG] target_length = tensor([175], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 175
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 176]), dec_len=tensor([175], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 176]) -> torch.Size([1, 176, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 176, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 731, 512]), dec_out.shape=torch.Size([1, 176, 640])
[DEBUG] enc_len=tensor([731], device='cuda:0'), target_length=tensor([175], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([731], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 731, 512]), dec_out.shape: torch.Size([1, 176, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 731, 176, 1001])

Validation DataLoader 0:  62%|██████▏   | 39/63 [00:01<00:01, 22.72it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 803]), enc_len = tensor([803], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 803])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=803
DEBUG: 转换后 enc_out.shape = torch.Size([1, 803, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 803, 512])
DEBUG: actual_T: 803, enc_len调整后: tensor([803], device='cuda:0')
[NeMo I 2025-08-20 09:47:15 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:15 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 202]), target_length: tensor([202], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 202])
[DEBUG] target_length = tensor([202], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 202
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 203]), dec_len=tensor([202], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 203]) -> torch.Size([1, 203, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 203, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 803, 512]), dec_out.shape=torch.Size([1, 203, 640])
[DEBUG] enc_len=tensor([803], device='cuda:0'), target_length=tensor([202], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([803], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 803, 512]), dec_out.shape: torch.Size([1, 203, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 803, 203, 1001])

Validation DataLoader 0:  63%|██████▎   | 40/63 [00:01<00:01, 22.67it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 773]), enc_len = tensor([773], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 773])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=773
DEBUG: 转换后 enc_out.shape = torch.Size([1, 773, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 773, 512])
DEBUG: actual_T: 773, enc_len调整后: tensor([773], device='cuda:0')
[NeMo I 2025-08-20 09:47:15 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:15 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 184]), target_length: tensor([184], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 184])
[DEBUG] target_length = tensor([184], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 184
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 185]), dec_len=tensor([184], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 185]) -> torch.Size([1, 185, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 185, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 773, 512]), dec_out.shape=torch.Size([1, 185, 640])
[DEBUG] enc_len=tensor([773], device='cuda:0'), target_length=tensor([184], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([773], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 773, 512]), dec_out.shape: torch.Size([1, 185, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 773, 185, 1001])

Validation DataLoader 0:  65%|██████▌   | 41/63 [00:01<00:00, 22.61it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 466]), enc_len = tensor([466], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 466])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=466
DEBUG: 转换后 enc_out.shape = torch.Size([1, 466, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 466, 512])
DEBUG: actual_T: 466, enc_len调整后: tensor([466], device='cuda:0')
[NeMo I 2025-08-20 09:47:15 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:15 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 119]), target_length: tensor([119], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 119])
[DEBUG] target_length = tensor([119], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 119
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 120]), dec_len=tensor([119], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 120]) -> torch.Size([1, 120, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 120, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 466, 512]), dec_out.shape=torch.Size([1, 120, 640])
[DEBUG] enc_len=tensor([466], device='cuda:0'), target_length=tensor([119], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([466], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 466, 512]), dec_out.shape: torch.Size([1, 120, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 466, 120, 1001])

Validation DataLoader 0:  67%|██████▋   | 42/63 [00:01<00:00, 22.65it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 877]), enc_len = tensor([877], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 877])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=877
DEBUG: 转换后 enc_out.shape = torch.Size([1, 877, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 877, 512])
DEBUG: actual_T: 877, enc_len调整后: tensor([877], device='cuda:0')
[NeMo I 2025-08-20 09:47:15 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:15 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 196]), target_length: tensor([196], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 196])
[DEBUG] target_length = tensor([196], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 196
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 197]), dec_len=tensor([196], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 197]) -> torch.Size([1, 197, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 197, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 877, 512]), dec_out.shape=torch.Size([1, 197, 640])
[DEBUG] enc_len=tensor([877], device='cuda:0'), target_length=tensor([196], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([877], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 877, 512]), dec_out.shape: torch.Size([1, 197, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 877, 197, 1001])

Validation DataLoader 0:  68%|██████▊   | 43/63 [00:01<00:00, 22.64it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 613]), enc_len = tensor([613], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 613])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=613
DEBUG: 转换后 enc_out.shape = torch.Size([1, 613, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 613, 512])
DEBUG: actual_T: 613, enc_len调整后: tensor([613], device='cuda:0')
[NeMo I 2025-08-20 09:47:15 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:15 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 129]), target_length: tensor([129], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 129])
[DEBUG] target_length = tensor([129], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 129
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 130]), dec_len=tensor([129], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 130]) -> torch.Size([1, 130, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 130, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 613, 512]), dec_out.shape=torch.Size([1, 130, 640])
[DEBUG] enc_len=tensor([613], device='cuda:0'), target_length=tensor([129], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([613], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 613, 512]), dec_out.shape: torch.Size([1, 130, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 613, 130, 1001])

Validation DataLoader 0:  70%|██████▉   | 44/63 [00:01<00:00, 22.68it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 807]), enc_len = tensor([807], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 807])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=807
DEBUG: 转换后 enc_out.shape = torch.Size([1, 807, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 807, 512])
DEBUG: actual_T: 807, enc_len调整后: tensor([807], device='cuda:0')
[NeMo I 2025-08-20 09:47:15 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:15 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 192]), target_length: tensor([192], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 192])
[DEBUG] target_length = tensor([192], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 192
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 193]), dec_len=tensor([192], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 193]) -> torch.Size([1, 193, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 193, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 807, 512]), dec_out.shape=torch.Size([1, 193, 640])
[DEBUG] enc_len=tensor([807], device='cuda:0'), target_length=tensor([192], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([807], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 807, 512]), dec_out.shape: torch.Size([1, 193, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 807, 193, 1001])

Validation DataLoader 0:  71%|███████▏  | 45/63 [00:01<00:00, 22.68it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 732]), enc_len = tensor([732], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 732])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=732
DEBUG: 转换后 enc_out.shape = torch.Size([1, 732, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 732, 512])
DEBUG: actual_T: 732, enc_len调整后: tensor([732], device='cuda:0')
[NeMo I 2025-08-20 09:47:15 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:15 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 145]), target_length: tensor([145], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 145])
[DEBUG] target_length = tensor([145], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 145
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 146]), dec_len=tensor([145], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 146]) -> torch.Size([1, 146, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 146, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 732, 512]), dec_out.shape=torch.Size([1, 146, 640])
[DEBUG] enc_len=tensor([732], device='cuda:0'), target_length=tensor([145], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([732], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 732, 512]), dec_out.shape: torch.Size([1, 146, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 732, 146, 1001])

Validation DataLoader 0:  73%|███████▎  | 46/63 [00:02<00:00, 22.70it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 754]), enc_len = tensor([754], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 754])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=754
DEBUG: 转换后 enc_out.shape = torch.Size([1, 754, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 754, 512])
DEBUG: actual_T: 754, enc_len调整后: tensor([754], device='cuda:0')
[NeMo I 2025-08-20 09:47:15 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:15 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 170]), target_length: tensor([170], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 170])
[DEBUG] target_length = tensor([170], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 170
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 171]), dec_len=tensor([170], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 171]) -> torch.Size([1, 171, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 171, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 754, 512]), dec_out.shape=torch.Size([1, 171, 640])
[DEBUG] enc_len=tensor([754], device='cuda:0'), target_length=tensor([170], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([754], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 754, 512]), dec_out.shape: torch.Size([1, 171, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 754, 171, 1001])

Validation DataLoader 0:  75%|███████▍  | 47/63 [00:02<00:00, 22.71it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 772]), enc_len = tensor([772], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 772])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=772
DEBUG: 转换后 enc_out.shape = torch.Size([1, 772, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 772, 512])
DEBUG: actual_T: 772, enc_len调整后: tensor([772], device='cuda:0')
[NeMo I 2025-08-20 09:47:15 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:15 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 188]), target_length: tensor([188], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 188])
[DEBUG] target_length = tensor([188], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 188
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 189]), dec_len=tensor([188], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 189]) -> torch.Size([1, 189, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 189, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 772, 512]), dec_out.shape=torch.Size([1, 189, 640])
[DEBUG] enc_len=tensor([772], device='cuda:0'), target_length=tensor([188], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([772], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 772, 512]), dec_out.shape: torch.Size([1, 189, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 772, 189, 1001])

Validation DataLoader 0:  76%|███████▌  | 48/63 [00:02<00:00, 22.71it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 585]), enc_len = tensor([584], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 585])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=585
DEBUG: 转换后 enc_out.shape = torch.Size([1, 585, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 585, 512])
DEBUG: actual_T: 585, enc_len调整后: tensor([584], device='cuda:0')
[NeMo I 2025-08-20 09:47:15 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:15 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 112]), target_length: tensor([112], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 112])
[DEBUG] target_length = tensor([112], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 112
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 113]), dec_len=tensor([112], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 113]) -> torch.Size([1, 113, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 113, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 585, 512]), dec_out.shape=torch.Size([1, 113, 640])
[DEBUG] enc_len=tensor([584], device='cuda:0'), target_length=tensor([112], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([584], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 585, 512]), dec_out.shape: torch.Size([1, 113, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 585, 113, 1001])

Validation DataLoader 0:  78%|███████▊  | 49/63 [00:02<00:00, 22.75it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 713]), enc_len = tensor([713], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 713])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=713
DEBUG: 转换后 enc_out.shape = torch.Size([1, 713, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 713, 512])
DEBUG: actual_T: 713, enc_len调整后: tensor([713], device='cuda:0')
[NeMo I 2025-08-20 09:47:15 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:15 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 162]), target_length: tensor([162], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 162])
[DEBUG] target_length = tensor([162], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 162
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 163]), dec_len=tensor([162], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 163]) -> torch.Size([1, 163, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 163, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 713, 512]), dec_out.shape=torch.Size([1, 163, 640])
[DEBUG] enc_len=tensor([713], device='cuda:0'), target_length=tensor([162], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([713], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 713, 512]), dec_out.shape: torch.Size([1, 163, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 713, 163, 1001])

Validation DataLoader 0:  79%|███████▉  | 50/63 [00:02<00:00, 22.77it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 570]), enc_len = tensor([570], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 570])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=570
DEBUG: 转换后 enc_out.shape = torch.Size([1, 570, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 570, 512])
DEBUG: actual_T: 570, enc_len调整后: tensor([570], device='cuda:0')
[NeMo I 2025-08-20 09:47:15 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:15 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 108]), target_length: tensor([108], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 108])
[DEBUG] target_length = tensor([108], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 108
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 109]), dec_len=tensor([108], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 109]) -> torch.Size([1, 109, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 109, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 570, 512]), dec_out.shape=torch.Size([1, 109, 640])
[DEBUG] enc_len=tensor([570], device='cuda:0'), target_length=tensor([108], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([570], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 570, 512]), dec_out.shape: torch.Size([1, 109, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 570, 109, 1001])

Validation DataLoader 0:  81%|████████  | 51/63 [00:02<00:00, 22.82it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 745]), enc_len = tensor([745], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 745])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=745
DEBUG: 转换后 enc_out.shape = torch.Size([1, 745, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 745, 512])
DEBUG: actual_T: 745, enc_len调整后: tensor([745], device='cuda:0')
[NeMo I 2025-08-20 09:47:15 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:15 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 185]), target_length: tensor([185], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 185])
[DEBUG] target_length = tensor([185], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 185
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 186]), dec_len=tensor([185], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 186]) -> torch.Size([1, 186, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 186, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 745, 512]), dec_out.shape=torch.Size([1, 186, 640])
[DEBUG] enc_len=tensor([745], device='cuda:0'), target_length=tensor([185], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([745], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 745, 512]), dec_out.shape: torch.Size([1, 186, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 745, 186, 1001])

Validation DataLoader 0:  83%|████████▎ | 52/63 [00:02<00:00, 22.83it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 973]), enc_len = tensor([973], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 973])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=973
DEBUG: 转换后 enc_out.shape = torch.Size([1, 973, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 973, 512])
DEBUG: actual_T: 973, enc_len调整后: tensor([973], device='cuda:0')
[NeMo I 2025-08-20 09:47:15 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:15 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 241]), target_length: tensor([241], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 241])
[DEBUG] target_length = tensor([241], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 241
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 242]), dec_len=tensor([241], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 242]) -> torch.Size([1, 242, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 242, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 973, 512]), dec_out.shape=torch.Size([1, 242, 640])
[DEBUG] enc_len=tensor([973], device='cuda:0'), target_length=tensor([241], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([973], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 973, 512]), dec_out.shape: torch.Size([1, 242, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 973, 242, 1001])

Validation DataLoader 0:  84%|████████▍ | 53/63 [00:02<00:00, 22.79it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 912]), enc_len = tensor([912], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 912])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=912
DEBUG: 转换后 enc_out.shape = torch.Size([1, 912, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 912, 512])
DEBUG: actual_T: 912, enc_len调整后: tensor([912], device='cuda:0')
[NeMo I 2025-08-20 09:47:15 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:15 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 225]), target_length: tensor([225], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 225])
[DEBUG] target_length = tensor([225], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 225
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 226]), dec_len=tensor([225], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 226]) -> torch.Size([1, 226, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 226, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 912, 512]), dec_out.shape=torch.Size([1, 226, 640])
[DEBUG] enc_len=tensor([912], device='cuda:0'), target_length=tensor([225], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([912], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 912, 512]), dec_out.shape: torch.Size([1, 226, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 912, 226, 1001])

Validation DataLoader 0:  86%|████████▌ | 54/63 [00:02<00:00, 22.78it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 869]), enc_len = tensor([869], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 869])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=869
DEBUG: 转换后 enc_out.shape = torch.Size([1, 869, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 869, 512])
DEBUG: actual_T: 869, enc_len调整后: tensor([869], device='cuda:0')
[NeMo I 2025-08-20 09:47:15 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:15 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 210]), target_length: tensor([210], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 210])
[DEBUG] target_length = tensor([210], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 210
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 211]), dec_len=tensor([210], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 211]) -> torch.Size([1, 211, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 211, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 869, 512]), dec_out.shape=torch.Size([1, 211, 640])
[DEBUG] enc_len=tensor([869], device='cuda:0'), target_length=tensor([210], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([869], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 869, 512]), dec_out.shape: torch.Size([1, 211, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 869, 211, 1001])

Validation DataLoader 0:  87%|████████▋ | 55/63 [00:02<00:00, 22.77it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 577]), enc_len = tensor([577], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 577])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=577
DEBUG: 转换后 enc_out.shape = torch.Size([1, 577, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 577, 512])
DEBUG: actual_T: 577, enc_len调整后: tensor([577], device='cuda:0')
[NeMo I 2025-08-20 09:47:15 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:15 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 153]), target_length: tensor([153], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 153])
[DEBUG] target_length = tensor([153], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 153
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 154]), dec_len=tensor([153], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 154]) -> torch.Size([1, 154, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 154, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 577, 512]), dec_out.shape=torch.Size([1, 154, 640])
[DEBUG] enc_len=tensor([577], device='cuda:0'), target_length=tensor([153], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([577], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 577, 512]), dec_out.shape: torch.Size([1, 154, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 577, 154, 1001])

Validation DataLoader 0:  89%|████████▉ | 56/63 [00:02<00:00, 22.80it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 912]), enc_len = tensor([912], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 912])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=912
DEBUG: 转换后 enc_out.shape = torch.Size([1, 912, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 912, 512])
DEBUG: actual_T: 912, enc_len调整后: tensor([912], device='cuda:0')
[NeMo I 2025-08-20 09:47:15 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:15 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 204]), target_length: tensor([204], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 204])
[DEBUG] target_length = tensor([204], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 204
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 205]), dec_len=tensor([204], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 205]) -> torch.Size([1, 205, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 205, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 912, 512]), dec_out.shape=torch.Size([1, 205, 640])
[DEBUG] enc_len=tensor([912], device='cuda:0'), target_length=tensor([204], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([912], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 912, 512]), dec_out.shape: torch.Size([1, 205, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 912, 205, 1001])

Validation DataLoader 0:  90%|█████████ | 57/63 [00:02<00:00, 22.79it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 522]), enc_len = tensor([522], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 522])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=522
DEBUG: 转换后 enc_out.shape = torch.Size([1, 522, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 522, 512])
DEBUG: actual_T: 522, enc_len调整后: tensor([522], device='cuda:0')
[NeMo I 2025-08-20 09:47:15 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:15 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 111]), target_length: tensor([111], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 111])
[DEBUG] target_length = tensor([111], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 111
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 112]), dec_len=tensor([111], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 112]) -> torch.Size([1, 112, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 112, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 522, 512]), dec_out.shape=torch.Size([1, 112, 640])
[DEBUG] enc_len=tensor([522], device='cuda:0'), target_length=tensor([111], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([522], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 522, 512]), dec_out.shape: torch.Size([1, 112, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 522, 112, 1001])

Validation DataLoader 0:  92%|█████████▏| 58/63 [00:02<00:00, 22.83it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 790]), enc_len = tensor([790], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 790])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=790
DEBUG: 转换后 enc_out.shape = torch.Size([1, 790, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 790, 512])
DEBUG: actual_T: 790, enc_len调整后: tensor([790], device='cuda:0')
[NeMo I 2025-08-20 09:47:15 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:15 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 213]), target_length: tensor([213], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 213])
[DEBUG] target_length = tensor([213], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 213
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 214]), dec_len=tensor([213], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 214]) -> torch.Size([1, 214, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 214, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 790, 512]), dec_out.shape=torch.Size([1, 214, 640])
[DEBUG] enc_len=tensor([790], device='cuda:0'), target_length=tensor([213], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([790], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 790, 512]), dec_out.shape: torch.Size([1, 214, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 790, 214, 1001])

Validation DataLoader 0:  94%|█████████▎| 59/63 [00:02<00:00, 22.83it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 1009]), enc_len = tensor([1008], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 1009])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=1009
DEBUG: 转换后 enc_out.shape = torch.Size([1, 1009, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 1009, 512])
DEBUG: actual_T: 1009, enc_len调整后: tensor([1008], device='cuda:0')
[NeMo I 2025-08-20 09:47:15 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:15 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 262]), target_length: tensor([262], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 262])
[DEBUG] target_length = tensor([262], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 262
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 263]), dec_len=tensor([262], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 263]) -> torch.Size([1, 263, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 263, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 1009, 512]), dec_out.shape=torch.Size([1, 263, 640])
[DEBUG] enc_len=tensor([1008], device='cuda:0'), target_length=tensor([262], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([1008], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 1009, 512]), dec_out.shape: torch.Size([1, 263, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 1009, 263, 1001])

Validation DataLoader 0:  95%|█████████▌| 60/63 [00:02<00:00, 22.79it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 383]), enc_len = tensor([382], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 383])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=383
DEBUG: 转换后 enc_out.shape = torch.Size([1, 383, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 383, 512])
DEBUG: actual_T: 383, enc_len调整后: tensor([382], device='cuda:0')
[NeMo I 2025-08-20 09:47:15 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:15 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 101]), target_length: tensor([101], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 101])
[DEBUG] target_length = tensor([101], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 101
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 102]), dec_len=tensor([101], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 102]) -> torch.Size([1, 102, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 102, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 383, 512]), dec_out.shape=torch.Size([1, 102, 640])
[DEBUG] enc_len=tensor([382], device='cuda:0'), target_length=tensor([101], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([382], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 383, 512]), dec_out.shape: torch.Size([1, 102, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 383, 102, 1001])

Validation DataLoader 0:  97%|█████████▋| 61/63 [00:02<00:00, 22.83it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 721]), enc_len = tensor([721], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 721])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=721
DEBUG: 转换后 enc_out.shape = torch.Size([1, 721, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 721, 512])
DEBUG: actual_T: 721, enc_len调整后: tensor([721], device='cuda:0')
[NeMo I 2025-08-20 09:47:15 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:15 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 135]), target_length: tensor([135], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 135])
[DEBUG] target_length = tensor([135], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 135
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 136]), dec_len=tensor([135], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 136]) -> torch.Size([1, 136, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 136, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 721, 512]), dec_out.shape=torch.Size([1, 136, 640])
[DEBUG] enc_len=tensor([721], device='cuda:0'), target_length=tensor([135], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([721], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 721, 512]), dec_out.shape: torch.Size([1, 136, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 721, 136, 1001])

Validation DataLoader 0:  98%|█████████▊| 62/63 [00:02<00:00, 22.86it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 833]), enc_len = tensor([833], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 833])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=833
DEBUG: 转换后 enc_out.shape = torch.Size([1, 833, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 833, 512])
DEBUG: actual_T: 833, enc_len调整后: tensor([833], device='cuda:0')
[NeMo I 2025-08-20 09:47:16 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:16 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 188]), target_length: tensor([188], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 188])
[DEBUG] target_length = tensor([188], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 188
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 189]), dec_len=tensor([188], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 189]) -> torch.Size([1, 189, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 189, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 833, 512]), dec_out.shape=torch.Size([1, 189, 640])
[DEBUG] enc_len=tensor([833], device='cuda:0'), target_length=tensor([188], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([833], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 833, 512]), dec_out.shape: torch.Size([1, 189, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 833, 189, 1001])

Validation DataLoader 0: 100%|██████████| 63/63 [00:02<00:00, 22.87it/s][AEpoch 0:  48%|████▊     | 30/63 [00:09<00:10,  3.04it/s, v_num=31, train_loss_step=2.11e+3, val_loss=1.56e+3]
                                                                        [ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 629]), enc_len = tensor([629], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 629])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=629
DEBUG: 转换后 enc_out.shape = torch.Size([1, 629, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 629, 512])
DEBUG: actual_T: 629, enc_len调整后: tensor([629], device='cuda:0')
[NeMo I 2025-08-20 09:47:16 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:16 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 178]), target_length: tensor([178], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 178])
[DEBUG] target_length = tensor([178], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 178
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 179]), dec_len=tensor([178], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 179]) -> torch.Size([1, 179, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 179, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 629, 512]), dec_out.shape=torch.Size([1, 179, 640])
[DEBUG] enc_len=tensor([629], device='cuda:0'), target_length=tensor([178], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([629], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 629, 512]), dec_out.shape: torch.Size([1, 179, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 629, 179, 1001])
Epoch 0:  49%|████▉     | 31/63 [00:10<00:11,  2.87it/s, v_num=31, train_loss_step=2.11e+3, val_loss=1.56e+3]Epoch 0:  49%|████▉     | 31/63 [00:10<00:11,  2.87it/s, v_num=31, train_loss_step=1.44e+3, val_loss=1.56e+3]DEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 809]), enc_len = tensor([809], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 809])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=809
DEBUG: 转换后 enc_out.shape = torch.Size([1, 809, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 809, 512])
DEBUG: actual_T: 809, enc_len调整后: tensor([809], device='cuda:0')
[NeMo I 2025-08-20 09:47:17 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:17 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 154]), target_length: tensor([154], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 154])
[DEBUG] target_length = tensor([154], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 154
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 155]), dec_len=tensor([154], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 155]) -> torch.Size([1, 155, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 155, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 809, 512]), dec_out.shape=torch.Size([1, 155, 640])
[DEBUG] enc_len=tensor([809], device='cuda:0'), target_length=tensor([154], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([809], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 809, 512]), dec_out.shape: torch.Size([1, 155, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 809, 155, 1001])
Epoch 0:  51%|█████     | 32/63 [00:10<00:10,  2.95it/s, v_num=31, train_loss_step=1.44e+3, val_loss=1.56e+3]Epoch 0:  51%|█████     | 32/63 [00:10<00:10,  2.95it/s, v_num=31, train_loss_step=1.36e+3, val_loss=1.56e+3]DEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 820]), enc_len = tensor([820], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 820])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=820
DEBUG: 转换后 enc_out.shape = torch.Size([1, 820, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 820, 512])
DEBUG: actual_T: 820, enc_len调整后: tensor([820], device='cuda:0')
[NeMo I 2025-08-20 09:47:17 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:17 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 236]), target_length: tensor([236], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 236])
[DEBUG] target_length = tensor([236], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 236
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 237]), dec_len=tensor([236], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 237]) -> torch.Size([1, 237, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 237, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 820, 512]), dec_out.shape=torch.Size([1, 237, 640])
[DEBUG] enc_len=tensor([820], device='cuda:0'), target_length=tensor([236], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([820], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 820, 512]), dec_out.shape: torch.Size([1, 237, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 820, 237, 1001])
Epoch 0:  52%|█████▏    | 33/63 [00:10<00:09,  3.02it/s, v_num=31, train_loss_step=1.36e+3, val_loss=1.56e+3]Epoch 0:  52%|█████▏    | 33/63 [00:10<00:09,  3.02it/s, v_num=31, train_loss_step=1.55e+3, val_loss=1.56e+3]DEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 731]), enc_len = tensor([731], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 731])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=731
DEBUG: 转换后 enc_out.shape = torch.Size([1, 731, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 731, 512])
DEBUG: actual_T: 731, enc_len调整后: tensor([731], device='cuda:0')
[NeMo I 2025-08-20 09:47:17 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:17 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 176]), target_length: tensor([176], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 176])
[DEBUG] target_length = tensor([176], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 176
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 177]), dec_len=tensor([176], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 177]) -> torch.Size([1, 177, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 177, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 731, 512]), dec_out.shape=torch.Size([1, 177, 640])
[DEBUG] enc_len=tensor([731], device='cuda:0'), target_length=tensor([176], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([731], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 731, 512]), dec_out.shape: torch.Size([1, 177, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 731, 177, 1001])
Epoch 0:  54%|█████▍    | 34/63 [00:11<00:09,  3.09it/s, v_num=31, train_loss_step=1.55e+3, val_loss=1.56e+3]Epoch 0:  54%|█████▍    | 34/63 [00:11<00:09,  3.09it/s, v_num=31, train_loss_step=1.13e+3, val_loss=1.56e+3]DEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 772]), enc_len = tensor([772], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 772])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=772
DEBUG: 转换后 enc_out.shape = torch.Size([1, 772, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 772, 512])
DEBUG: actual_T: 772, enc_len调整后: tensor([772], device='cuda:0')
[NeMo I 2025-08-20 09:47:17 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:17 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 188]), target_length: tensor([188], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 188])
[DEBUG] target_length = tensor([188], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 188
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 189]), dec_len=tensor([188], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 189]) -> torch.Size([1, 189, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 189, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 772, 512]), dec_out.shape=torch.Size([1, 189, 640])
[DEBUG] enc_len=tensor([772], device='cuda:0'), target_length=tensor([188], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([772], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 772, 512]), dec_out.shape: torch.Size([1, 189, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 772, 189, 1001])
Epoch 0:  56%|█████▌    | 35/63 [00:11<00:08,  3.16it/s, v_num=31, train_loss_step=1.13e+3, val_loss=1.56e+3]Epoch 0:  56%|█████▌    | 35/63 [00:11<00:08,  3.16it/s, v_num=31, train_loss_step=1.17e+3, val_loss=1.56e+3]DEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 912]), enc_len = tensor([912], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 912])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=912
DEBUG: 转换后 enc_out.shape = torch.Size([1, 912, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 912, 512])
DEBUG: actual_T: 912, enc_len调整后: tensor([912], device='cuda:0')
[NeMo I 2025-08-20 09:47:17 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:17 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 204]), target_length: tensor([204], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 204])
[DEBUG] target_length = tensor([204], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 204
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 205]), dec_len=tensor([204], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 205]) -> torch.Size([1, 205, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 205, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 912, 512]), dec_out.shape=torch.Size([1, 205, 640])
[DEBUG] enc_len=tensor([912], device='cuda:0'), target_length=tensor([204], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([912], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 912, 512]), dec_out.shape: torch.Size([1, 205, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 912, 205, 1001])
Epoch 0:  57%|█████▋    | 36/63 [00:11<00:08,  3.22it/s, v_num=31, train_loss_step=1.17e+3, val_loss=1.56e+3]Epoch 0:  57%|█████▋    | 36/63 [00:11<00:08,  3.22it/s, v_num=31, train_loss_step=1.28e+3, val_loss=1.56e+3]DEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 537]), enc_len = tensor([537], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 537])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=537
DEBUG: 转换后 enc_out.shape = torch.Size([1, 537, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 537, 512])
DEBUG: actual_T: 537, enc_len调整后: tensor([537], device='cuda:0')
[NeMo I 2025-08-20 09:47:17 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:17 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 99]), target_length: tensor([99], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 99])
[DEBUG] target_length = tensor([99], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 99
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 100]), dec_len=tensor([99], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 100]) -> torch.Size([1, 100, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 100, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 537, 512]), dec_out.shape=torch.Size([1, 100, 640])
[DEBUG] enc_len=tensor([537], device='cuda:0'), target_length=tensor([99], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([537], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 537, 512]), dec_out.shape: torch.Size([1, 100, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 537, 100, 1001])
Epoch 0:  59%|█████▊    | 37/63 [00:11<00:07,  3.29it/s, v_num=31, train_loss_step=1.28e+3, val_loss=1.56e+3]Epoch 0:  59%|█████▊    | 37/63 [00:11<00:07,  3.29it/s, v_num=31, train_loss_step=638.0, val_loss=1.56e+3]  DEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 848]), enc_len = tensor([848], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 848])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=848
DEBUG: 转换后 enc_out.shape = torch.Size([1, 848, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 848, 512])
DEBUG: actual_T: 848, enc_len调整后: tensor([848], device='cuda:0')
[NeMo I 2025-08-20 09:47:17 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:17 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 179]), target_length: tensor([179], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 179])
[DEBUG] target_length = tensor([179], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 179
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 180]), dec_len=tensor([179], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 180]) -> torch.Size([1, 180, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 180, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 848, 512]), dec_out.shape=torch.Size([1, 180, 640])
[DEBUG] enc_len=tensor([848], device='cuda:0'), target_length=tensor([179], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([848], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 848, 512]), dec_out.shape: torch.Size([1, 180, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 848, 180, 1001])
Epoch 0:  60%|██████    | 38/63 [00:11<00:07,  3.36it/s, v_num=31, train_loss_step=638.0, val_loss=1.56e+3]Epoch 0:  60%|██████    | 38/63 [00:11<00:07,  3.36it/s, v_num=31, train_loss_step=1.17e+3, val_loss=1.56e+3]DEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 669]), enc_len = tensor([669], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 669])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=669
DEBUG: 转换后 enc_out.shape = torch.Size([1, 669, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 669, 512])
DEBUG: actual_T: 669, enc_len调整后: tensor([669], device='cuda:0')
[NeMo I 2025-08-20 09:47:17 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:17 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 156]), target_length: tensor([156], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 156])
[DEBUG] target_length = tensor([156], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 156
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 157]), dec_len=tensor([156], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 157]) -> torch.Size([1, 157, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 157, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 669, 512]), dec_out.shape=torch.Size([1, 157, 640])
[DEBUG] enc_len=tensor([669], device='cuda:0'), target_length=tensor([156], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([669], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 669, 512]), dec_out.shape: torch.Size([1, 157, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 669, 157, 1001])
Epoch 0:  62%|██████▏   | 39/63 [00:11<00:07,  3.43it/s, v_num=31, train_loss_step=1.17e+3, val_loss=1.56e+3]Epoch 0:  62%|██████▏   | 39/63 [00:11<00:07,  3.43it/s, v_num=31, train_loss_step=1.01e+3, val_loss=1.56e+3]DEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 606]), enc_len = tensor([605], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 606])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=606
DEBUG: 转换后 enc_out.shape = torch.Size([1, 606, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 606, 512])
DEBUG: actual_T: 606, enc_len调整后: tensor([605], device='cuda:0')
[NeMo I 2025-08-20 09:47:17 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:17 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 144]), target_length: tensor([144], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 144])
[DEBUG] target_length = tensor([144], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 144
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 145]), dec_len=tensor([144], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 145]) -> torch.Size([1, 145, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 145, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 606, 512]), dec_out.shape=torch.Size([1, 145, 640])
[DEBUG] enc_len=tensor([605], device='cuda:0'), target_length=tensor([144], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([605], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 606, 512]), dec_out.shape: torch.Size([1, 145, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 606, 145, 1001])
Epoch 0:  63%|██████▎   | 40/63 [00:11<00:06,  3.49it/s, v_num=31, train_loss_step=1.01e+3, val_loss=1.56e+3]Epoch 0:  63%|██████▎   | 40/63 [00:11<00:06,  3.49it/s, v_num=31, train_loss_step=961.0, val_loss=1.56e+3]  DEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 613]), enc_len = tensor([613], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 613])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=613
DEBUG: 转换后 enc_out.shape = torch.Size([1, 613, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 613, 512])
DEBUG: actual_T: 613, enc_len调整后: tensor([613], device='cuda:0')
[NeMo I 2025-08-20 09:47:17 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:17 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 129]), target_length: tensor([129], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 129])
[DEBUG] target_length = tensor([129], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 129
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 130]), dec_len=tensor([129], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 130]) -> torch.Size([1, 130, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 130, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 613, 512]), dec_out.shape=torch.Size([1, 130, 640])
[DEBUG] enc_len=tensor([613], device='cuda:0'), target_length=tensor([129], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([613], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 613, 512]), dec_out.shape: torch.Size([1, 130, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 613, 130, 1001])
Epoch 0:  65%|██████▌   | 41/63 [00:11<00:06,  3.56it/s, v_num=31, train_loss_step=961.0, val_loss=1.56e+3]Epoch 0:  65%|██████▌   | 41/63 [00:11<00:06,  3.56it/s, v_num=31, train_loss_step=910.0, val_loss=1.56e+3]DEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 677]), enc_len = tensor([676], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 677])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=677
DEBUG: 转换后 enc_out.shape = torch.Size([1, 677, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 677, 512])
DEBUG: actual_T: 677, enc_len调整后: tensor([676], device='cuda:0')
[NeMo I 2025-08-20 09:47:17 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:17 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 155]), target_length: tensor([155], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 155])
[DEBUG] target_length = tensor([155], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 155
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 156]), dec_len=tensor([155], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 156]) -> torch.Size([1, 156, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 156, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 677, 512]), dec_out.shape=torch.Size([1, 156, 640])
[DEBUG] enc_len=tensor([676], device='cuda:0'), target_length=tensor([155], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([676], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 677, 512]), dec_out.shape: torch.Size([1, 156, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 677, 156, 1001])
Epoch 0:  67%|██████▋   | 42/63 [00:11<00:05,  3.63it/s, v_num=31, train_loss_step=910.0, val_loss=1.56e+3]Epoch 0:  67%|██████▋   | 42/63 [00:11<00:05,  3.63it/s, v_num=31, train_loss_step=1.04e+3, val_loss=1.56e+3]DEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 466]), enc_len = tensor([466], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 466])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=466
DEBUG: 转换后 enc_out.shape = torch.Size([1, 466, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 466, 512])
DEBUG: actual_T: 466, enc_len调整后: tensor([466], device='cuda:0')
[NeMo I 2025-08-20 09:47:17 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:17 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 119]), target_length: tensor([119], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 119])
[DEBUG] target_length = tensor([119], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 119
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 120]), dec_len=tensor([119], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 120]) -> torch.Size([1, 120, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 120, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 466, 512]), dec_out.shape=torch.Size([1, 120, 640])
[DEBUG] enc_len=tensor([466], device='cuda:0'), target_length=tensor([119], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([466], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 466, 512]), dec_out.shape: torch.Size([1, 120, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 466, 120, 1001])
Epoch 0:  68%|██████▊   | 43/63 [00:11<00:05,  3.69it/s, v_num=31, train_loss_step=1.04e+3, val_loss=1.56e+3]Epoch 0:  68%|██████▊   | 43/63 [00:11<00:05,  3.69it/s, v_num=31, train_loss_step=783.0, val_loss=1.56e+3]  DEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 383]), enc_len = tensor([382], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 383])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=383
DEBUG: 转换后 enc_out.shape = torch.Size([1, 383, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 383, 512])
DEBUG: actual_T: 383, enc_len调整后: tensor([382], device='cuda:0')
[NeMo I 2025-08-20 09:47:17 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:17 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 101]), target_length: tensor([101], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 101])
[DEBUG] target_length = tensor([101], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 101
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 102]), dec_len=tensor([101], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 102]) -> torch.Size([1, 102, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 102, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 383, 512]), dec_out.shape=torch.Size([1, 102, 640])
[DEBUG] enc_len=tensor([382], device='cuda:0'), target_length=tensor([101], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([382], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 383, 512]), dec_out.shape: torch.Size([1, 102, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 383, 102, 1001])
Epoch 0:  70%|██████▉   | 44/63 [00:11<00:05,  3.76it/s, v_num=31, train_loss_step=783.0, val_loss=1.56e+3]Epoch 0:  70%|██████▉   | 44/63 [00:11<00:05,  3.76it/s, v_num=31, train_loss_step=643.0, val_loss=1.56e+3]DEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 790]), enc_len = tensor([790], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 790])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=790
DEBUG: 转换后 enc_out.shape = torch.Size([1, 790, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 790, 512])
DEBUG: actual_T: 790, enc_len调整后: tensor([790], device='cuda:0')
[NeMo I 2025-08-20 09:47:17 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:17 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 213]), target_length: tensor([213], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 213])
[DEBUG] target_length = tensor([213], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 213
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 214]), dec_len=tensor([213], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 214]) -> torch.Size([1, 214, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 214, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 790, 512]), dec_out.shape=torch.Size([1, 214, 640])
[DEBUG] enc_len=tensor([790], device='cuda:0'), target_length=tensor([213], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([790], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 790, 512]), dec_out.shape: torch.Size([1, 214, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 790, 214, 1001])
Epoch 0:  71%|███████▏  | 45/63 [00:11<00:04,  3.82it/s, v_num=31, train_loss_step=643.0, val_loss=1.56e+3]Epoch 0:  71%|███████▏  | 45/63 [00:11<00:04,  3.82it/s, v_num=31, train_loss_step=1.35e+3, val_loss=1.56e+3]
Validation: 0it [00:00, ?it/s][A
Validation:   0%|          | 0/63 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/63 [00:00<?, ?it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 684]), enc_len = tensor([684], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 684])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=684
DEBUG: 转换后 enc_out.shape = torch.Size([1, 684, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 684, 512])
DEBUG: actual_T: 684, enc_len调整后: tensor([684], device='cuda:0')
[NeMo I 2025-08-20 09:47:18 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:18 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 169]), target_length: tensor([169], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 169])
[DEBUG] target_length = tensor([169], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 169
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 170]), dec_len=tensor([169], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 170]) -> torch.Size([1, 170, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 170, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 684, 512]), dec_out.shape=torch.Size([1, 170, 640])
[DEBUG] enc_len=tensor([684], device='cuda:0'), target_length=tensor([169], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([684], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 684, 512]), dec_out.shape: torch.Size([1, 170, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 684, 170, 1001])

Validation DataLoader 0:   2%|▏         | 1/63 [00:00<00:03, 15.91it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 699]), enc_len = tensor([699], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 699])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=699
DEBUG: 转换后 enc_out.shape = torch.Size([1, 699, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 699, 512])
DEBUG: actual_T: 699, enc_len调整后: tensor([699], device='cuda:0')
[NeMo I 2025-08-20 09:47:18 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:18 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 175]), target_length: tensor([175], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 175])
[DEBUG] target_length = tensor([175], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 175
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 176]), dec_len=tensor([175], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 176]) -> torch.Size([1, 176, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 176, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 699, 512]), dec_out.shape=torch.Size([1, 176, 640])
[DEBUG] enc_len=tensor([699], device='cuda:0'), target_length=tensor([175], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([699], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 699, 512]), dec_out.shape: torch.Size([1, 176, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 699, 176, 1001])

Validation DataLoader 0:   3%|▎         | 2/63 [00:00<00:03, 18.68it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 887]), enc_len = tensor([887], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 887])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=887
DEBUG: 转换后 enc_out.shape = torch.Size([1, 887, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 887, 512])
DEBUG: actual_T: 887, enc_len调整后: tensor([887], device='cuda:0')
[NeMo I 2025-08-20 09:47:18 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:18 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 231]), target_length: tensor([231], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 231])
[DEBUG] target_length = tensor([231], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 231
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 232]), dec_len=tensor([231], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 232]) -> torch.Size([1, 232, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 232, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 887, 512]), dec_out.shape=torch.Size([1, 232, 640])
[DEBUG] enc_len=tensor([887], device='cuda:0'), target_length=tensor([231], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([887], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 887, 512]), dec_out.shape: torch.Size([1, 232, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 887, 232, 1001])

Validation DataLoader 0:   5%|▍         | 3/63 [00:00<00:03, 19.51it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 678]), enc_len = tensor([678], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 678])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=678
DEBUG: 转换后 enc_out.shape = torch.Size([1, 678, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 678, 512])
DEBUG: actual_T: 678, enc_len调整后: tensor([678], device='cuda:0')
[NeMo I 2025-08-20 09:47:18 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:18 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 145]), target_length: tensor([145], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 145])
[DEBUG] target_length = tensor([145], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 145
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 146]), dec_len=tensor([145], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 146]) -> torch.Size([1, 146, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 146, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 678, 512]), dec_out.shape=torch.Size([1, 146, 640])
[DEBUG] enc_len=tensor([678], device='cuda:0'), target_length=tensor([145], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([678], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 678, 512]), dec_out.shape: torch.Size([1, 146, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 678, 146, 1001])

Validation DataLoader 0:   6%|▋         | 4/63 [00:00<00:02, 20.19it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 677]), enc_len = tensor([676], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 677])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=677
DEBUG: 转换后 enc_out.shape = torch.Size([1, 677, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 677, 512])
DEBUG: actual_T: 677, enc_len调整后: tensor([676], device='cuda:0')
[NeMo I 2025-08-20 09:47:18 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:18 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 155]), target_length: tensor([155], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 155])
[DEBUG] target_length = tensor([155], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 155
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 156]), dec_len=tensor([155], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 156]) -> torch.Size([1, 156, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 156, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 677, 512]), dec_out.shape=torch.Size([1, 156, 640])
[DEBUG] enc_len=tensor([676], device='cuda:0'), target_length=tensor([155], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([676], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 677, 512]), dec_out.shape: torch.Size([1, 156, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 677, 156, 1001])

Validation DataLoader 0:   8%|▊         | 5/63 [00:00<00:02, 20.81it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 694]), enc_len = tensor([694], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 694])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=694
DEBUG: 转换后 enc_out.shape = torch.Size([1, 694, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 694, 512])
DEBUG: actual_T: 694, enc_len调整后: tensor([694], device='cuda:0')
[NeMo I 2025-08-20 09:47:18 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:18 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 165]), target_length: tensor([165], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 165])
[DEBUG] target_length = tensor([165], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 165
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 166]), dec_len=tensor([165], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 166]) -> torch.Size([1, 166, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 166, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 694, 512]), dec_out.shape=torch.Size([1, 166, 640])
[DEBUG] enc_len=tensor([694], device='cuda:0'), target_length=tensor([165], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([694], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 694, 512]), dec_out.shape: torch.Size([1, 166, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 694, 166, 1001])

Validation DataLoader 0:  10%|▉         | 6/63 [00:00<00:02, 21.26it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 615]), enc_len = tensor([615], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 615])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=615
DEBUG: 转换后 enc_out.shape = torch.Size([1, 615, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 615, 512])
DEBUG: actual_T: 615, enc_len调整后: tensor([615], device='cuda:0')
[NeMo I 2025-08-20 09:47:18 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:18 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 112]), target_length: tensor([112], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 112])
[DEBUG] target_length = tensor([112], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 112
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 113]), dec_len=tensor([112], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 113]) -> torch.Size([1, 113, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 113, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 615, 512]), dec_out.shape=torch.Size([1, 113, 640])
[DEBUG] enc_len=tensor([615], device='cuda:0'), target_length=tensor([112], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([615], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 615, 512]), dec_out.shape: torch.Size([1, 113, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 615, 113, 1001])

Validation DataLoader 0:  11%|█         | 7/63 [00:00<00:02, 21.69it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 966]), enc_len = tensor([966], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 966])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=966
DEBUG: 转换后 enc_out.shape = torch.Size([1, 966, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 966, 512])
DEBUG: actual_T: 966, enc_len调整后: tensor([966], device='cuda:0')
[NeMo I 2025-08-20 09:47:18 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:18 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 237]), target_length: tensor([237], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 237])
[DEBUG] target_length = tensor([237], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 237
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 238]), dec_len=tensor([237], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 238]) -> torch.Size([1, 238, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 238, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 966, 512]), dec_out.shape=torch.Size([1, 238, 640])
[DEBUG] enc_len=tensor([966], device='cuda:0'), target_length=tensor([237], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([966], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 966, 512]), dec_out.shape: torch.Size([1, 238, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 966, 238, 1001])

Validation DataLoader 0:  13%|█▎        | 8/63 [00:00<00:02, 21.32it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 809]), enc_len = tensor([809], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 809])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=809
DEBUG: 转换后 enc_out.shape = torch.Size([1, 809, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 809, 512])
DEBUG: actual_T: 809, enc_len调整后: tensor([809], device='cuda:0')
[NeMo I 2025-08-20 09:47:18 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:18 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 154]), target_length: tensor([154], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 154])
[DEBUG] target_length = tensor([154], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 154
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 155]), dec_len=tensor([154], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 155]) -> torch.Size([1, 155, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 155, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 809, 512]), dec_out.shape=torch.Size([1, 155, 640])
[DEBUG] enc_len=tensor([809], device='cuda:0'), target_length=tensor([154], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([809], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 809, 512]), dec_out.shape: torch.Size([1, 155, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 809, 155, 1001])

Validation DataLoader 0:  14%|█▍        | 9/63 [00:00<00:02, 21.47it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 820]), enc_len = tensor([820], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 820])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=820
DEBUG: 转换后 enc_out.shape = torch.Size([1, 820, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 820, 512])
DEBUG: actual_T: 820, enc_len调整后: tensor([820], device='cuda:0')
[NeMo I 2025-08-20 09:47:18 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:18 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 236]), target_length: tensor([236], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 236])
[DEBUG] target_length = tensor([236], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 236
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 237]), dec_len=tensor([236], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 237]) -> torch.Size([1, 237, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 237, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 820, 512]), dec_out.shape=torch.Size([1, 237, 640])
[DEBUG] enc_len=tensor([820], device='cuda:0'), target_length=tensor([236], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([820], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 820, 512]), dec_out.shape: torch.Size([1, 237, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 820, 237, 1001])

Validation DataLoader 0:  16%|█▌        | 10/63 [00:00<00:02, 21.50it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 537]), enc_len = tensor([537], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 537])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=537
DEBUG: 转换后 enc_out.shape = torch.Size([1, 537, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 537, 512])
DEBUG: actual_T: 537, enc_len调整后: tensor([537], device='cuda:0')
[NeMo I 2025-08-20 09:47:18 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:18 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 99]), target_length: tensor([99], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 99])
[DEBUG] target_length = tensor([99], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 99
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 100]), dec_len=tensor([99], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 100]) -> torch.Size([1, 100, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 100, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 537, 512]), dec_out.shape=torch.Size([1, 100, 640])
[DEBUG] enc_len=tensor([537], device='cuda:0'), target_length=tensor([99], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([537], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 537, 512]), dec_out.shape: torch.Size([1, 100, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 537, 100, 1001])

Validation DataLoader 0:  17%|█▋        | 11/63 [00:00<00:02, 21.78it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 848]), enc_len = tensor([848], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 848])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=848
DEBUG: 转换后 enc_out.shape = torch.Size([1, 848, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 848, 512])
DEBUG: actual_T: 848, enc_len调整后: tensor([848], device='cuda:0')
[NeMo I 2025-08-20 09:47:18 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:18 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 179]), target_length: tensor([179], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 179])
[DEBUG] target_length = tensor([179], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 179
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 180]), dec_len=tensor([179], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 180]) -> torch.Size([1, 180, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 180, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 848, 512]), dec_out.shape=torch.Size([1, 180, 640])
[DEBUG] enc_len=tensor([848], device='cuda:0'), target_length=tensor([179], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([848], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 848, 512]), dec_out.shape: torch.Size([1, 180, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 848, 180, 1001])

Validation DataLoader 0:  19%|█▉        | 12/63 [00:00<00:02, 21.86it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 693]), enc_len = tensor([693], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 693])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=693
DEBUG: 转换后 enc_out.shape = torch.Size([1, 693, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 693, 512])
DEBUG: actual_T: 693, enc_len调整后: tensor([693], device='cuda:0')
[NeMo I 2025-08-20 09:47:18 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:18 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 175]), target_length: tensor([175], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 175])
[DEBUG] target_length = tensor([175], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 175
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 176]), dec_len=tensor([175], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 176]) -> torch.Size([1, 176, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 176, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 693, 512]), dec_out.shape=torch.Size([1, 176, 640])
[DEBUG] enc_len=tensor([693], device='cuda:0'), target_length=tensor([175], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([693], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 693, 512]), dec_out.shape: torch.Size([1, 176, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 693, 176, 1001])

Validation DataLoader 0:  21%|██        | 13/63 [00:00<00:02, 21.98it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 690]), enc_len = tensor([690], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 690])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=690
DEBUG: 转换后 enc_out.shape = torch.Size([1, 690, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 690, 512])
DEBUG: actual_T: 690, enc_len调整后: tensor([690], device='cuda:0')
[NeMo I 2025-08-20 09:47:18 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:18 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 176]), target_length: tensor([176], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 176])
[DEBUG] target_length = tensor([176], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 176
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 177]), dec_len=tensor([176], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 177]) -> torch.Size([1, 177, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 177, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 690, 512]), dec_out.shape=torch.Size([1, 177, 640])
[DEBUG] enc_len=tensor([690], device='cuda:0'), target_length=tensor([176], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([690], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 690, 512]), dec_out.shape: torch.Size([1, 177, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 690, 177, 1001])

Validation DataLoader 0:  22%|██▏       | 14/63 [00:00<00:02, 22.08it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 629]), enc_len = tensor([629], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 629])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=629
DEBUG: 转换后 enc_out.shape = torch.Size([1, 629, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 629, 512])
DEBUG: actual_T: 629, enc_len调整后: tensor([629], device='cuda:0')
[NeMo I 2025-08-20 09:47:18 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:18 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 178]), target_length: tensor([178], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 178])
[DEBUG] target_length = tensor([178], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 178
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 179]), dec_len=tensor([178], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 179]) -> torch.Size([1, 179, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 179, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 629, 512]), dec_out.shape=torch.Size([1, 179, 640])
[DEBUG] enc_len=tensor([629], device='cuda:0'), target_length=tensor([178], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([629], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 629, 512]), dec_out.shape: torch.Size([1, 179, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 629, 179, 1001])

Validation DataLoader 0:  24%|██▍       | 15/63 [00:00<00:02, 22.19it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 669]), enc_len = tensor([669], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 669])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=669
DEBUG: 转换后 enc_out.shape = torch.Size([1, 669, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 669, 512])
DEBUG: actual_T: 669, enc_len调整后: tensor([669], device='cuda:0')
[NeMo I 2025-08-20 09:47:18 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:18 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 156]), target_length: tensor([156], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 156])
[DEBUG] target_length = tensor([156], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 156
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 157]), dec_len=tensor([156], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 157]) -> torch.Size([1, 157, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 157, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 669, 512]), dec_out.shape=torch.Size([1, 157, 640])
[DEBUG] enc_len=tensor([669], device='cuda:0'), target_length=tensor([156], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([669], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 669, 512]), dec_out.shape: torch.Size([1, 157, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 669, 157, 1001])

Validation DataLoader 0:  25%|██▌       | 16/63 [00:00<00:02, 22.30it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 607]), enc_len = tensor([607], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 607])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=607
DEBUG: 转换后 enc_out.shape = torch.Size([1, 607, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 607, 512])
DEBUG: actual_T: 607, enc_len调整后: tensor([607], device='cuda:0')
[NeMo I 2025-08-20 09:47:18 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:18 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 166]), target_length: tensor([166], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 166])
[DEBUG] target_length = tensor([166], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 166
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 167]), dec_len=tensor([166], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 167]) -> torch.Size([1, 167, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 167, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 607, 512]), dec_out.shape=torch.Size([1, 167, 640])
[DEBUG] enc_len=tensor([607], device='cuda:0'), target_length=tensor([166], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([607], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 607, 512]), dec_out.shape: torch.Size([1, 167, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 607, 167, 1001])

Validation DataLoader 0:  27%|██▋       | 17/63 [00:00<00:02, 22.37it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 633]), enc_len = tensor([632], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 633])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=633
DEBUG: 转换后 enc_out.shape = torch.Size([1, 633, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 633, 512])
DEBUG: actual_T: 633, enc_len调整后: tensor([632], device='cuda:0')
[NeMo I 2025-08-20 09:47:18 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:18 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 162]), target_length: tensor([162], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 162])
[DEBUG] target_length = tensor([162], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 162
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 163]), dec_len=tensor([162], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 163]) -> torch.Size([1, 163, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 163, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 633, 512]), dec_out.shape=torch.Size([1, 163, 640])
[DEBUG] enc_len=tensor([632], device='cuda:0'), target_length=tensor([162], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([632], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 633, 512]), dec_out.shape: torch.Size([1, 163, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 633, 163, 1001])

Validation DataLoader 0:  29%|██▊       | 18/63 [00:00<00:02, 22.45it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 745]), enc_len = tensor([744], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 745])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=745
DEBUG: 转换后 enc_out.shape = torch.Size([1, 745, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 745, 512])
DEBUG: actual_T: 745, enc_len调整后: tensor([744], device='cuda:0')
[NeMo I 2025-08-20 09:47:18 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:18 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 171]), target_length: tensor([171], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 171])
[DEBUG] target_length = tensor([171], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 171
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 172]), dec_len=tensor([171], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 172]) -> torch.Size([1, 172, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 172, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 745, 512]), dec_out.shape=torch.Size([1, 172, 640])
[DEBUG] enc_len=tensor([744], device='cuda:0'), target_length=tensor([171], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([744], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 745, 512]), dec_out.shape: torch.Size([1, 172, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 745, 172, 1001])

Validation DataLoader 0:  30%|███       | 19/63 [00:00<00:01, 22.51it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 825]), enc_len = tensor([825], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 825])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=825
DEBUG: 转换后 enc_out.shape = torch.Size([1, 825, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 825, 512])
DEBUG: actual_T: 825, enc_len调整后: tensor([825], device='cuda:0')
[NeMo I 2025-08-20 09:47:18 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:18 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 168]), target_length: tensor([168], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 168])
[DEBUG] target_length = tensor([168], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 168
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 169]), dec_len=tensor([168], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 169]) -> torch.Size([1, 169, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 169, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 825, 512]), dec_out.shape=torch.Size([1, 169, 640])
[DEBUG] enc_len=tensor([825], device='cuda:0'), target_length=tensor([168], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([825], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 825, 512]), dec_out.shape: torch.Size([1, 169, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 825, 169, 1001])

Validation DataLoader 0:  32%|███▏      | 20/63 [00:00<00:01, 22.54it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 758]), enc_len = tensor([758], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 758])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=758
DEBUG: 转换后 enc_out.shape = torch.Size([1, 758, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 758, 512])
DEBUG: actual_T: 758, enc_len调整后: tensor([758], device='cuda:0')
[NeMo I 2025-08-20 09:47:19 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:19 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 193]), target_length: tensor([193], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 193])
[DEBUG] target_length = tensor([193], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 193
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 194]), dec_len=tensor([193], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 194]) -> torch.Size([1, 194, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 194, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 758, 512]), dec_out.shape=torch.Size([1, 194, 640])
[DEBUG] enc_len=tensor([758], device='cuda:0'), target_length=tensor([193], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([758], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 758, 512]), dec_out.shape: torch.Size([1, 194, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 758, 194, 1001])

Validation DataLoader 0:  33%|███▎      | 21/63 [00:00<00:01, 22.49it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 803]), enc_len = tensor([802], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 803])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=803
DEBUG: 转换后 enc_out.shape = torch.Size([1, 803, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 803, 512])
DEBUG: actual_T: 803, enc_len调整后: tensor([802], device='cuda:0')
[NeMo I 2025-08-20 09:47:19 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:19 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 177]), target_length: tensor([177], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 177])
[DEBUG] target_length = tensor([177], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 177
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 178]), dec_len=tensor([177], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 178]) -> torch.Size([1, 178, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 178, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 803, 512]), dec_out.shape=torch.Size([1, 178, 640])
[DEBUG] enc_len=tensor([802], device='cuda:0'), target_length=tensor([177], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([802], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 803, 512]), dec_out.shape: torch.Size([1, 178, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 803, 178, 1001])

Validation DataLoader 0:  35%|███▍      | 22/63 [00:00<00:01, 22.53it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 731]), enc_len = tensor([731], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 731])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=731
DEBUG: 转换后 enc_out.shape = torch.Size([1, 731, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 731, 512])
DEBUG: actual_T: 731, enc_len调整后: tensor([731], device='cuda:0')
[NeMo I 2025-08-20 09:47:19 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:19 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 176]), target_length: tensor([176], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 176])
[DEBUG] target_length = tensor([176], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 176
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 177]), dec_len=tensor([176], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 177]) -> torch.Size([1, 177, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 177, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 731, 512]), dec_out.shape=torch.Size([1, 177, 640])
[DEBUG] enc_len=tensor([731], device='cuda:0'), target_length=tensor([176], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([731], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 731, 512]), dec_out.shape: torch.Size([1, 177, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 731, 177, 1001])

Validation DataLoader 0:  37%|███▋      | 23/63 [00:01<00:01, 22.59it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 824]), enc_len = tensor([824], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 824])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=824
DEBUG: 转换后 enc_out.shape = torch.Size([1, 824, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 824, 512])
DEBUG: actual_T: 824, enc_len调整后: tensor([824], device='cuda:0')
[NeMo I 2025-08-20 09:47:19 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:19 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 181]), target_length: tensor([181], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 181])
[DEBUG] target_length = tensor([181], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 181
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 182]), dec_len=tensor([181], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 182]) -> torch.Size([1, 182, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 182, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 824, 512]), dec_out.shape=torch.Size([1, 182, 640])
[DEBUG] enc_len=tensor([824], device='cuda:0'), target_length=tensor([181], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([824], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 824, 512]), dec_out.shape: torch.Size([1, 182, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 824, 182, 1001])

Validation DataLoader 0:  38%|███▊      | 24/63 [00:01<00:01, 22.54it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 559]), enc_len = tensor([559], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 559])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=559
DEBUG: 转换后 enc_out.shape = torch.Size([1, 559, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 559, 512])
DEBUG: actual_T: 559, enc_len调整后: tensor([559], device='cuda:0')
[NeMo I 2025-08-20 09:47:19 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:19 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 128]), target_length: tensor([128], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 128])
[DEBUG] target_length = tensor([128], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 128
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 129]), dec_len=tensor([128], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 129]) -> torch.Size([1, 129, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 129, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 559, 512]), dec_out.shape=torch.Size([1, 129, 640])
[DEBUG] enc_len=tensor([559], device='cuda:0'), target_length=tensor([128], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([559], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 559, 512]), dec_out.shape: torch.Size([1, 129, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 559, 129, 1001])

Validation DataLoader 0:  40%|███▉      | 25/63 [00:01<00:01, 22.62it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 440]), enc_len = tensor([439], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 440])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=440
DEBUG: 转换后 enc_out.shape = torch.Size([1, 440, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 440, 512])
DEBUG: actual_T: 440, enc_len调整后: tensor([439], device='cuda:0')
[NeMo I 2025-08-20 09:47:19 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:19 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 92]), target_length: tensor([92], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 92])
[DEBUG] target_length = tensor([92], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 92
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 93]), dec_len=tensor([92], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 93]) -> torch.Size([1, 93, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 93, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 440, 512]), dec_out.shape=torch.Size([1, 93, 640])
[DEBUG] enc_len=tensor([439], device='cuda:0'), target_length=tensor([92], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([439], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 440, 512]), dec_out.shape: torch.Size([1, 93, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 440, 93, 1001])

Validation DataLoader 0:  41%|████▏     | 26/63 [00:01<00:01, 22.73it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 642]), enc_len = tensor([642], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 642])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=642
DEBUG: 转换后 enc_out.shape = torch.Size([1, 642, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 642, 512])
DEBUG: actual_T: 642, enc_len调整后: tensor([642], device='cuda:0')
[NeMo I 2025-08-20 09:47:19 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:19 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 145]), target_length: tensor([145], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 145])
[DEBUG] target_length = tensor([145], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 145
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 146]), dec_len=tensor([145], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 146]) -> torch.Size([1, 146, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 146, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 642, 512]), dec_out.shape=torch.Size([1, 146, 640])
[DEBUG] enc_len=tensor([642], device='cuda:0'), target_length=tensor([145], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([642], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 642, 512]), dec_out.shape: torch.Size([1, 146, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 642, 146, 1001])

Validation DataLoader 0:  43%|████▎     | 27/63 [00:01<00:01, 22.67it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 925]), enc_len = tensor([925], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 925])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=925
DEBUG: 转换后 enc_out.shape = torch.Size([1, 925, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 925, 512])
DEBUG: actual_T: 925, enc_len调整后: tensor([925], device='cuda:0')
[NeMo I 2025-08-20 09:47:19 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:19 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 227]), target_length: tensor([227], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 227])
[DEBUG] target_length = tensor([227], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 227
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 228]), dec_len=tensor([227], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 228]) -> torch.Size([1, 228, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 228, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 925, 512]), dec_out.shape=torch.Size([1, 228, 640])
[DEBUG] enc_len=tensor([925], device='cuda:0'), target_length=tensor([227], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([925], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 925, 512]), dec_out.shape: torch.Size([1, 228, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 925, 228, 1001])

Validation DataLoader 0:  44%|████▍     | 28/63 [00:01<00:01, 22.64it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 952]), enc_len = tensor([952], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 952])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=952
DEBUG: 转换后 enc_out.shape = torch.Size([1, 952, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 952, 512])
DEBUG: actual_T: 952, enc_len调整后: tensor([952], device='cuda:0')
[NeMo I 2025-08-20 09:47:19 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:19 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 233]), target_length: tensor([233], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 233])
[DEBUG] target_length = tensor([233], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 233
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 234]), dec_len=tensor([233], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 234]) -> torch.Size([1, 234, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 234, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 952, 512]), dec_out.shape=torch.Size([1, 234, 640])
[DEBUG] enc_len=tensor([952], device='cuda:0'), target_length=tensor([233], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([952], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 952, 512]), dec_out.shape: torch.Size([1, 234, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 952, 234, 1001])

Validation DataLoader 0:  46%|████▌     | 29/63 [00:01<00:01, 22.60it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 562]), enc_len = tensor([562], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 562])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=562
DEBUG: 转换后 enc_out.shape = torch.Size([1, 562, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 562, 512])
DEBUG: actual_T: 562, enc_len调整后: tensor([562], device='cuda:0')
[NeMo I 2025-08-20 09:47:19 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:19 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 139]), target_length: tensor([139], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 139])
[DEBUG] target_length = tensor([139], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 139
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 140]), dec_len=tensor([139], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 140]) -> torch.Size([1, 140, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 140, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 562, 512]), dec_out.shape=torch.Size([1, 140, 640])
[DEBUG] enc_len=tensor([562], device='cuda:0'), target_length=tensor([139], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([562], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 562, 512]), dec_out.shape: torch.Size([1, 140, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 562, 140, 1001])

Validation DataLoader 0:  48%|████▊     | 30/63 [00:01<00:01, 22.61it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 726]), enc_len = tensor([725], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 726])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=726
DEBUG: 转换后 enc_out.shape = torch.Size([1, 726, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 726, 512])
DEBUG: actual_T: 726, enc_len调整后: tensor([725], device='cuda:0')
[NeMo I 2025-08-20 09:47:19 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:19 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 178]), target_length: tensor([178], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 178])
[DEBUG] target_length = tensor([178], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 178
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 179]), dec_len=tensor([178], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 179]) -> torch.Size([1, 179, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 179, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 726, 512]), dec_out.shape=torch.Size([1, 179, 640])
[DEBUG] enc_len=tensor([725], device='cuda:0'), target_length=tensor([178], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([725], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 726, 512]), dec_out.shape: torch.Size([1, 179, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 726, 179, 1001])

Validation DataLoader 0:  49%|████▉     | 31/63 [00:01<00:01, 22.64it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 606]), enc_len = tensor([605], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 606])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=606
DEBUG: 转换后 enc_out.shape = torch.Size([1, 606, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 606, 512])
DEBUG: actual_T: 606, enc_len调整后: tensor([605], device='cuda:0')
[NeMo I 2025-08-20 09:47:19 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:19 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 144]), target_length: tensor([144], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 144])
[DEBUG] target_length = tensor([144], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 144
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 145]), dec_len=tensor([144], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 145]) -> torch.Size([1, 145, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 145, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 606, 512]), dec_out.shape=torch.Size([1, 145, 640])
[DEBUG] enc_len=tensor([605], device='cuda:0'), target_length=tensor([144], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([605], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 606, 512]), dec_out.shape: torch.Size([1, 145, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 606, 145, 1001])

Validation DataLoader 0:  51%|█████     | 32/63 [00:01<00:01, 22.65it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 468]), enc_len = tensor([468], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 468])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=468
DEBUG: 转换后 enc_out.shape = torch.Size([1, 468, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 468, 512])
DEBUG: actual_T: 468, enc_len调整后: tensor([468], device='cuda:0')
[NeMo I 2025-08-20 09:47:19 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:19 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 111]), target_length: tensor([111], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 111])
[DEBUG] target_length = tensor([111], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 111
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 112]), dec_len=tensor([111], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 112]) -> torch.Size([1, 112, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 112, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 468, 512]), dec_out.shape=torch.Size([1, 112, 640])
[DEBUG] enc_len=tensor([468], device='cuda:0'), target_length=tensor([111], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([468], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 468, 512]), dec_out.shape: torch.Size([1, 112, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 468, 112, 1001])

Validation DataLoader 0:  52%|█████▏    | 33/63 [00:01<00:01, 22.72it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 575]), enc_len = tensor([575], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 575])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=575
DEBUG: 转换后 enc_out.shape = torch.Size([1, 575, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 575, 512])
DEBUG: actual_T: 575, enc_len调整后: tensor([575], device='cuda:0')
[NeMo I 2025-08-20 09:47:19 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:19 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 114]), target_length: tensor([114], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 114])
[DEBUG] target_length = tensor([114], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 114
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 115]), dec_len=tensor([114], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 115]) -> torch.Size([1, 115, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 115, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 575, 512]), dec_out.shape=torch.Size([1, 115, 640])
[DEBUG] enc_len=tensor([575], device='cuda:0'), target_length=tensor([114], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([575], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 575, 512]), dec_out.shape: torch.Size([1, 115, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 575, 115, 1001])

Validation DataLoader 0:  54%|█████▍    | 34/63 [00:01<00:01, 22.72it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 787]), enc_len = tensor([787], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 787])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=787
DEBUG: 转换后 enc_out.shape = torch.Size([1, 787, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 787, 512])
DEBUG: actual_T: 787, enc_len调整后: tensor([787], device='cuda:0')
[NeMo I 2025-08-20 09:47:19 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:19 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 189]), target_length: tensor([189], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 189])
[DEBUG] target_length = tensor([189], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 189
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 190]), dec_len=tensor([189], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 190]) -> torch.Size([1, 190, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 190, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 787, 512]), dec_out.shape=torch.Size([1, 190, 640])
[DEBUG] enc_len=tensor([787], device='cuda:0'), target_length=tensor([189], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([787], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 787, 512]), dec_out.shape: torch.Size([1, 190, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 787, 190, 1001])

Validation DataLoader 0:  56%|█████▌    | 35/63 [00:01<00:01, 22.72it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 812]), enc_len = tensor([812], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 812])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=812
DEBUG: 转换后 enc_out.shape = torch.Size([1, 812, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 812, 512])
DEBUG: actual_T: 812, enc_len调整后: tensor([812], device='cuda:0')
[NeMo I 2025-08-20 09:47:19 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:19 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 205]), target_length: tensor([205], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 205])
[DEBUG] target_length = tensor([205], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 205
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 206]), dec_len=tensor([205], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 206]) -> torch.Size([1, 206, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 206, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 812, 512]), dec_out.shape=torch.Size([1, 206, 640])
[DEBUG] enc_len=tensor([812], device='cuda:0'), target_length=tensor([205], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([812], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 812, 512]), dec_out.shape: torch.Size([1, 206, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 812, 206, 1001])

Validation DataLoader 0:  57%|█████▋    | 36/63 [00:01<00:01, 22.67it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 549]), enc_len = tensor([549], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 549])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=549
DEBUG: 转换后 enc_out.shape = torch.Size([1, 549, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 549, 512])
DEBUG: actual_T: 549, enc_len调整后: tensor([549], device='cuda:0')
[NeMo I 2025-08-20 09:47:19 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:19 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 114]), target_length: tensor([114], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 114])
[DEBUG] target_length = tensor([114], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 114
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 115]), dec_len=tensor([114], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 115]) -> torch.Size([1, 115, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 115, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 549, 512]), dec_out.shape=torch.Size([1, 115, 640])
[DEBUG] enc_len=tensor([549], device='cuda:0'), target_length=tensor([114], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([549], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 549, 512]), dec_out.shape: torch.Size([1, 115, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 549, 115, 1001])

Validation DataLoader 0:  59%|█████▊    | 37/63 [00:01<00:01, 22.73it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 828]), enc_len = tensor([828], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 828])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=828
DEBUG: 转换后 enc_out.shape = torch.Size([1, 828, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 828, 512])
DEBUG: actual_T: 828, enc_len调整后: tensor([828], device='cuda:0')
[NeMo I 2025-08-20 09:47:19 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:19 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 199]), target_length: tensor([199], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 199])
[DEBUG] target_length = tensor([199], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 199
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 200]), dec_len=tensor([199], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 200]) -> torch.Size([1, 200, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 200, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 828, 512]), dec_out.shape=torch.Size([1, 200, 640])
[DEBUG] enc_len=tensor([828], device='cuda:0'), target_length=tensor([199], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([828], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 828, 512]), dec_out.shape: torch.Size([1, 200, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 828, 200, 1001])

Validation DataLoader 0:  60%|██████    | 38/63 [00:01<00:01, 22.73it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 731]), enc_len = tensor([731], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 731])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=731
DEBUG: 转换后 enc_out.shape = torch.Size([1, 731, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 731, 512])
DEBUG: actual_T: 731, enc_len调整后: tensor([731], device='cuda:0')
[NeMo I 2025-08-20 09:47:19 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:19 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 175]), target_length: tensor([175], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 175])
[DEBUG] target_length = tensor([175], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 175
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 176]), dec_len=tensor([175], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 176]) -> torch.Size([1, 176, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 176, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 731, 512]), dec_out.shape=torch.Size([1, 176, 640])
[DEBUG] enc_len=tensor([731], device='cuda:0'), target_length=tensor([175], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([731], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 731, 512]), dec_out.shape: torch.Size([1, 176, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 731, 176, 1001])

Validation DataLoader 0:  62%|██████▏   | 39/63 [00:01<00:01, 22.70it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 803]), enc_len = tensor([803], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 803])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=803
DEBUG: 转换后 enc_out.shape = torch.Size([1, 803, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 803, 512])
DEBUG: actual_T: 803, enc_len调整后: tensor([803], device='cuda:0')
[NeMo I 2025-08-20 09:47:19 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:19 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 202]), target_length: tensor([202], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 202])
[DEBUG] target_length = tensor([202], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 202
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 203]), dec_len=tensor([202], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 203]) -> torch.Size([1, 203, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 203, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 803, 512]), dec_out.shape=torch.Size([1, 203, 640])
[DEBUG] enc_len=tensor([803], device='cuda:0'), target_length=tensor([202], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([803], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 803, 512]), dec_out.shape: torch.Size([1, 203, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 803, 203, 1001])

Validation DataLoader 0:  63%|██████▎   | 40/63 [00:01<00:01, 22.65it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 773]), enc_len = tensor([773], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 773])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=773
DEBUG: 转换后 enc_out.shape = torch.Size([1, 773, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 773, 512])
DEBUG: actual_T: 773, enc_len调整后: tensor([773], device='cuda:0')
[NeMo I 2025-08-20 09:47:19 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:19 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 184]), target_length: tensor([184], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 184])
[DEBUG] target_length = tensor([184], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 184
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 185]), dec_len=tensor([184], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 185]) -> torch.Size([1, 185, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 185, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 773, 512]), dec_out.shape=torch.Size([1, 185, 640])
[DEBUG] enc_len=tensor([773], device='cuda:0'), target_length=tensor([184], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([773], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 773, 512]), dec_out.shape: torch.Size([1, 185, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 773, 185, 1001])

Validation DataLoader 0:  65%|██████▌   | 41/63 [00:01<00:00, 22.63it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 466]), enc_len = tensor([466], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 466])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=466
DEBUG: 转换后 enc_out.shape = torch.Size([1, 466, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 466, 512])
DEBUG: actual_T: 466, enc_len调整后: tensor([466], device='cuda:0')
[NeMo I 2025-08-20 09:47:19 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:19 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 119]), target_length: tensor([119], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 119])
[DEBUG] target_length = tensor([119], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 119
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 120]), dec_len=tensor([119], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 120]) -> torch.Size([1, 120, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 120, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 466, 512]), dec_out.shape=torch.Size([1, 120, 640])
[DEBUG] enc_len=tensor([466], device='cuda:0'), target_length=tensor([119], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([466], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 466, 512]), dec_out.shape: torch.Size([1, 120, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 466, 120, 1001])

Validation DataLoader 0:  67%|██████▋   | 42/63 [00:01<00:00, 22.68it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 877]), enc_len = tensor([877], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 877])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=877
DEBUG: 转换后 enc_out.shape = torch.Size([1, 877, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 877, 512])
DEBUG: actual_T: 877, enc_len调整后: tensor([877], device='cuda:0')
[NeMo I 2025-08-20 09:47:19 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:19 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 196]), target_length: tensor([196], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 196])
[DEBUG] target_length = tensor([196], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 196
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 197]), dec_len=tensor([196], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 197]) -> torch.Size([1, 197, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 197, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 877, 512]), dec_out.shape=torch.Size([1, 197, 640])
[DEBUG] enc_len=tensor([877], device='cuda:0'), target_length=tensor([196], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([877], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 877, 512]), dec_out.shape: torch.Size([1, 197, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 877, 197, 1001])

Validation DataLoader 0:  68%|██████▊   | 43/63 [00:01<00:00, 22.67it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 613]), enc_len = tensor([613], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 613])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=613
DEBUG: 转换后 enc_out.shape = torch.Size([1, 613, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 613, 512])
DEBUG: actual_T: 613, enc_len调整后: tensor([613], device='cuda:0')
[NeMo I 2025-08-20 09:47:20 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:20 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 129]), target_length: tensor([129], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 129])
[DEBUG] target_length = tensor([129], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 129
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 130]), dec_len=tensor([129], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 130]) -> torch.Size([1, 130, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 130, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 613, 512]), dec_out.shape=torch.Size([1, 130, 640])
[DEBUG] enc_len=tensor([613], device='cuda:0'), target_length=tensor([129], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([613], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 613, 512]), dec_out.shape: torch.Size([1, 130, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 613, 130, 1001])

Validation DataLoader 0:  70%|██████▉   | 44/63 [00:01<00:00, 22.71it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 807]), enc_len = tensor([807], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 807])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=807
DEBUG: 转换后 enc_out.shape = torch.Size([1, 807, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 807, 512])
DEBUG: actual_T: 807, enc_len调整后: tensor([807], device='cuda:0')
[NeMo I 2025-08-20 09:47:20 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:20 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 192]), target_length: tensor([192], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 192])
[DEBUG] target_length = tensor([192], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 192
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 193]), dec_len=tensor([192], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 193]) -> torch.Size([1, 193, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 193, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 807, 512]), dec_out.shape=torch.Size([1, 193, 640])
[DEBUG] enc_len=tensor([807], device='cuda:0'), target_length=tensor([192], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([807], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 807, 512]), dec_out.shape: torch.Size([1, 193, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 807, 193, 1001])

Validation DataLoader 0:  71%|███████▏  | 45/63 [00:01<00:00, 22.67it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 732]), enc_len = tensor([732], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 732])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=732
DEBUG: 转换后 enc_out.shape = torch.Size([1, 732, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 732, 512])
DEBUG: actual_T: 732, enc_len调整后: tensor([732], device='cuda:0')
[NeMo I 2025-08-20 09:47:20 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:20 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 145]), target_length: tensor([145], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 145])
[DEBUG] target_length = tensor([145], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 145
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 146]), dec_len=tensor([145], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 146]) -> torch.Size([1, 146, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 146, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 732, 512]), dec_out.shape=torch.Size([1, 146, 640])
[DEBUG] enc_len=tensor([732], device='cuda:0'), target_length=tensor([145], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([732], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 732, 512]), dec_out.shape: torch.Size([1, 146, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 732, 146, 1001])

Validation DataLoader 0:  73%|███████▎  | 46/63 [00:02<00:00, 22.69it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 754]), enc_len = tensor([754], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 754])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=754
DEBUG: 转换后 enc_out.shape = torch.Size([1, 754, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 754, 512])
DEBUG: actual_T: 754, enc_len调整后: tensor([754], device='cuda:0')
[NeMo I 2025-08-20 09:47:20 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:20 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 170]), target_length: tensor([170], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 170])
[DEBUG] target_length = tensor([170], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 170
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 171]), dec_len=tensor([170], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 171]) -> torch.Size([1, 171, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 171, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 754, 512]), dec_out.shape=torch.Size([1, 171, 640])
[DEBUG] enc_len=tensor([754], device='cuda:0'), target_length=tensor([170], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([754], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 754, 512]), dec_out.shape: torch.Size([1, 171, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 754, 171, 1001])

Validation DataLoader 0:  75%|███████▍  | 47/63 [00:02<00:00, 22.65it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 772]), enc_len = tensor([772], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 772])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=772
DEBUG: 转换后 enc_out.shape = torch.Size([1, 772, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 772, 512])
DEBUG: actual_T: 772, enc_len调整后: tensor([772], device='cuda:0')
[NeMo I 2025-08-20 09:47:20 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:20 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 188]), target_length: tensor([188], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 188])
[DEBUG] target_length = tensor([188], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 188
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 189]), dec_len=tensor([188], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 189]) -> torch.Size([1, 189, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 189, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 772, 512]), dec_out.shape=torch.Size([1, 189, 640])
[DEBUG] enc_len=tensor([772], device='cuda:0'), target_length=tensor([188], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([772], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 772, 512]), dec_out.shape: torch.Size([1, 189, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 772, 189, 1001])

Validation DataLoader 0:  76%|███████▌  | 48/63 [00:02<00:00, 22.66it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 585]), enc_len = tensor([584], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 585])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=585
DEBUG: 转换后 enc_out.shape = torch.Size([1, 585, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 585, 512])
DEBUG: actual_T: 585, enc_len调整后: tensor([584], device='cuda:0')
[NeMo I 2025-08-20 09:47:20 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:20 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 112]), target_length: tensor([112], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 112])
[DEBUG] target_length = tensor([112], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 112
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 113]), dec_len=tensor([112], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 113]) -> torch.Size([1, 113, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 113, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 585, 512]), dec_out.shape=torch.Size([1, 113, 640])
[DEBUG] enc_len=tensor([584], device='cuda:0'), target_length=tensor([112], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([584], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 585, 512]), dec_out.shape: torch.Size([1, 113, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 585, 113, 1001])

Validation DataLoader 0:  78%|███████▊  | 49/63 [00:02<00:00, 22.71it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 713]), enc_len = tensor([713], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 713])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=713
DEBUG: 转换后 enc_out.shape = torch.Size([1, 713, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 713, 512])
DEBUG: actual_T: 713, enc_len调整后: tensor([713], device='cuda:0')
[NeMo I 2025-08-20 09:47:20 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:20 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 162]), target_length: tensor([162], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 162])
[DEBUG] target_length = tensor([162], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 162
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 163]), dec_len=tensor([162], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 163]) -> torch.Size([1, 163, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 163, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 713, 512]), dec_out.shape=torch.Size([1, 163, 640])
[DEBUG] enc_len=tensor([713], device='cuda:0'), target_length=tensor([162], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([713], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 713, 512]), dec_out.shape: torch.Size([1, 163, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 713, 163, 1001])

Validation DataLoader 0:  79%|███████▉  | 50/63 [00:02<00:00, 22.68it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 570]), enc_len = tensor([570], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 570])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=570
DEBUG: 转换后 enc_out.shape = torch.Size([1, 570, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 570, 512])
DEBUG: actual_T: 570, enc_len调整后: tensor([570], device='cuda:0')
[NeMo I 2025-08-20 09:47:20 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:20 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 108]), target_length: tensor([108], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 108])
[DEBUG] target_length = tensor([108], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 108
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 109]), dec_len=tensor([108], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 109]) -> torch.Size([1, 109, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 109, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 570, 512]), dec_out.shape=torch.Size([1, 109, 640])
[DEBUG] enc_len=tensor([570], device='cuda:0'), target_length=tensor([108], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([570], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 570, 512]), dec_out.shape: torch.Size([1, 109, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 570, 109, 1001])

Validation DataLoader 0:  81%|████████  | 51/63 [00:02<00:00, 22.73it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 745]), enc_len = tensor([745], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 745])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=745
DEBUG: 转换后 enc_out.shape = torch.Size([1, 745, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 745, 512])
DEBUG: actual_T: 745, enc_len调整后: tensor([745], device='cuda:0')
[NeMo I 2025-08-20 09:47:20 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:20 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 185]), target_length: tensor([185], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 185])
[DEBUG] target_length = tensor([185], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 185
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 186]), dec_len=tensor([185], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 186]) -> torch.Size([1, 186, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 186, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 745, 512]), dec_out.shape=torch.Size([1, 186, 640])
[DEBUG] enc_len=tensor([745], device='cuda:0'), target_length=tensor([185], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([745], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 745, 512]), dec_out.shape: torch.Size([1, 186, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 745, 186, 1001])

Validation DataLoader 0:  83%|████████▎ | 52/63 [00:02<00:00, 22.74it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 973]), enc_len = tensor([973], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 973])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=973
DEBUG: 转换后 enc_out.shape = torch.Size([1, 973, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 973, 512])
DEBUG: actual_T: 973, enc_len调整后: tensor([973], device='cuda:0')
[NeMo I 2025-08-20 09:47:20 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:20 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 241]), target_length: tensor([241], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 241])
[DEBUG] target_length = tensor([241], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 241
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 242]), dec_len=tensor([241], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 242]) -> torch.Size([1, 242, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 242, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 973, 512]), dec_out.shape=torch.Size([1, 242, 640])
[DEBUG] enc_len=tensor([973], device='cuda:0'), target_length=tensor([241], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([973], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 973, 512]), dec_out.shape: torch.Size([1, 242, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 973, 242, 1001])

Validation DataLoader 0:  84%|████████▍ | 53/63 [00:02<00:00, 22.66it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 912]), enc_len = tensor([912], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 912])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=912
DEBUG: 转换后 enc_out.shape = torch.Size([1, 912, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 912, 512])
DEBUG: actual_T: 912, enc_len调整后: tensor([912], device='cuda:0')
[NeMo I 2025-08-20 09:47:20 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:20 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 225]), target_length: tensor([225], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 225])
[DEBUG] target_length = tensor([225], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 225
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 226]), dec_len=tensor([225], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 226]) -> torch.Size([1, 226, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 226, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 912, 512]), dec_out.shape=torch.Size([1, 226, 640])
[DEBUG] enc_len=tensor([912], device='cuda:0'), target_length=tensor([225], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([912], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 912, 512]), dec_out.shape: torch.Size([1, 226, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 912, 226, 1001])

Validation DataLoader 0:  86%|████████▌ | 54/63 [00:02<00:00, 22.64it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 869]), enc_len = tensor([869], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 869])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=869
DEBUG: 转换后 enc_out.shape = torch.Size([1, 869, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 869, 512])
DEBUG: actual_T: 869, enc_len调整后: tensor([869], device='cuda:0')
[NeMo I 2025-08-20 09:47:20 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:20 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 210]), target_length: tensor([210], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 210])
[DEBUG] target_length = tensor([210], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 210
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 211]), dec_len=tensor([210], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 211]) -> torch.Size([1, 211, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 211, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 869, 512]), dec_out.shape=torch.Size([1, 211, 640])
[DEBUG] enc_len=tensor([869], device='cuda:0'), target_length=tensor([210], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([869], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 869, 512]), dec_out.shape: torch.Size([1, 211, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 869, 211, 1001])

Validation DataLoader 0:  87%|████████▋ | 55/63 [00:02<00:00, 22.61it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 577]), enc_len = tensor([577], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 577])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=577
DEBUG: 转换后 enc_out.shape = torch.Size([1, 577, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 577, 512])
DEBUG: actual_T: 577, enc_len调整后: tensor([577], device='cuda:0')
[NeMo I 2025-08-20 09:47:20 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:20 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 153]), target_length: tensor([153], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 153])
[DEBUG] target_length = tensor([153], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 153
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 154]), dec_len=tensor([153], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 154]) -> torch.Size([1, 154, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 154, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 577, 512]), dec_out.shape=torch.Size([1, 154, 640])
[DEBUG] enc_len=tensor([577], device='cuda:0'), target_length=tensor([153], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([577], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 577, 512]), dec_out.shape: torch.Size([1, 154, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 577, 154, 1001])

Validation DataLoader 0:  89%|████████▉ | 56/63 [00:02<00:00, 22.64it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 912]), enc_len = tensor([912], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 912])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=912
DEBUG: 转换后 enc_out.shape = torch.Size([1, 912, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 912, 512])
DEBUG: actual_T: 912, enc_len调整后: tensor([912], device='cuda:0')
[NeMo I 2025-08-20 09:47:20 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:20 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 204]), target_length: tensor([204], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 204])
[DEBUG] target_length = tensor([204], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 204
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 205]), dec_len=tensor([204], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 205]) -> torch.Size([1, 205, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 205, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 912, 512]), dec_out.shape=torch.Size([1, 205, 640])
[DEBUG] enc_len=tensor([912], device='cuda:0'), target_length=tensor([204], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([912], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 912, 512]), dec_out.shape: torch.Size([1, 205, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 912, 205, 1001])

Validation DataLoader 0:  90%|█████████ | 57/63 [00:02<00:00, 22.63it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 522]), enc_len = tensor([522], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 522])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=522
DEBUG: 转换后 enc_out.shape = torch.Size([1, 522, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 522, 512])
DEBUG: actual_T: 522, enc_len调整后: tensor([522], device='cuda:0')
[NeMo I 2025-08-20 09:47:20 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:20 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 111]), target_length: tensor([111], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 111])
[DEBUG] target_length = tensor([111], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 111
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 112]), dec_len=tensor([111], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 112]) -> torch.Size([1, 112, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 112, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 522, 512]), dec_out.shape=torch.Size([1, 112, 640])
[DEBUG] enc_len=tensor([522], device='cuda:0'), target_length=tensor([111], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([522], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 522, 512]), dec_out.shape: torch.Size([1, 112, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 522, 112, 1001])

Validation DataLoader 0:  92%|█████████▏| 58/63 [00:02<00:00, 22.65it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 790]), enc_len = tensor([790], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 790])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=790
DEBUG: 转换后 enc_out.shape = torch.Size([1, 790, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 790, 512])
DEBUG: actual_T: 790, enc_len调整后: tensor([790], device='cuda:0')
[NeMo I 2025-08-20 09:47:20 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:20 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 213]), target_length: tensor([213], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 213])
[DEBUG] target_length = tensor([213], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 213
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 214]), dec_len=tensor([213], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 214]) -> torch.Size([1, 214, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 214, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 790, 512]), dec_out.shape=torch.Size([1, 214, 640])
[DEBUG] enc_len=tensor([790], device='cuda:0'), target_length=tensor([213], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([790], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 790, 512]), dec_out.shape: torch.Size([1, 214, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 790, 214, 1001])

Validation DataLoader 0:  94%|█████████▎| 59/63 [00:02<00:00, 22.64it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 1009]), enc_len = tensor([1008], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 1009])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=1009
DEBUG: 转换后 enc_out.shape = torch.Size([1, 1009, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 1009, 512])
DEBUG: actual_T: 1009, enc_len调整后: tensor([1008], device='cuda:0')
[NeMo I 2025-08-20 09:47:20 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:20 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 262]), target_length: tensor([262], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 262])
[DEBUG] target_length = tensor([262], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 262
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 263]), dec_len=tensor([262], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 263]) -> torch.Size([1, 263, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 263, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 1009, 512]), dec_out.shape=torch.Size([1, 263, 640])
[DEBUG] enc_len=tensor([1008], device='cuda:0'), target_length=tensor([262], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([1008], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 1009, 512]), dec_out.shape: torch.Size([1, 263, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 1009, 263, 1001])

Validation DataLoader 0:  95%|█████████▌| 60/63 [00:02<00:00, 22.61it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 383]), enc_len = tensor([382], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 383])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=383
DEBUG: 转换后 enc_out.shape = torch.Size([1, 383, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 383, 512])
DEBUG: actual_T: 383, enc_len调整后: tensor([382], device='cuda:0')
[NeMo I 2025-08-20 09:47:20 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:20 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 101]), target_length: tensor([101], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 101])
[DEBUG] target_length = tensor([101], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 101
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 102]), dec_len=tensor([101], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 102]) -> torch.Size([1, 102, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 102, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 383, 512]), dec_out.shape=torch.Size([1, 102, 640])
[DEBUG] enc_len=tensor([382], device='cuda:0'), target_length=tensor([101], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([382], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 383, 512]), dec_out.shape: torch.Size([1, 102, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 383, 102, 1001])

Validation DataLoader 0:  97%|█████████▋| 61/63 [00:02<00:00, 22.65it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 721]), enc_len = tensor([721], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 721])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=721
DEBUG: 转换后 enc_out.shape = torch.Size([1, 721, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 721, 512])
DEBUG: actual_T: 721, enc_len调整后: tensor([721], device='cuda:0')
[NeMo I 2025-08-20 09:47:20 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:20 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 135]), target_length: tensor([135], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 135])
[DEBUG] target_length = tensor([135], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 135
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 136]), dec_len=tensor([135], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 136]) -> torch.Size([1, 136, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 136, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 721, 512]), dec_out.shape=torch.Size([1, 136, 640])
[DEBUG] enc_len=tensor([721], device='cuda:0'), target_length=tensor([135], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([721], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 721, 512]), dec_out.shape: torch.Size([1, 136, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 721, 136, 1001])

Validation DataLoader 0:  98%|█████████▊| 62/63 [00:02<00:00, 22.69it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 833]), enc_len = tensor([833], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 833])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=833
DEBUG: 转换后 enc_out.shape = torch.Size([1, 833, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 833, 512])
DEBUG: actual_T: 833, enc_len调整后: tensor([833], device='cuda:0')
[NeMo I 2025-08-20 09:47:20 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:20 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 188]), target_length: tensor([188], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 188])
[DEBUG] target_length = tensor([188], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 188
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 189]), dec_len=tensor([188], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 189]) -> torch.Size([1, 189, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 189, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 833, 512]), dec_out.shape=torch.Size([1, 189, 640])
[DEBUG] enc_len=tensor([833], device='cuda:0'), target_length=tensor([188], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([833], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 833, 512]), dec_out.shape: torch.Size([1, 189, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 833, 189, 1001])

Validation DataLoader 0: 100%|██████████| 63/63 [00:02<00:00, 22.68it/s][AEpoch 0:  71%|███████▏  | 45/63 [00:14<00:05,  3.07it/s, v_num=31, train_loss_step=1.35e+3, val_loss=1.06e+3]
                                                                        [ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 966]), enc_len = tensor([966], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 966])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=966
DEBUG: 转换后 enc_out.shape = torch.Size([1, 966, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 966, 512])
DEBUG: actual_T: 966, enc_len调整后: tensor([966], device='cuda:0')
[NeMo I 2025-08-20 09:47:21 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:21 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 237]), target_length: tensor([237], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 237])
[DEBUG] target_length = tensor([237], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 237
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 238]), dec_len=tensor([237], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 238]) -> torch.Size([1, 238, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 238, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 966, 512]), dec_out.shape=torch.Size([1, 238, 640])
[DEBUG] enc_len=tensor([966], device='cuda:0'), target_length=tensor([237], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([966], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 966, 512]), dec_out.shape: torch.Size([1, 238, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 966, 238, 1001])
Epoch 0:  73%|███████▎  | 46/63 [00:15<00:05,  2.95it/s, v_num=31, train_loss_step=1.35e+3, val_loss=1.06e+3]Epoch 0:  73%|███████▎  | 46/63 [00:15<00:05,  2.95it/s, v_num=31, train_loss_step=1.44e+3, val_loss=1.06e+3]DEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 684]), enc_len = tensor([684], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 684])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=684
DEBUG: 转换后 enc_out.shape = torch.Size([1, 684, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 684, 512])
DEBUG: actual_T: 684, enc_len调整后: tensor([684], device='cuda:0')
[NeMo I 2025-08-20 09:47:21 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:21 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 169]), target_length: tensor([169], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 169])
[DEBUG] target_length = tensor([169], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 169
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 170]), dec_len=tensor([169], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 170]) -> torch.Size([1, 170, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 170, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 684, 512]), dec_out.shape=torch.Size([1, 170, 640])
[DEBUG] enc_len=tensor([684], device='cuda:0'), target_length=tensor([169], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([684], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 684, 512]), dec_out.shape: torch.Size([1, 170, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 684, 170, 1001])
Epoch 0:  75%|███████▍  | 47/63 [00:15<00:05,  3.00it/s, v_num=31, train_loss_step=1.44e+3, val_loss=1.06e+3]Epoch 0:  75%|███████▍  | 47/63 [00:15<00:05,  3.00it/s, v_num=31, train_loss_step=996.0, val_loss=1.06e+3]  DEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 726]), enc_len = tensor([725], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 726])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=726
DEBUG: 转换后 enc_out.shape = torch.Size([1, 726, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 726, 512])
DEBUG: actual_T: 726, enc_len调整后: tensor([725], device='cuda:0')
[NeMo I 2025-08-20 09:47:21 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:21 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 178]), target_length: tensor([178], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 178])
[DEBUG] target_length = tensor([178], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 178
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 179]), dec_len=tensor([178], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 179]) -> torch.Size([1, 179, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 179, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 726, 512]), dec_out.shape=torch.Size([1, 179, 640])
[DEBUG] enc_len=tensor([725], device='cuda:0'), target_length=tensor([178], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([725], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 726, 512]), dec_out.shape: torch.Size([1, 179, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 726, 179, 1001])
Epoch 0:  76%|███████▌  | 48/63 [00:15<00:04,  3.05it/s, v_num=31, train_loss_step=996.0, val_loss=1.06e+3]Epoch 0:  76%|███████▌  | 48/63 [00:15<00:04,  3.05it/s, v_num=31, train_loss_step=1.07e+3, val_loss=1.06e+3]DEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 522]), enc_len = tensor([522], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 522])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=522
DEBUG: 转换后 enc_out.shape = torch.Size([1, 522, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 522, 512])
DEBUG: actual_T: 522, enc_len调整后: tensor([522], device='cuda:0')
[NeMo I 2025-08-20 09:47:21 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:21 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 111]), target_length: tensor([111], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 111])
[DEBUG] target_length = tensor([111], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 111
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 112]), dec_len=tensor([111], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 112]) -> torch.Size([1, 112, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 112, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 522, 512]), dec_out.shape=torch.Size([1, 112, 640])
[DEBUG] enc_len=tensor([522], device='cuda:0'), target_length=tensor([111], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([522], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 522, 512]), dec_out.shape: torch.Size([1, 112, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 522, 112, 1001])
Epoch 0:  78%|███████▊  | 49/63 [00:15<00:04,  3.10it/s, v_num=31, train_loss_step=1.07e+3, val_loss=1.06e+3]Epoch 0:  78%|███████▊  | 49/63 [00:15<00:04,  3.10it/s, v_num=31, train_loss_step=625.0, val_loss=1.06e+3]  DEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 828]), enc_len = tensor([828], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 828])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=828
DEBUG: 转换后 enc_out.shape = torch.Size([1, 828, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 828, 512])
DEBUG: actual_T: 828, enc_len调整后: tensor([828], device='cuda:0')
[NeMo I 2025-08-20 09:47:22 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:22 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 199]), target_length: tensor([199], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 199])
[DEBUG] target_length = tensor([199], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 199
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 200]), dec_len=tensor([199], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 200]) -> torch.Size([1, 200, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 200, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 828, 512]), dec_out.shape=torch.Size([1, 200, 640])
[DEBUG] enc_len=tensor([828], device='cuda:0'), target_length=tensor([199], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([828], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 828, 512]), dec_out.shape: torch.Size([1, 200, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 828, 200, 1001])
Epoch 0:  79%|███████▉  | 50/63 [00:15<00:04,  3.14it/s, v_num=31, train_loss_step=625.0, val_loss=1.06e+3]Epoch 0:  79%|███████▉  | 50/63 [00:15<00:04,  3.14it/s, v_num=31, train_loss_step=1.08e+3, val_loss=1.06e+3]DEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 721]), enc_len = tensor([721], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 721])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=721
DEBUG: 转换后 enc_out.shape = torch.Size([1, 721, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 721, 512])
DEBUG: actual_T: 721, enc_len调整后: tensor([721], device='cuda:0')
[NeMo I 2025-08-20 09:47:22 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:22 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 135]), target_length: tensor([135], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 135])
[DEBUG] target_length = tensor([135], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 135
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 136]), dec_len=tensor([135], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 136]) -> torch.Size([1, 136, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 136, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 721, 512]), dec_out.shape=torch.Size([1, 136, 640])
[DEBUG] enc_len=tensor([721], device='cuda:0'), target_length=tensor([135], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([721], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 721, 512]), dec_out.shape: torch.Size([1, 136, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 721, 136, 1001])
Epoch 0:  81%|████████  | 51/63 [00:15<00:03,  3.19it/s, v_num=31, train_loss_step=1.08e+3, val_loss=1.06e+3]Epoch 0:  81%|████████  | 51/63 [00:15<00:03,  3.19it/s, v_num=31, train_loss_step=792.0, val_loss=1.06e+3]  DEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 973]), enc_len = tensor([973], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 973])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=973
DEBUG: 转换后 enc_out.shape = torch.Size([1, 973, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 973, 512])
DEBUG: actual_T: 973, enc_len调整后: tensor([973], device='cuda:0')
[NeMo I 2025-08-20 09:47:22 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:22 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 241]), target_length: tensor([241], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 241])
[DEBUG] target_length = tensor([241], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 241
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 242]), dec_len=tensor([241], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 242]) -> torch.Size([1, 242, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 242, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 973, 512]), dec_out.shape=torch.Size([1, 242, 640])
[DEBUG] enc_len=tensor([973], device='cuda:0'), target_length=tensor([241], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([973], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 973, 512]), dec_out.shape: torch.Size([1, 242, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 973, 242, 1001])
Epoch 0:  83%|████████▎ | 52/63 [00:16<00:03,  3.24it/s, v_num=31, train_loss_step=792.0, val_loss=1.06e+3]Epoch 0:  83%|████████▎ | 52/63 [00:16<00:03,  3.24it/s, v_num=31, train_loss_step=1.29e+3, val_loss=1.06e+3]DEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 877]), enc_len = tensor([877], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 877])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=877
DEBUG: 转换后 enc_out.shape = torch.Size([1, 877, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 877, 512])
DEBUG: actual_T: 877, enc_len调整后: tensor([877], device='cuda:0')
[NeMo I 2025-08-20 09:47:22 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:22 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 196]), target_length: tensor([196], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 196])
[DEBUG] target_length = tensor([196], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 196
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 197]), dec_len=tensor([196], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 197]) -> torch.Size([1, 197, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 197, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 877, 512]), dec_out.shape=torch.Size([1, 197, 640])
[DEBUG] enc_len=tensor([877], device='cuda:0'), target_length=tensor([196], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([877], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 877, 512]), dec_out.shape: torch.Size([1, 197, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 877, 197, 1001])
Epoch 0:  84%|████████▍ | 53/63 [00:16<00:03,  3.29it/s, v_num=31, train_loss_step=1.29e+3, val_loss=1.06e+3]Epoch 0:  84%|████████▍ | 53/63 [00:16<00:03,  3.29it/s, v_num=31, train_loss_step=1.05e+3, val_loss=1.06e+3]DEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 468]), enc_len = tensor([468], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 468])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=468
DEBUG: 转换后 enc_out.shape = torch.Size([1, 468, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 468, 512])
DEBUG: actual_T: 468, enc_len调整后: tensor([468], device='cuda:0')
[NeMo I 2025-08-20 09:47:22 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:22 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 111]), target_length: tensor([111], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 111])
[DEBUG] target_length = tensor([111], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 111
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 112]), dec_len=tensor([111], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 112]) -> torch.Size([1, 112, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 112, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 468, 512]), dec_out.shape=torch.Size([1, 112, 640])
[DEBUG] enc_len=tensor([468], device='cuda:0'), target_length=tensor([111], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([468], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 468, 512]), dec_out.shape: torch.Size([1, 112, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 468, 112, 1001])
Epoch 0:  86%|████████▌ | 54/63 [00:16<00:02,  3.34it/s, v_num=31, train_loss_step=1.05e+3, val_loss=1.06e+3]Epoch 0:  86%|████████▌ | 54/63 [00:16<00:02,  3.34it/s, v_num=31, train_loss_step=624.0, val_loss=1.06e+3]  DEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 754]), enc_len = tensor([754], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 754])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=754
DEBUG: 转换后 enc_out.shape = torch.Size([1, 754, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 754, 512])
DEBUG: actual_T: 754, enc_len调整后: tensor([754], device='cuda:0')
[NeMo I 2025-08-20 09:47:22 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:22 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 170]), target_length: tensor([170], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 170])
[DEBUG] target_length = tensor([170], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 170
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 171]), dec_len=tensor([170], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 171]) -> torch.Size([1, 171, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 171, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 754, 512]), dec_out.shape=torch.Size([1, 171, 640])
[DEBUG] enc_len=tensor([754], device='cuda:0'), target_length=tensor([170], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([754], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 754, 512]), dec_out.shape: torch.Size([1, 171, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 754, 171, 1001])
Epoch 0:  87%|████████▋ | 55/63 [00:16<00:02,  3.38it/s, v_num=31, train_loss_step=624.0, val_loss=1.06e+3]Epoch 0:  87%|████████▋ | 55/63 [00:16<00:02,  3.38it/s, v_num=31, train_loss_step=930.0, val_loss=1.06e+3]DEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 615]), enc_len = tensor([615], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 615])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=615
DEBUG: 转换后 enc_out.shape = torch.Size([1, 615, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 615, 512])
DEBUG: actual_T: 615, enc_len调整后: tensor([615], device='cuda:0')
[NeMo I 2025-08-20 09:47:22 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:22 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 112]), target_length: tensor([112], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 112])
[DEBUG] target_length = tensor([112], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 112
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 113]), dec_len=tensor([112], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 113]) -> torch.Size([1, 113, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 113, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 615, 512]), dec_out.shape=torch.Size([1, 113, 640])
[DEBUG] enc_len=tensor([615], device='cuda:0'), target_length=tensor([112], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([615], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 615, 512]), dec_out.shape: torch.Size([1, 113, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 615, 113, 1001])
Epoch 0:  89%|████████▉ | 56/63 [00:16<00:02,  3.43it/s, v_num=31, train_loss_step=930.0, val_loss=1.06e+3]Epoch 0:  89%|████████▉ | 56/63 [00:16<00:02,  3.43it/s, v_num=31, train_loss_step=653.0, val_loss=1.06e+3]DEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 694]), enc_len = tensor([694], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 694])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=694
DEBUG: 转换后 enc_out.shape = torch.Size([1, 694, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 694, 512])
DEBUG: actual_T: 694, enc_len调整后: tensor([694], device='cuda:0')
[NeMo I 2025-08-20 09:47:22 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:22 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 165]), target_length: tensor([165], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 165])
[DEBUG] target_length = tensor([165], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 165
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 166]), dec_len=tensor([165], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 166]) -> torch.Size([1, 166, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 166, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 694, 512]), dec_out.shape=torch.Size([1, 166, 640])
[DEBUG] enc_len=tensor([694], device='cuda:0'), target_length=tensor([165], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([694], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 694, 512]), dec_out.shape: torch.Size([1, 166, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 694, 166, 1001])
Epoch 0:  90%|█████████ | 57/63 [00:16<00:01,  3.48it/s, v_num=31, train_loss_step=653.0, val_loss=1.06e+3]Epoch 0:  90%|█████████ | 57/63 [00:16<00:01,  3.48it/s, v_num=31, train_loss_step=843.0, val_loss=1.06e+3]DEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 678]), enc_len = tensor([678], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 678])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=678
DEBUG: 转换后 enc_out.shape = torch.Size([1, 678, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 678, 512])
DEBUG: actual_T: 678, enc_len调整后: tensor([678], device='cuda:0')
[NeMo I 2025-08-20 09:47:22 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:22 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 145]), target_length: tensor([145], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 145])
[DEBUG] target_length = tensor([145], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 145
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 146]), dec_len=tensor([145], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 146]) -> torch.Size([1, 146, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 146, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 678, 512]), dec_out.shape=torch.Size([1, 146, 640])
[DEBUG] enc_len=tensor([678], device='cuda:0'), target_length=tensor([145], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([678], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 678, 512]), dec_out.shape: torch.Size([1, 146, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 678, 146, 1001])
Epoch 0:  92%|█████████▏| 58/63 [00:16<00:01,  3.52it/s, v_num=31, train_loss_step=843.0, val_loss=1.06e+3]Epoch 0:  92%|█████████▏| 58/63 [00:16<00:01,  3.52it/s, v_num=31, train_loss_step=774.0, val_loss=1.06e+3]DEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 1009]), enc_len = tensor([1008], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 1009])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=1009
DEBUG: 转换后 enc_out.shape = torch.Size([1, 1009, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 1009, 512])
DEBUG: actual_T: 1009, enc_len调整后: tensor([1008], device='cuda:0')
[NeMo I 2025-08-20 09:47:22 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:22 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 262]), target_length: tensor([262], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 262])
[DEBUG] target_length = tensor([262], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 262
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 263]), dec_len=tensor([262], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 263]) -> torch.Size([1, 263, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 263, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 1009, 512]), dec_out.shape=torch.Size([1, 263, 640])
[DEBUG] enc_len=tensor([1008], device='cuda:0'), target_length=tensor([262], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([1008], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 1009, 512]), dec_out.shape: torch.Size([1, 263, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 1009, 263, 1001])
Epoch 0:  94%|█████████▎| 59/63 [00:16<00:01,  3.56it/s, v_num=31, train_loss_step=774.0, val_loss=1.06e+3]Epoch 0:  94%|█████████▎| 59/63 [00:16<00:01,  3.56it/s, v_num=31, train_loss_step=1.33e+3, val_loss=1.06e+3]DEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 731]), enc_len = tensor([731], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 731])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=731
DEBUG: 转换后 enc_out.shape = torch.Size([1, 731, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 731, 512])
DEBUG: actual_T: 731, enc_len调整后: tensor([731], device='cuda:0')
[NeMo I 2025-08-20 09:47:22 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:22 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 175]), target_length: tensor([175], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 175])
[DEBUG] target_length = tensor([175], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 175
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 176]), dec_len=tensor([175], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 176]) -> torch.Size([1, 176, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 176, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 731, 512]), dec_out.shape=torch.Size([1, 176, 640])
[DEBUG] enc_len=tensor([731], device='cuda:0'), target_length=tensor([175], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([731], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 731, 512]), dec_out.shape: torch.Size([1, 176, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 731, 176, 1001])
Epoch 0:  95%|█████████▌| 60/63 [00:16<00:00,  3.60it/s, v_num=31, train_loss_step=1.33e+3, val_loss=1.06e+3]Epoch 0:  95%|█████████▌| 60/63 [00:16<00:00,  3.60it/s, v_num=31, train_loss_step=902.0, val_loss=1.06e+3]  
Validation: 0it [00:00, ?it/s][A
Validation:   0%|          | 0/63 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/63 [00:00<?, ?it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 684]), enc_len = tensor([684], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 684])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=684
DEBUG: 转换后 enc_out.shape = torch.Size([1, 684, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 684, 512])
DEBUG: actual_T: 684, enc_len调整后: tensor([684], device='cuda:0')
[NeMo I 2025-08-20 09:47:22 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:22 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 169]), target_length: tensor([169], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 169])
[DEBUG] target_length = tensor([169], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 169
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 170]), dec_len=tensor([169], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 170]) -> torch.Size([1, 170, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 170, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 684, 512]), dec_out.shape=torch.Size([1, 170, 640])
[DEBUG] enc_len=tensor([684], device='cuda:0'), target_length=tensor([169], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([684], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 684, 512]), dec_out.shape: torch.Size([1, 170, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 684, 170, 1001])

Validation DataLoader 0:   2%|▏         | 1/63 [00:00<00:03, 16.11it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 699]), enc_len = tensor([699], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 699])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=699
DEBUG: 转换后 enc_out.shape = torch.Size([1, 699, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 699, 512])
DEBUG: actual_T: 699, enc_len调整后: tensor([699], device='cuda:0')
[NeMo I 2025-08-20 09:47:23 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:23 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 175]), target_length: tensor([175], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 175])
[DEBUG] target_length = tensor([175], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 175
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 176]), dec_len=tensor([175], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 176]) -> torch.Size([1, 176, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 176, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 699, 512]), dec_out.shape=torch.Size([1, 176, 640])
[DEBUG] enc_len=tensor([699], device='cuda:0'), target_length=tensor([175], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([699], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 699, 512]), dec_out.shape: torch.Size([1, 176, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 699, 176, 1001])

Validation DataLoader 0:   3%|▎         | 2/63 [00:00<00:03, 18.91it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 887]), enc_len = tensor([887], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 887])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=887
DEBUG: 转换后 enc_out.shape = torch.Size([1, 887, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 887, 512])
DEBUG: actual_T: 887, enc_len调整后: tensor([887], device='cuda:0')
[NeMo I 2025-08-20 09:47:23 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:23 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 231]), target_length: tensor([231], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 231])
[DEBUG] target_length = tensor([231], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 231
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 232]), dec_len=tensor([231], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 232]) -> torch.Size([1, 232, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 232, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 887, 512]), dec_out.shape=torch.Size([1, 232, 640])
[DEBUG] enc_len=tensor([887], device='cuda:0'), target_length=tensor([231], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([887], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 887, 512]), dec_out.shape: torch.Size([1, 232, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 887, 232, 1001])

Validation DataLoader 0:   5%|▍         | 3/63 [00:00<00:03, 19.73it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 678]), enc_len = tensor([678], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 678])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=678
DEBUG: 转换后 enc_out.shape = torch.Size([1, 678, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 678, 512])
DEBUG: actual_T: 678, enc_len调整后: tensor([678], device='cuda:0')
[NeMo I 2025-08-20 09:47:23 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:23 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 145]), target_length: tensor([145], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 145])
[DEBUG] target_length = tensor([145], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 145
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 146]), dec_len=tensor([145], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 146]) -> torch.Size([1, 146, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 146, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 678, 512]), dec_out.shape=torch.Size([1, 146, 640])
[DEBUG] enc_len=tensor([678], device='cuda:0'), target_length=tensor([145], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([678], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 678, 512]), dec_out.shape: torch.Size([1, 146, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 678, 146, 1001])

Validation DataLoader 0:   6%|▋         | 4/63 [00:00<00:02, 20.67it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 677]), enc_len = tensor([676], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 677])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=677
DEBUG: 转换后 enc_out.shape = torch.Size([1, 677, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 677, 512])
DEBUG: actual_T: 677, enc_len调整后: tensor([676], device='cuda:0')
[NeMo I 2025-08-20 09:47:23 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:23 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 155]), target_length: tensor([155], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 155])
[DEBUG] target_length = tensor([155], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 155
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 156]), dec_len=tensor([155], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 156]) -> torch.Size([1, 156, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 156, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 677, 512]), dec_out.shape=torch.Size([1, 156, 640])
[DEBUG] enc_len=tensor([676], device='cuda:0'), target_length=tensor([155], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([676], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 677, 512]), dec_out.shape: torch.Size([1, 156, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 677, 156, 1001])

Validation DataLoader 0:   8%|▊         | 5/63 [00:00<00:02, 21.32it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 694]), enc_len = tensor([694], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 694])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=694
DEBUG: 转换后 enc_out.shape = torch.Size([1, 694, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 694, 512])
DEBUG: actual_T: 694, enc_len调整后: tensor([694], device='cuda:0')
[NeMo I 2025-08-20 09:47:23 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:23 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 165]), target_length: tensor([165], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 165])
[DEBUG] target_length = tensor([165], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 165
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 166]), dec_len=tensor([165], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 166]) -> torch.Size([1, 166, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 166, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 694, 512]), dec_out.shape=torch.Size([1, 166, 640])
[DEBUG] enc_len=tensor([694], device='cuda:0'), target_length=tensor([165], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([694], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 694, 512]), dec_out.shape: torch.Size([1, 166, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 694, 166, 1001])

Validation DataLoader 0:  10%|▉         | 6/63 [00:00<00:02, 21.73it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 615]), enc_len = tensor([615], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 615])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=615
DEBUG: 转换后 enc_out.shape = torch.Size([1, 615, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 615, 512])
DEBUG: actual_T: 615, enc_len调整后: tensor([615], device='cuda:0')
[NeMo I 2025-08-20 09:47:23 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:23 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 112]), target_length: tensor([112], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 112])
[DEBUG] target_length = tensor([112], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 112
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 113]), dec_len=tensor([112], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 113]) -> torch.Size([1, 113, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 113, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 615, 512]), dec_out.shape=torch.Size([1, 113, 640])
[DEBUG] enc_len=tensor([615], device='cuda:0'), target_length=tensor([112], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([615], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 615, 512]), dec_out.shape: torch.Size([1, 113, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 615, 113, 1001])

Validation DataLoader 0:  11%|█         | 7/63 [00:00<00:02, 22.07it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 966]), enc_len = tensor([966], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 966])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=966
DEBUG: 转换后 enc_out.shape = torch.Size([1, 966, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 966, 512])
DEBUG: actual_T: 966, enc_len调整后: tensor([966], device='cuda:0')
[NeMo I 2025-08-20 09:47:23 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:23 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 237]), target_length: tensor([237], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 237])
[DEBUG] target_length = tensor([237], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 237
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 238]), dec_len=tensor([237], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 238]) -> torch.Size([1, 238, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 238, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 966, 512]), dec_out.shape=torch.Size([1, 238, 640])
[DEBUG] enc_len=tensor([966], device='cuda:0'), target_length=tensor([237], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([966], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 966, 512]), dec_out.shape: torch.Size([1, 238, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 966, 238, 1001])

Validation DataLoader 0:  13%|█▎        | 8/63 [00:00<00:02, 21.96it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 809]), enc_len = tensor([809], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 809])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=809
DEBUG: 转换后 enc_out.shape = torch.Size([1, 809, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 809, 512])
DEBUG: actual_T: 809, enc_len调整后: tensor([809], device='cuda:0')
[NeMo I 2025-08-20 09:47:23 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:23 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 154]), target_length: tensor([154], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 154])
[DEBUG] target_length = tensor([154], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 154
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 155]), dec_len=tensor([154], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 155]) -> torch.Size([1, 155, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 155, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 809, 512]), dec_out.shape=torch.Size([1, 155, 640])
[DEBUG] enc_len=tensor([809], device='cuda:0'), target_length=tensor([154], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([809], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 809, 512]), dec_out.shape: torch.Size([1, 155, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 809, 155, 1001])

Validation DataLoader 0:  14%|█▍        | 9/63 [00:00<00:02, 22.17it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 820]), enc_len = tensor([820], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 820])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=820
DEBUG: 转换后 enc_out.shape = torch.Size([1, 820, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 820, 512])
DEBUG: actual_T: 820, enc_len调整后: tensor([820], device='cuda:0')
[NeMo I 2025-08-20 09:47:23 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:23 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 236]), target_length: tensor([236], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 236])
[DEBUG] target_length = tensor([236], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 236
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 237]), dec_len=tensor([236], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 237]) -> torch.Size([1, 237, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 237, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 820, 512]), dec_out.shape=torch.Size([1, 237, 640])
[DEBUG] enc_len=tensor([820], device='cuda:0'), target_length=tensor([236], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([820], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 820, 512]), dec_out.shape: torch.Size([1, 237, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 820, 237, 1001])

Validation DataLoader 0:  16%|█▌        | 10/63 [00:00<00:02, 22.19it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 537]), enc_len = tensor([537], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 537])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=537
DEBUG: 转换后 enc_out.shape = torch.Size([1, 537, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 537, 512])
DEBUG: actual_T: 537, enc_len调整后: tensor([537], device='cuda:0')
[NeMo I 2025-08-20 09:47:23 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:23 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 99]), target_length: tensor([99], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 99])
[DEBUG] target_length = tensor([99], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 99
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 100]), dec_len=tensor([99], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 100]) -> torch.Size([1, 100, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 100, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 537, 512]), dec_out.shape=torch.Size([1, 100, 640])
[DEBUG] enc_len=tensor([537], device='cuda:0'), target_length=tensor([99], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([537], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 537, 512]), dec_out.shape: torch.Size([1, 100, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 537, 100, 1001])

Validation DataLoader 0:  17%|█▋        | 11/63 [00:00<00:02, 22.46it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 848]), enc_len = tensor([848], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 848])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=848
DEBUG: 转换后 enc_out.shape = torch.Size([1, 848, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 848, 512])
DEBUG: actual_T: 848, enc_len调整后: tensor([848], device='cuda:0')
[NeMo I 2025-08-20 09:47:23 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:23 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 179]), target_length: tensor([179], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 179])
[DEBUG] target_length = tensor([179], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 179
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 180]), dec_len=tensor([179], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 180]) -> torch.Size([1, 180, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 180, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 848, 512]), dec_out.shape=torch.Size([1, 180, 640])
[DEBUG] enc_len=tensor([848], device='cuda:0'), target_length=tensor([179], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([848], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 848, 512]), dec_out.shape: torch.Size([1, 180, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 848, 180, 1001])

Validation DataLoader 0:  19%|█▉        | 12/63 [00:00<00:02, 22.28it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 693]), enc_len = tensor([693], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 693])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=693
DEBUG: 转换后 enc_out.shape = torch.Size([1, 693, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 693, 512])
DEBUG: actual_T: 693, enc_len调整后: tensor([693], device='cuda:0')
[NeMo I 2025-08-20 09:47:23 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:23 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 175]), target_length: tensor([175], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 175])
[DEBUG] target_length = tensor([175], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 175
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 176]), dec_len=tensor([175], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 176]) -> torch.Size([1, 176, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 176, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 693, 512]), dec_out.shape=torch.Size([1, 176, 640])
[DEBUG] enc_len=tensor([693], device='cuda:0'), target_length=tensor([175], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([693], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 693, 512]), dec_out.shape: torch.Size([1, 176, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 693, 176, 1001])

Validation DataLoader 0:  21%|██        | 13/63 [00:00<00:02, 22.42it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 690]), enc_len = tensor([690], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 690])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=690
DEBUG: 转换后 enc_out.shape = torch.Size([1, 690, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 690, 512])
DEBUG: actual_T: 690, enc_len调整后: tensor([690], device='cuda:0')
[NeMo I 2025-08-20 09:47:23 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:23 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 176]), target_length: tensor([176], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 176])
[DEBUG] target_length = tensor([176], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 176
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 177]), dec_len=tensor([176], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 177]) -> torch.Size([1, 177, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 177, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 690, 512]), dec_out.shape=torch.Size([1, 177, 640])
[DEBUG] enc_len=tensor([690], device='cuda:0'), target_length=tensor([176], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([690], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 690, 512]), dec_out.shape: torch.Size([1, 177, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 690, 177, 1001])

Validation DataLoader 0:  22%|██▏       | 14/63 [00:00<00:02, 22.55it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 629]), enc_len = tensor([629], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 629])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=629
DEBUG: 转换后 enc_out.shape = torch.Size([1, 629, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 629, 512])
DEBUG: actual_T: 629, enc_len调整后: tensor([629], device='cuda:0')
[NeMo I 2025-08-20 09:47:23 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:23 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 178]), target_length: tensor([178], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 178])
[DEBUG] target_length = tensor([178], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 178
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 179]), dec_len=tensor([178], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 179]) -> torch.Size([1, 179, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 179, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 629, 512]), dec_out.shape=torch.Size([1, 179, 640])
[DEBUG] enc_len=tensor([629], device='cuda:0'), target_length=tensor([178], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([629], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 629, 512]), dec_out.shape: torch.Size([1, 179, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 629, 179, 1001])

Validation DataLoader 0:  24%|██▍       | 15/63 [00:00<00:02, 22.56it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 669]), enc_len = tensor([669], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 669])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=669
DEBUG: 转换后 enc_out.shape = torch.Size([1, 669, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 669, 512])
DEBUG: actual_T: 669, enc_len调整后: tensor([669], device='cuda:0')
[NeMo I 2025-08-20 09:47:23 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:23 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 156]), target_length: tensor([156], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 156])
[DEBUG] target_length = tensor([156], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 156
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 157]), dec_len=tensor([156], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 157]) -> torch.Size([1, 157, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 157, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 669, 512]), dec_out.shape=torch.Size([1, 157, 640])
[DEBUG] enc_len=tensor([669], device='cuda:0'), target_length=tensor([156], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([669], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 669, 512]), dec_out.shape: torch.Size([1, 157, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 669, 157, 1001])

Validation DataLoader 0:  25%|██▌       | 16/63 [00:00<00:02, 22.66it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 607]), enc_len = tensor([607], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 607])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=607
DEBUG: 转换后 enc_out.shape = torch.Size([1, 607, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 607, 512])
DEBUG: actual_T: 607, enc_len调整后: tensor([607], device='cuda:0')
[NeMo I 2025-08-20 09:47:23 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:23 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 166]), target_length: tensor([166], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 166])
[DEBUG] target_length = tensor([166], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 166
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 167]), dec_len=tensor([166], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 167]) -> torch.Size([1, 167, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 167, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 607, 512]), dec_out.shape=torch.Size([1, 167, 640])
[DEBUG] enc_len=tensor([607], device='cuda:0'), target_length=tensor([166], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([607], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 607, 512]), dec_out.shape: torch.Size([1, 167, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 607, 167, 1001])

Validation DataLoader 0:  27%|██▋       | 17/63 [00:00<00:02, 22.75it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 633]), enc_len = tensor([632], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 633])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=633
DEBUG: 转换后 enc_out.shape = torch.Size([1, 633, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 633, 512])
DEBUG: actual_T: 633, enc_len调整后: tensor([632], device='cuda:0')
[NeMo I 2025-08-20 09:47:23 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:23 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 162]), target_length: tensor([162], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 162])
[DEBUG] target_length = tensor([162], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 162
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 163]), dec_len=tensor([162], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 163]) -> torch.Size([1, 163, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 163, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 633, 512]), dec_out.shape=torch.Size([1, 163, 640])
[DEBUG] enc_len=tensor([632], device='cuda:0'), target_length=tensor([162], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([632], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 633, 512]), dec_out.shape: torch.Size([1, 163, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 633, 163, 1001])

Validation DataLoader 0:  29%|██▊       | 18/63 [00:00<00:01, 22.85it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 745]), enc_len = tensor([744], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 745])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=745
DEBUG: 转换后 enc_out.shape = torch.Size([1, 745, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 745, 512])
DEBUG: actual_T: 745, enc_len调整后: tensor([744], device='cuda:0')
[NeMo I 2025-08-20 09:47:23 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:23 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 171]), target_length: tensor([171], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 171])
[DEBUG] target_length = tensor([171], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 171
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 172]), dec_len=tensor([171], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 172]) -> torch.Size([1, 172, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 172, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 745, 512]), dec_out.shape=torch.Size([1, 172, 640])
[DEBUG] enc_len=tensor([744], device='cuda:0'), target_length=tensor([171], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([744], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 745, 512]), dec_out.shape: torch.Size([1, 172, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 745, 172, 1001])

Validation DataLoader 0:  30%|███       | 19/63 [00:00<00:01, 22.81it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 825]), enc_len = tensor([825], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 825])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=825
DEBUG: 转换后 enc_out.shape = torch.Size([1, 825, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 825, 512])
DEBUG: actual_T: 825, enc_len调整后: tensor([825], device='cuda:0')
[NeMo I 2025-08-20 09:47:23 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:23 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 168]), target_length: tensor([168], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 168])
[DEBUG] target_length = tensor([168], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 168
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 169]), dec_len=tensor([168], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 169]) -> torch.Size([1, 169, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 169, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 825, 512]), dec_out.shape=torch.Size([1, 169, 640])
[DEBUG] enc_len=tensor([825], device='cuda:0'), target_length=tensor([168], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([825], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 825, 512]), dec_out.shape: torch.Size([1, 169, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 825, 169, 1001])

Validation DataLoader 0:  32%|███▏      | 20/63 [00:00<00:01, 22.85it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 758]), enc_len = tensor([758], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 758])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=758
DEBUG: 转换后 enc_out.shape = torch.Size([1, 758, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 758, 512])
DEBUG: actual_T: 758, enc_len调整后: tensor([758], device='cuda:0')
[NeMo I 2025-08-20 09:47:23 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:23 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 193]), target_length: tensor([193], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 193])
[DEBUG] target_length = tensor([193], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 193
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 194]), dec_len=tensor([193], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 194]) -> torch.Size([1, 194, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 194, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 758, 512]), dec_out.shape=torch.Size([1, 194, 640])
[DEBUG] enc_len=tensor([758], device='cuda:0'), target_length=tensor([193], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([758], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 758, 512]), dec_out.shape: torch.Size([1, 194, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 758, 194, 1001])

Validation DataLoader 0:  33%|███▎      | 21/63 [00:00<00:01, 22.88it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 803]), enc_len = tensor([802], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 803])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=803
DEBUG: 转换后 enc_out.shape = torch.Size([1, 803, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 803, 512])
DEBUG: actual_T: 803, enc_len调整后: tensor([802], device='cuda:0')
[NeMo I 2025-08-20 09:47:23 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:23 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 177]), target_length: tensor([177], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 177])
[DEBUG] target_length = tensor([177], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 177
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 178]), dec_len=tensor([177], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 178]) -> torch.Size([1, 178, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 178, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 803, 512]), dec_out.shape=torch.Size([1, 178, 640])
[DEBUG] enc_len=tensor([802], device='cuda:0'), target_length=tensor([177], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([802], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 803, 512]), dec_out.shape: torch.Size([1, 178, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 803, 178, 1001])

Validation DataLoader 0:  35%|███▍      | 22/63 [00:00<00:01, 22.84it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 731]), enc_len = tensor([731], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 731])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=731
DEBUG: 转换后 enc_out.shape = torch.Size([1, 731, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 731, 512])
DEBUG: actual_T: 731, enc_len调整后: tensor([731], device='cuda:0')
[NeMo I 2025-08-20 09:47:23 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:23 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 176]), target_length: tensor([176], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 176])
[DEBUG] target_length = tensor([176], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 176
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 177]), dec_len=tensor([176], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 177]) -> torch.Size([1, 177, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 177, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 731, 512]), dec_out.shape=torch.Size([1, 177, 640])
[DEBUG] enc_len=tensor([731], device='cuda:0'), target_length=tensor([176], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([731], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 731, 512]), dec_out.shape: torch.Size([1, 177, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 731, 177, 1001])

Validation DataLoader 0:  37%|███▋      | 23/63 [00:01<00:01, 22.90it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 824]), enc_len = tensor([824], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 824])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=824
DEBUG: 转换后 enc_out.shape = torch.Size([1, 824, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 824, 512])
DEBUG: actual_T: 824, enc_len调整后: tensor([824], device='cuda:0')
[NeMo I 2025-08-20 09:47:23 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:23 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 181]), target_length: tensor([181], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 181])
[DEBUG] target_length = tensor([181], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 181
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 182]), dec_len=tensor([181], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 182]) -> torch.Size([1, 182, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 182, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 824, 512]), dec_out.shape=torch.Size([1, 182, 640])
[DEBUG] enc_len=tensor([824], device='cuda:0'), target_length=tensor([181], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([824], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 824, 512]), dec_out.shape: torch.Size([1, 182, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 824, 182, 1001])

Validation DataLoader 0:  38%|███▊      | 24/63 [00:01<00:01, 22.76it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 559]), enc_len = tensor([559], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 559])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=559
DEBUG: 转换后 enc_out.shape = torch.Size([1, 559, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 559, 512])
DEBUG: actual_T: 559, enc_len调整后: tensor([559], device='cuda:0')
[NeMo I 2025-08-20 09:47:24 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:24 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 128]), target_length: tensor([128], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 128])
[DEBUG] target_length = tensor([128], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 128
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 129]), dec_len=tensor([128], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 129]) -> torch.Size([1, 129, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 129, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 559, 512]), dec_out.shape=torch.Size([1, 129, 640])
[DEBUG] enc_len=tensor([559], device='cuda:0'), target_length=tensor([128], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([559], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 559, 512]), dec_out.shape: torch.Size([1, 129, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 559, 129, 1001])

Validation DataLoader 0:  40%|███▉      | 25/63 [00:01<00:01, 22.84it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 440]), enc_len = tensor([439], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 440])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=440
DEBUG: 转换后 enc_out.shape = torch.Size([1, 440, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 440, 512])
DEBUG: actual_T: 440, enc_len调整后: tensor([439], device='cuda:0')
[NeMo I 2025-08-20 09:47:24 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:24 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 92]), target_length: tensor([92], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 92])
[DEBUG] target_length = tensor([92], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 92
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 93]), dec_len=tensor([92], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 93]) -> torch.Size([1, 93, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 93, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 440, 512]), dec_out.shape=torch.Size([1, 93, 640])
[DEBUG] enc_len=tensor([439], device='cuda:0'), target_length=tensor([92], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([439], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 440, 512]), dec_out.shape: torch.Size([1, 93, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 440, 93, 1001])

Validation DataLoader 0:  41%|████▏     | 26/63 [00:01<00:01, 22.89it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 642]), enc_len = tensor([642], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 642])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=642
DEBUG: 转换后 enc_out.shape = torch.Size([1, 642, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 642, 512])
DEBUG: actual_T: 642, enc_len调整后: tensor([642], device='cuda:0')
[NeMo I 2025-08-20 09:47:24 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:24 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 145]), target_length: tensor([145], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 145])
[DEBUG] target_length = tensor([145], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 145
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 146]), dec_len=tensor([145], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 146]) -> torch.Size([1, 146, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 146, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 642, 512]), dec_out.shape=torch.Size([1, 146, 640])
[DEBUG] enc_len=tensor([642], device='cuda:0'), target_length=tensor([145], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([642], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 642, 512]), dec_out.shape: torch.Size([1, 146, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 642, 146, 1001])

Validation DataLoader 0:  43%|████▎     | 27/63 [00:01<00:01, 22.93it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 925]), enc_len = tensor([925], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 925])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=925
DEBUG: 转换后 enc_out.shape = torch.Size([1, 925, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 925, 512])
DEBUG: actual_T: 925, enc_len调整后: tensor([925], device='cuda:0')
[NeMo I 2025-08-20 09:47:24 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:24 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 227]), target_length: tensor([227], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 227])
[DEBUG] target_length = tensor([227], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 227
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 228]), dec_len=tensor([227], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 228]) -> torch.Size([1, 228, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 228, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 925, 512]), dec_out.shape=torch.Size([1, 228, 640])
[DEBUG] enc_len=tensor([925], device='cuda:0'), target_length=tensor([227], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([925], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 925, 512]), dec_out.shape: torch.Size([1, 228, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 925, 228, 1001])

Validation DataLoader 0:  44%|████▍     | 28/63 [00:01<00:01, 22.86it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 952]), enc_len = tensor([952], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 952])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=952
DEBUG: 转换后 enc_out.shape = torch.Size([1, 952, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 952, 512])
DEBUG: actual_T: 952, enc_len调整后: tensor([952], device='cuda:0')
[NeMo I 2025-08-20 09:47:24 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:24 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 233]), target_length: tensor([233], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 233])
[DEBUG] target_length = tensor([233], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 233
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 234]), dec_len=tensor([233], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 234]) -> torch.Size([1, 234, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 234, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 952, 512]), dec_out.shape=torch.Size([1, 234, 640])
[DEBUG] enc_len=tensor([952], device='cuda:0'), target_length=tensor([233], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([952], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 952, 512]), dec_out.shape: torch.Size([1, 234, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 952, 234, 1001])

Validation DataLoader 0:  46%|████▌     | 29/63 [00:01<00:01, 22.81it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 562]), enc_len = tensor([562], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 562])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=562
DEBUG: 转换后 enc_out.shape = torch.Size([1, 562, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 562, 512])
DEBUG: actual_T: 562, enc_len调整后: tensor([562], device='cuda:0')
[NeMo I 2025-08-20 09:47:24 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:24 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 139]), target_length: tensor([139], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 139])
[DEBUG] target_length = tensor([139], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 139
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 140]), dec_len=tensor([139], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 140]) -> torch.Size([1, 140, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 140, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 562, 512]), dec_out.shape=torch.Size([1, 140, 640])
[DEBUG] enc_len=tensor([562], device='cuda:0'), target_length=tensor([139], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([562], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 562, 512]), dec_out.shape: torch.Size([1, 140, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 562, 140, 1001])

Validation DataLoader 0:  48%|████▊     | 30/63 [00:01<00:01, 22.82it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 726]), enc_len = tensor([725], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 726])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=726
DEBUG: 转换后 enc_out.shape = torch.Size([1, 726, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 726, 512])
DEBUG: actual_T: 726, enc_len调整后: tensor([725], device='cuda:0')
[NeMo I 2025-08-20 09:47:24 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:24 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 178]), target_length: tensor([178], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 178])
[DEBUG] target_length = tensor([178], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 178
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 179]), dec_len=tensor([178], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 179]) -> torch.Size([1, 179, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 179, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 726, 512]), dec_out.shape=torch.Size([1, 179, 640])
[DEBUG] enc_len=tensor([725], device='cuda:0'), target_length=tensor([178], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([725], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 726, 512]), dec_out.shape: torch.Size([1, 179, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 726, 179, 1001])

Validation DataLoader 0:  49%|████▉     | 31/63 [00:01<00:01, 22.85it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 606]), enc_len = tensor([605], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 606])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=606
DEBUG: 转换后 enc_out.shape = torch.Size([1, 606, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 606, 512])
DEBUG: actual_T: 606, enc_len调整后: tensor([605], device='cuda:0')
[NeMo I 2025-08-20 09:47:24 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:24 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 144]), target_length: tensor([144], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 144])
[DEBUG] target_length = tensor([144], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 144
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 145]), dec_len=tensor([144], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 145]) -> torch.Size([1, 145, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 145, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 606, 512]), dec_out.shape=torch.Size([1, 145, 640])
[DEBUG] enc_len=tensor([605], device='cuda:0'), target_length=tensor([144], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([605], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 606, 512]), dec_out.shape: torch.Size([1, 145, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 606, 145, 1001])

Validation DataLoader 0:  51%|█████     | 32/63 [00:01<00:01, 22.91it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 468]), enc_len = tensor([468], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 468])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=468
DEBUG: 转换后 enc_out.shape = torch.Size([1, 468, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 468, 512])
DEBUG: actual_T: 468, enc_len调整后: tensor([468], device='cuda:0')
[NeMo I 2025-08-20 09:47:24 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:24 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 111]), target_length: tensor([111], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 111])
[DEBUG] target_length = tensor([111], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 111
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 112]), dec_len=tensor([111], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 112]) -> torch.Size([1, 112, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 112, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 468, 512]), dec_out.shape=torch.Size([1, 112, 640])
[DEBUG] enc_len=tensor([468], device='cuda:0'), target_length=tensor([111], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([468], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 468, 512]), dec_out.shape: torch.Size([1, 112, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 468, 112, 1001])

Validation DataLoader 0:  52%|█████▏    | 33/63 [00:01<00:01, 22.99it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 575]), enc_len = tensor([575], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 575])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=575
DEBUG: 转换后 enc_out.shape = torch.Size([1, 575, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 575, 512])
DEBUG: actual_T: 575, enc_len调整后: tensor([575], device='cuda:0')
[NeMo I 2025-08-20 09:47:24 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:24 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 114]), target_length: tensor([114], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 114])
[DEBUG] target_length = tensor([114], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 114
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 115]), dec_len=tensor([114], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 115]) -> torch.Size([1, 115, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 115, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 575, 512]), dec_out.shape=torch.Size([1, 115, 640])
[DEBUG] enc_len=tensor([575], device='cuda:0'), target_length=tensor([114], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([575], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 575, 512]), dec_out.shape: torch.Size([1, 115, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 575, 115, 1001])

Validation DataLoader 0:  54%|█████▍    | 34/63 [00:01<00:01, 22.97it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 787]), enc_len = tensor([787], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 787])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=787
DEBUG: 转换后 enc_out.shape = torch.Size([1, 787, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 787, 512])
DEBUG: actual_T: 787, enc_len调整后: tensor([787], device='cuda:0')
[NeMo I 2025-08-20 09:47:24 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:24 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 189]), target_length: tensor([189], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 189])
[DEBUG] target_length = tensor([189], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 189
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 190]), dec_len=tensor([189], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 190]) -> torch.Size([1, 190, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 190, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 787, 512]), dec_out.shape=torch.Size([1, 190, 640])
[DEBUG] enc_len=tensor([787], device='cuda:0'), target_length=tensor([189], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([787], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 787, 512]), dec_out.shape: torch.Size([1, 190, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 787, 190, 1001])

Validation DataLoader 0:  56%|█████▌    | 35/63 [00:01<00:01, 22.98it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 812]), enc_len = tensor([812], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 812])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=812
DEBUG: 转换后 enc_out.shape = torch.Size([1, 812, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 812, 512])
DEBUG: actual_T: 812, enc_len调整后: tensor([812], device='cuda:0')
[NeMo I 2025-08-20 09:47:24 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:24 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 205]), target_length: tensor([205], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 205])
[DEBUG] target_length = tensor([205], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 205
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 206]), dec_len=tensor([205], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 206]) -> torch.Size([1, 206, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 206, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 812, 512]), dec_out.shape=torch.Size([1, 206, 640])
[DEBUG] enc_len=tensor([812], device='cuda:0'), target_length=tensor([205], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([812], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 812, 512]), dec_out.shape: torch.Size([1, 206, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 812, 206, 1001])

Validation DataLoader 0:  57%|█████▋    | 36/63 [00:01<00:01, 22.98it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 549]), enc_len = tensor([549], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 549])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=549
DEBUG: 转换后 enc_out.shape = torch.Size([1, 549, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 549, 512])
DEBUG: actual_T: 549, enc_len调整后: tensor([549], device='cuda:0')
[NeMo I 2025-08-20 09:47:24 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:24 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 114]), target_length: tensor([114], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 114])
[DEBUG] target_length = tensor([114], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 114
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 115]), dec_len=tensor([114], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 115]) -> torch.Size([1, 115, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 115, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 549, 512]), dec_out.shape=torch.Size([1, 115, 640])
[DEBUG] enc_len=tensor([549], device='cuda:0'), target_length=tensor([114], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([549], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 549, 512]), dec_out.shape: torch.Size([1, 115, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 549, 115, 1001])

Validation DataLoader 0:  59%|█████▊    | 37/63 [00:01<00:01, 23.05it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 828]), enc_len = tensor([828], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 828])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=828
DEBUG: 转换后 enc_out.shape = torch.Size([1, 828, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 828, 512])
DEBUG: actual_T: 828, enc_len调整后: tensor([828], device='cuda:0')
[NeMo I 2025-08-20 09:47:24 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:24 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 199]), target_length: tensor([199], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 199])
[DEBUG] target_length = tensor([199], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 199
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 200]), dec_len=tensor([199], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 200]) -> torch.Size([1, 200, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 200, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 828, 512]), dec_out.shape=torch.Size([1, 200, 640])
[DEBUG] enc_len=tensor([828], device='cuda:0'), target_length=tensor([199], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([828], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 828, 512]), dec_out.shape: torch.Size([1, 200, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 828, 200, 1001])

Validation DataLoader 0:  60%|██████    | 38/63 [00:01<00:01, 22.99it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 731]), enc_len = tensor([731], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 731])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=731
DEBUG: 转换后 enc_out.shape = torch.Size([1, 731, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 731, 512])
DEBUG: actual_T: 731, enc_len调整后: tensor([731], device='cuda:0')
[NeMo I 2025-08-20 09:47:24 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:24 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 175]), target_length: tensor([175], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 175])
[DEBUG] target_length = tensor([175], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 175
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 176]), dec_len=tensor([175], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 176]) -> torch.Size([1, 176, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 176, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 731, 512]), dec_out.shape=torch.Size([1, 176, 640])
[DEBUG] enc_len=tensor([731], device='cuda:0'), target_length=tensor([175], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([731], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 731, 512]), dec_out.shape: torch.Size([1, 176, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 731, 176, 1001])

Validation DataLoader 0:  62%|██████▏   | 39/63 [00:01<00:01, 23.02it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 803]), enc_len = tensor([803], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 803])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=803
DEBUG: 转换后 enc_out.shape = torch.Size([1, 803, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 803, 512])
DEBUG: actual_T: 803, enc_len调整后: tensor([803], device='cuda:0')
[NeMo I 2025-08-20 09:47:24 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:24 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 202]), target_length: tensor([202], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 202])
[DEBUG] target_length = tensor([202], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 202
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 203]), dec_len=tensor([202], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 203]) -> torch.Size([1, 203, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 203, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 803, 512]), dec_out.shape=torch.Size([1, 203, 640])
[DEBUG] enc_len=tensor([803], device='cuda:0'), target_length=tensor([202], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([803], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 803, 512]), dec_out.shape: torch.Size([1, 203, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 803, 203, 1001])

Validation DataLoader 0:  63%|██████▎   | 40/63 [00:01<00:00, 23.02it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 773]), enc_len = tensor([773], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 773])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=773
DEBUG: 转换后 enc_out.shape = torch.Size([1, 773, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 773, 512])
DEBUG: actual_T: 773, enc_len调整后: tensor([773], device='cuda:0')
[NeMo I 2025-08-20 09:47:24 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:24 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 184]), target_length: tensor([184], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 184])
[DEBUG] target_length = tensor([184], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 184
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 185]), dec_len=tensor([184], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 185]) -> torch.Size([1, 185, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 185, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 773, 512]), dec_out.shape=torch.Size([1, 185, 640])
[DEBUG] enc_len=tensor([773], device='cuda:0'), target_length=tensor([184], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([773], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 773, 512]), dec_out.shape: torch.Size([1, 185, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 773, 185, 1001])

Validation DataLoader 0:  65%|██████▌   | 41/63 [00:01<00:00, 23.04it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 466]), enc_len = tensor([466], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 466])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=466
DEBUG: 转换后 enc_out.shape = torch.Size([1, 466, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 466, 512])
DEBUG: actual_T: 466, enc_len调整后: tensor([466], device='cuda:0')
[NeMo I 2025-08-20 09:47:24 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:24 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 119]), target_length: tensor([119], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 119])
[DEBUG] target_length = tensor([119], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 119
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 120]), dec_len=tensor([119], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 120]) -> torch.Size([1, 120, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 120, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 466, 512]), dec_out.shape=torch.Size([1, 120, 640])
[DEBUG] enc_len=tensor([466], device='cuda:0'), target_length=tensor([119], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([466], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 466, 512]), dec_out.shape: torch.Size([1, 120, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 466, 120, 1001])

Validation DataLoader 0:  67%|██████▋   | 42/63 [00:01<00:00, 23.05it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 877]), enc_len = tensor([877], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 877])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=877
DEBUG: 转换后 enc_out.shape = torch.Size([1, 877, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 877, 512])
DEBUG: actual_T: 877, enc_len调整后: tensor([877], device='cuda:0')
[NeMo I 2025-08-20 09:47:24 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:24 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 196]), target_length: tensor([196], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 196])
[DEBUG] target_length = tensor([196], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 196
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 197]), dec_len=tensor([196], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 197]) -> torch.Size([1, 197, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 197, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 877, 512]), dec_out.shape=torch.Size([1, 197, 640])
[DEBUG] enc_len=tensor([877], device='cuda:0'), target_length=tensor([196], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([877], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 877, 512]), dec_out.shape: torch.Size([1, 197, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 877, 197, 1001])

Validation DataLoader 0:  68%|██████▊   | 43/63 [00:01<00:00, 23.05it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 613]), enc_len = tensor([613], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 613])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=613
DEBUG: 转换后 enc_out.shape = torch.Size([1, 613, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 613, 512])
DEBUG: actual_T: 613, enc_len调整后: tensor([613], device='cuda:0')
[NeMo I 2025-08-20 09:47:24 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:24 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 129]), target_length: tensor([129], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 129])
[DEBUG] target_length = tensor([129], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 129
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 130]), dec_len=tensor([129], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 130]) -> torch.Size([1, 130, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 130, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 613, 512]), dec_out.shape=torch.Size([1, 130, 640])
[DEBUG] enc_len=tensor([613], device='cuda:0'), target_length=tensor([129], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([613], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 613, 512]), dec_out.shape: torch.Size([1, 130, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 613, 130, 1001])

Validation DataLoader 0:  70%|██████▉   | 44/63 [00:01<00:00, 23.10it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 807]), enc_len = tensor([807], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 807])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=807
DEBUG: 转换后 enc_out.shape = torch.Size([1, 807, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 807, 512])
DEBUG: actual_T: 807, enc_len调整后: tensor([807], device='cuda:0')
[NeMo I 2025-08-20 09:47:24 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:24 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 192]), target_length: tensor([192], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 192])
[DEBUG] target_length = tensor([192], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 192
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 193]), dec_len=tensor([192], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 193]) -> torch.Size([1, 193, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 193, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 807, 512]), dec_out.shape=torch.Size([1, 193, 640])
[DEBUG] enc_len=tensor([807], device='cuda:0'), target_length=tensor([192], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([807], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 807, 512]), dec_out.shape: torch.Size([1, 193, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 807, 193, 1001])

Validation DataLoader 0:  71%|███████▏  | 45/63 [00:01<00:00, 23.11it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 732]), enc_len = tensor([732], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 732])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=732
DEBUG: 转换后 enc_out.shape = torch.Size([1, 732, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 732, 512])
DEBUG: actual_T: 732, enc_len调整后: tensor([732], device='cuda:0')
[NeMo I 2025-08-20 09:47:24 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:24 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 145]), target_length: tensor([145], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 145])
[DEBUG] target_length = tensor([145], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 145
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 146]), dec_len=tensor([145], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 146]) -> torch.Size([1, 146, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 146, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 732, 512]), dec_out.shape=torch.Size([1, 146, 640])
[DEBUG] enc_len=tensor([732], device='cuda:0'), target_length=tensor([145], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([732], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 732, 512]), dec_out.shape: torch.Size([1, 146, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 732, 146, 1001])

Validation DataLoader 0:  73%|███████▎  | 46/63 [00:01<00:00, 23.07it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 754]), enc_len = tensor([754], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 754])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=754
DEBUG: 转换后 enc_out.shape = torch.Size([1, 754, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 754, 512])
DEBUG: actual_T: 754, enc_len调整后: tensor([754], device='cuda:0')
[NeMo I 2025-08-20 09:47:24 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:24 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 170]), target_length: tensor([170], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 170])
[DEBUG] target_length = tensor([170], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 170
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 171]), dec_len=tensor([170], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 171]) -> torch.Size([1, 171, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 171, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 754, 512]), dec_out.shape=torch.Size([1, 171, 640])
[DEBUG] enc_len=tensor([754], device='cuda:0'), target_length=tensor([170], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([754], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 754, 512]), dec_out.shape: torch.Size([1, 171, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 754, 171, 1001])

Validation DataLoader 0:  75%|███████▍  | 47/63 [00:02<00:00, 23.09it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 772]), enc_len = tensor([772], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 772])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=772
DEBUG: 转换后 enc_out.shape = torch.Size([1, 772, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 772, 512])
DEBUG: actual_T: 772, enc_len调整后: tensor([772], device='cuda:0')
[NeMo I 2025-08-20 09:47:24 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:24 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 188]), target_length: tensor([188], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 188])
[DEBUG] target_length = tensor([188], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 188
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 189]), dec_len=tensor([188], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 189]) -> torch.Size([1, 189, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 189, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 772, 512]), dec_out.shape=torch.Size([1, 189, 640])
[DEBUG] enc_len=tensor([772], device='cuda:0'), target_length=tensor([188], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([772], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 772, 512]), dec_out.shape: torch.Size([1, 189, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 772, 189, 1001])

Validation DataLoader 0:  76%|███████▌  | 48/63 [00:02<00:00, 23.11it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 585]), enc_len = tensor([584], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 585])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=585
DEBUG: 转换后 enc_out.shape = torch.Size([1, 585, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 585, 512])
DEBUG: actual_T: 585, enc_len调整后: tensor([584], device='cuda:0')
[NeMo I 2025-08-20 09:47:25 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:25 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 112]), target_length: tensor([112], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 112])
[DEBUG] target_length = tensor([112], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 112
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 113]), dec_len=tensor([112], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 113]) -> torch.Size([1, 113, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 113, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 585, 512]), dec_out.shape=torch.Size([1, 113, 640])
[DEBUG] enc_len=tensor([584], device='cuda:0'), target_length=tensor([112], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([584], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 585, 512]), dec_out.shape: torch.Size([1, 113, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 585, 113, 1001])

Validation DataLoader 0:  78%|███████▊  | 49/63 [00:02<00:00, 23.11it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 713]), enc_len = tensor([713], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 713])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=713
DEBUG: 转换后 enc_out.shape = torch.Size([1, 713, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 713, 512])
DEBUG: actual_T: 713, enc_len调整后: tensor([713], device='cuda:0')
[NeMo I 2025-08-20 09:47:25 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:25 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 162]), target_length: tensor([162], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 162])
[DEBUG] target_length = tensor([162], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 162
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 163]), dec_len=tensor([162], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 163]) -> torch.Size([1, 163, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 163, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 713, 512]), dec_out.shape=torch.Size([1, 163, 640])
[DEBUG] enc_len=tensor([713], device='cuda:0'), target_length=tensor([162], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([713], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 713, 512]), dec_out.shape: torch.Size([1, 163, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 713, 163, 1001])

Validation DataLoader 0:  79%|███████▉  | 50/63 [00:02<00:00, 23.07it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 570]), enc_len = tensor([570], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 570])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=570
DEBUG: 转换后 enc_out.shape = torch.Size([1, 570, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 570, 512])
DEBUG: actual_T: 570, enc_len调整后: tensor([570], device='cuda:0')
[NeMo I 2025-08-20 09:47:25 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:25 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 108]), target_length: tensor([108], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 108])
[DEBUG] target_length = tensor([108], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 108
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 109]), dec_len=tensor([108], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 109]) -> torch.Size([1, 109, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 109, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 570, 512]), dec_out.shape=torch.Size([1, 109, 640])
[DEBUG] enc_len=tensor([570], device='cuda:0'), target_length=tensor([108], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([570], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 570, 512]), dec_out.shape: torch.Size([1, 109, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 570, 109, 1001])

Validation DataLoader 0:  81%|████████  | 51/63 [00:02<00:00, 23.07it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 745]), enc_len = tensor([745], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 745])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=745
DEBUG: 转换后 enc_out.shape = torch.Size([1, 745, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 745, 512])
DEBUG: actual_T: 745, enc_len调整后: tensor([745], device='cuda:0')
[NeMo I 2025-08-20 09:47:25 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:25 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 185]), target_length: tensor([185], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 185])
[DEBUG] target_length = tensor([185], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 185
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 186]), dec_len=tensor([185], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 186]) -> torch.Size([1, 186, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 186, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 745, 512]), dec_out.shape=torch.Size([1, 186, 640])
[DEBUG] enc_len=tensor([745], device='cuda:0'), target_length=tensor([185], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([745], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 745, 512]), dec_out.shape: torch.Size([1, 186, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 745, 186, 1001])

Validation DataLoader 0:  83%|████████▎ | 52/63 [00:02<00:00, 23.05it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 973]), enc_len = tensor([973], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 973])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=973
DEBUG: 转换后 enc_out.shape = torch.Size([1, 973, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 973, 512])
DEBUG: actual_T: 973, enc_len调整后: tensor([973], device='cuda:0')
[NeMo I 2025-08-20 09:47:25 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:25 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 241]), target_length: tensor([241], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 241])
[DEBUG] target_length = tensor([241], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 241
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 242]), dec_len=tensor([241], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 242]) -> torch.Size([1, 242, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 242, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 973, 512]), dec_out.shape=torch.Size([1, 242, 640])
[DEBUG] enc_len=tensor([973], device='cuda:0'), target_length=tensor([241], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([973], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 973, 512]), dec_out.shape: torch.Size([1, 242, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 973, 242, 1001])

Validation DataLoader 0:  84%|████████▍ | 53/63 [00:02<00:00, 22.99it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 912]), enc_len = tensor([912], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 912])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=912
DEBUG: 转换后 enc_out.shape = torch.Size([1, 912, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 912, 512])
DEBUG: actual_T: 912, enc_len调整后: tensor([912], device='cuda:0')
[NeMo I 2025-08-20 09:47:25 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:25 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 225]), target_length: tensor([225], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 225])
[DEBUG] target_length = tensor([225], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 225
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 226]), dec_len=tensor([225], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 226]) -> torch.Size([1, 226, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 226, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 912, 512]), dec_out.shape=torch.Size([1, 226, 640])
[DEBUG] enc_len=tensor([912], device='cuda:0'), target_length=tensor([225], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([912], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 912, 512]), dec_out.shape: torch.Size([1, 226, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 912, 226, 1001])

Validation DataLoader 0:  86%|████████▌ | 54/63 [00:02<00:00, 22.97it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 869]), enc_len = tensor([869], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 869])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=869
DEBUG: 转换后 enc_out.shape = torch.Size([1, 869, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 869, 512])
DEBUG: actual_T: 869, enc_len调整后: tensor([869], device='cuda:0')
[NeMo I 2025-08-20 09:47:25 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:25 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 210]), target_length: tensor([210], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 210])
[DEBUG] target_length = tensor([210], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 210
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 211]), dec_len=tensor([210], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 211]) -> torch.Size([1, 211, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 211, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 869, 512]), dec_out.shape=torch.Size([1, 211, 640])
[DEBUG] enc_len=tensor([869], device='cuda:0'), target_length=tensor([210], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([869], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 869, 512]), dec_out.shape: torch.Size([1, 211, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 869, 211, 1001])

Validation DataLoader 0:  87%|████████▋ | 55/63 [00:02<00:00, 22.96it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 577]), enc_len = tensor([577], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 577])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=577
DEBUG: 转换后 enc_out.shape = torch.Size([1, 577, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 577, 512])
DEBUG: actual_T: 577, enc_len调整后: tensor([577], device='cuda:0')
[NeMo I 2025-08-20 09:47:25 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:25 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 153]), target_length: tensor([153], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 153])
[DEBUG] target_length = tensor([153], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 153
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 154]), dec_len=tensor([153], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 154]) -> torch.Size([1, 154, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 154, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 577, 512]), dec_out.shape=torch.Size([1, 154, 640])
[DEBUG] enc_len=tensor([577], device='cuda:0'), target_length=tensor([153], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([577], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 577, 512]), dec_out.shape: torch.Size([1, 154, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 577, 154, 1001])

Validation DataLoader 0:  89%|████████▉ | 56/63 [00:02<00:00, 23.00it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 912]), enc_len = tensor([912], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 912])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=912
DEBUG: 转换后 enc_out.shape = torch.Size([1, 912, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 912, 512])
DEBUG: actual_T: 912, enc_len调整后: tensor([912], device='cuda:0')
[NeMo I 2025-08-20 09:47:25 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:25 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 204]), target_length: tensor([204], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 204])
[DEBUG] target_length = tensor([204], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 204
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 205]), dec_len=tensor([204], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 205]) -> torch.Size([1, 205, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 205, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 912, 512]), dec_out.shape=torch.Size([1, 205, 640])
[DEBUG] enc_len=tensor([912], device='cuda:0'), target_length=tensor([204], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([912], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 912, 512]), dec_out.shape: torch.Size([1, 205, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 912, 205, 1001])

Validation DataLoader 0:  90%|█████████ | 57/63 [00:02<00:00, 22.99it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 522]), enc_len = tensor([522], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 522])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=522
DEBUG: 转换后 enc_out.shape = torch.Size([1, 522, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 522, 512])
DEBUG: actual_T: 522, enc_len调整后: tensor([522], device='cuda:0')
[NeMo I 2025-08-20 09:47:25 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:25 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 111]), target_length: tensor([111], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 111])
[DEBUG] target_length = tensor([111], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 111
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 112]), dec_len=tensor([111], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 112]) -> torch.Size([1, 112, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 112, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 522, 512]), dec_out.shape=torch.Size([1, 112, 640])
[DEBUG] enc_len=tensor([522], device='cuda:0'), target_length=tensor([111], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([522], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 522, 512]), dec_out.shape: torch.Size([1, 112, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 522, 112, 1001])

Validation DataLoader 0:  92%|█████████▏| 58/63 [00:02<00:00, 23.04it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 790]), enc_len = tensor([790], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 790])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=790
DEBUG: 转换后 enc_out.shape = torch.Size([1, 790, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 790, 512])
DEBUG: actual_T: 790, enc_len调整后: tensor([790], device='cuda:0')
[NeMo I 2025-08-20 09:47:25 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:25 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 213]), target_length: tensor([213], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 213])
[DEBUG] target_length = tensor([213], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 213
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 214]), dec_len=tensor([213], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 214]) -> torch.Size([1, 214, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 214, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 790, 512]), dec_out.shape=torch.Size([1, 214, 640])
[DEBUG] enc_len=tensor([790], device='cuda:0'), target_length=tensor([213], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([790], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 790, 512]), dec_out.shape: torch.Size([1, 214, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 790, 214, 1001])

Validation DataLoader 0:  94%|█████████▎| 59/63 [00:02<00:00, 23.03it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 1009]), enc_len = tensor([1008], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 1009])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=1009
DEBUG: 转换后 enc_out.shape = torch.Size([1, 1009, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 1009, 512])
DEBUG: actual_T: 1009, enc_len调整后: tensor([1008], device='cuda:0')
[NeMo I 2025-08-20 09:47:25 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:25 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 262]), target_length: tensor([262], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 262])
[DEBUG] target_length = tensor([262], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 262
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 263]), dec_len=tensor([262], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 263]) -> torch.Size([1, 263, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 263, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 1009, 512]), dec_out.shape=torch.Size([1, 263, 640])
[DEBUG] enc_len=tensor([1008], device='cuda:0'), target_length=tensor([262], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([1008], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 1009, 512]), dec_out.shape: torch.Size([1, 263, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 1009, 263, 1001])

Validation DataLoader 0:  95%|█████████▌| 60/63 [00:02<00:00, 22.99it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 383]), enc_len = tensor([382], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 383])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=383
DEBUG: 转换后 enc_out.shape = torch.Size([1, 383, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 383, 512])
DEBUG: actual_T: 383, enc_len调整后: tensor([382], device='cuda:0')
[NeMo I 2025-08-20 09:47:25 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:25 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 101]), target_length: tensor([101], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 101])
[DEBUG] target_length = tensor([101], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 101
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 102]), dec_len=tensor([101], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 102]) -> torch.Size([1, 102, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 102, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 383, 512]), dec_out.shape=torch.Size([1, 102, 640])
[DEBUG] enc_len=tensor([382], device='cuda:0'), target_length=tensor([101], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([382], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 383, 512]), dec_out.shape: torch.Size([1, 102, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 383, 102, 1001])

Validation DataLoader 0:  97%|█████████▋| 61/63 [00:02<00:00, 23.04it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 721]), enc_len = tensor([721], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 721])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=721
DEBUG: 转换后 enc_out.shape = torch.Size([1, 721, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 721, 512])
DEBUG: actual_T: 721, enc_len调整后: tensor([721], device='cuda:0')
[NeMo I 2025-08-20 09:47:25 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:25 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 135]), target_length: tensor([135], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 135])
[DEBUG] target_length = tensor([135], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 135
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 136]), dec_len=tensor([135], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 136]) -> torch.Size([1, 136, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 136, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 721, 512]), dec_out.shape=torch.Size([1, 136, 640])
[DEBUG] enc_len=tensor([721], device='cuda:0'), target_length=tensor([135], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([721], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 721, 512]), dec_out.shape: torch.Size([1, 136, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 721, 136, 1001])

Validation DataLoader 0:  98%|█████████▊| 62/63 [00:02<00:00, 23.08it/s][ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 833]), enc_len = tensor([833], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 833])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=833
DEBUG: 转换后 enc_out.shape = torch.Size([1, 833, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 833, 512])
DEBUG: actual_T: 833, enc_len调整后: tensor([833], device='cuda:0')
[NeMo I 2025-08-20 09:47:25 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:25 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 188]), target_length: tensor([188], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 188])
[DEBUG] target_length = tensor([188], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 188
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 189]), dec_len=tensor([188], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 189]) -> torch.Size([1, 189, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 189, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 833, 512]), dec_out.shape=torch.Size([1, 189, 640])
[DEBUG] enc_len=tensor([833], device='cuda:0'), target_length=tensor([188], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([833], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 833, 512]), dec_out.shape: torch.Size([1, 189, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 833, 189, 1001])

Validation DataLoader 0: 100%|██████████| 63/63 [00:02<00:00, 23.10it/s][AEpoch 0:  95%|█████████▌| 60/63 [00:19<00:00,  3.08it/s, v_num=31, train_loss_step=902.0, val_loss=858.0]  
                                                                        [ADEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 745]), enc_len = tensor([744], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 745])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=745
DEBUG: 转换后 enc_out.shape = torch.Size([1, 745, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 745, 512])
DEBUG: actual_T: 745, enc_len调整后: tensor([744], device='cuda:0')
[NeMo I 2025-08-20 09:47:26 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:26 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 171]), target_length: tensor([171], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 171])
[DEBUG] target_length = tensor([171], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 171
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 172]), dec_len=tensor([171], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 172]) -> torch.Size([1, 172, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 172, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 745, 512]), dec_out.shape=torch.Size([1, 172, 640])
[DEBUG] enc_len=tensor([744], device='cuda:0'), target_length=tensor([171], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([744], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 745, 512]), dec_out.shape: torch.Size([1, 172, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 745, 172, 1001])
Epoch 0:  97%|█████████▋| 61/63 [00:20<00:00,  2.96it/s, v_num=31, train_loss_step=902.0, val_loss=858.0]Epoch 0:  97%|█████████▋| 61/63 [00:20<00:00,  2.96it/s, v_num=31, train_loss_step=855.0, val_loss=858.0]DEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 807]), enc_len = tensor([807], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 807])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=807
DEBUG: 转换后 enc_out.shape = torch.Size([1, 807, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 807, 512])
DEBUG: actual_T: 807, enc_len调整后: tensor([807], device='cuda:0')
[NeMo I 2025-08-20 09:47:26 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:26 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 192]), target_length: tensor([192], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 192])
[DEBUG] target_length = tensor([192], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 192
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 193]), dec_len=tensor([192], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 193]) -> torch.Size([1, 193, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 193, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 807, 512]), dec_out.shape=torch.Size([1, 193, 640])
[DEBUG] enc_len=tensor([807], device='cuda:0'), target_length=tensor([192], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([807], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 807, 512]), dec_out.shape: torch.Size([1, 193, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 807, 193, 1001])
Epoch 0:  98%|█████████▊| 62/63 [00:20<00:00,  2.99it/s, v_num=31, train_loss_step=855.0, val_loss=858.0]Epoch 0:  98%|█████████▊| 62/63 [00:20<00:00,  2.99it/s, v_num=31, train_loss_step=914.0, val_loss=858.0]DEBUG: 原始编码器输出 enc_out.shape = torch.Size([1, 512, 824]), enc_len = tensor([824], device='cuda:0')
DEBUG: enc_out.shape = torch.Size([1, 512, 824])
DEBUG: 检测到格式 [B, M, T_enc]: B=1, M=512, T_enc=824
DEBUG: 转换后 enc_out.shape = torch.Size([1, 824, 512])
DEBUG: 注入后编码器输出 enc_out.shape = torch.Size([1, 824, 512])
DEBUG: actual_T: 824, enc_len调整后: tensor([824], device='cuda:0')
[NeMo I 2025-08-20 09:47:26 train_multispeaker_asr_adapter:212] [DEBUG] tokenizer类型: <class 'omegaconf.listconfig.ListConfig'>
[NeMo W 2025-08-20 09:47:26 train_multispeaker_asr_adapter:226] [DEBUG] 使用ASCII fallback tokenization
[DEBUG] targets.shape: torch.Size([1, 181]), target_length: tensor([181], device='cuda:0', dtype=torch.int32)
[DEBUG] targets.shape = torch.Size([1, 181])
[DEBUG] target_length = tensor([181], device='cuda:0', dtype=torch.int32)
[DEBUG] target_length.max() = 181
[DEBUG] decoder_output type: <class 'tuple'>, length: 3
[DEBUG] 解码器输出 (3元组): dec_out.shape=torch.Size([1, 640, 182]), dec_len=tensor([181], device='cuda:0', dtype=torch.int32)
[DEBUG] 转换解码器输出形状: torch.Size([1, 640, 182]) -> torch.Size([1, 182, 640])
[DEBUG] Final dec_out shape: torch.Size([1, 182, 640]) (expected: [B, U, D])
[DEBUG] 调用joint前: enc_out.shape=torch.Size([1, 824, 512]), dec_out.shape=torch.Size([1, 182, 640])
[DEBUG] enc_len=tensor([824], device='cuda:0'), target_length=tensor([181], device='cuda:0', dtype=torch.int32)
[DEBUG] 使用原始enc_len: tensor([824], device='cuda:0')
[DEBUG] 调用 joint.joint() 方法
[DEBUG] enc_out.shape: torch.Size([1, 824, 512]), dec_out.shape: torch.Size([1, 182, 640])
[DEBUG] joint 输出 logits.shape: torch.Size([1, 824, 182, 1001])
Epoch 0: 100%|██████████| 63/63 [00:20<00:00,  3.03it/s, v_num=31, train_loss_step=914.0, val_loss=858.0]Epoch 0: 100%|██████████| 63/63 [00:20<00:00,  3.03it/s, v_num=31, train_loss_step=934.0, val_loss=858.0]Epoch 0: 100%|██████████| 63/63 [00:20<00:00,  3.03it/s, v_num=31, train_loss_step=934.0, val_loss=858.0, train_loss_epoch=2.73e+3]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|██████████| 63/63 [00:20<00:00,  3.03it/s, v_num=31, train_loss_step=934.0, val_loss=858.0, train_loss_epoch=2.73e+3]
